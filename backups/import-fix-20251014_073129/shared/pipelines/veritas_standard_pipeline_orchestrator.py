#!/usr/bin/env python3
"""
Standard VERITAS Pipeline Orchestrator
Verwendet das default_pipeline_schema.json f√ºr die vollst√§ndige VERITAS-Pipeline

Author: AI Assistant  
Date: 2025-09-04
"""

import logging
import json
import time
from typing import Dict, List, Any, Optional
from pathlib import Path
import threading
from queue import Queue

# Import robust database functions
from ingestion_internal_ig_wrapper import get_robust_pipeline_connection
from ingestion_schema_manager import PipelineSchemaManager

logger = logging.getLogger(__name__)

class VeritasPipelineOrchestrator:
    """
    Standard VERITAS Pipeline Orchestrator
    Verwendet das default_pipeline_schema.json f√ºr die vollst√§ndige Pipeline-Verarbeitung
    """
    
    def __init__(self, pipeline_db_manager=None):
        """
        Initialisiert den VERITAS-Pipeline-Orchestrator
        """
        # Schema-Manager mit Standard-Template
        self.schema_manager = PipelineSchemaManager()
        
        # Stelle sicher, dass das Standard-Pipeline-Schema existiert
        self._ensure_standard_pipeline_schema()
        
        # Lade Pipeline-Konfiguration aus der Datenbank
        self.job_chains = self._load_job_chains_from_db()
        self.backend_integrations = self._load_backend_integrations_from_db()
        self.pipeline_metrics = self._initialize_pipeline_metrics()
        
        self.processing_queue = Queue()
        self.orchestration_active = False
        self.orchestration_thread = None
        
        logger.info("üéØ VERITAS Pipeline-Orchestrator mit Standard-Schema initialisiert")
    
    def _ensure_standard_pipeline_schema(self):
        """
        Stellt sicher, dass das Standard-Pipeline-Schema existiert
        """
        try:
            validation = self.schema_manager.validate_database_schema()
            
            # √úberpr√ºfe ob alle Standard-Pipeline-Tabellen existieren
            required_tables = ['pipelines', 'job_chains', 'backend_integrations', 'pipeline_metrics']
            existing_tables = validation.get('tables', [])
            
            missing_tables = [t for t in required_tables if t not in existing_tables]
            
            if missing_tables:
                logger.info(f"üìã Fehlende Pipeline-Tabellen: {missing_tables}")
                logger.info("üî® Erstelle Standard-Pipeline-Schema aus default_pipeline_schema.json...")
                
                # Lade explizit das Standard-Schema
                schema = self.schema_manager.load_schema_template("default_pipeline_schema.json")
                if schema:
                    success = self.schema_manager.create_database_from_schema(schema)
                    if success:
                        logger.info(f"‚úÖ Standard-Pipeline-Schema v{schema.get('version', 'unknown')} erstellt")
                    else:
                        logger.error("‚ùå Standard-Pipeline-Schema-Erstellung fehlgeschlagen")
                else:
                    logger.error("‚ùå Standard-Pipeline-Schema konnte nicht geladen werden")
            else:
                logger.info("‚úÖ Standard-Pipeline-Schema vollst√§ndig vorhanden")
                
        except Exception as e:
            logger.error(f"‚ùå Standard-Schema-Validation fehlgeschlagen: {e}")
    
    def _initialize_pipeline_metrics(self) -> Dict[str, Any]:
        """
        Initialisiert Pipeline-Metriken f√ºr alle Standard-Job-Types
        """
        standard_job_types = [
            "preprocessor", "enhanced_metadata", "chunking", "nlp", 
            "llm", "uds3", "internal_relations", "cross_relations", 
            "keywords", "kge", "quality", "backend", "postprocessor"
        ]
        
        metrics = {}
        for job_type in standard_job_types:
            metrics[job_type] = {
                'total_jobs': 0,
                'completed_jobs': 0,
                'failed_jobs': 0,
                'avg_processing_time': 0.0,
                'avg_quality_score': 0.0
            }
        
        logger.info(f"üìä Pipeline-Metriken f√ºr {len(standard_job_types)} Job-Types initialisiert")
        return metrics
    
    def _load_job_chains_from_db(self) -> Dict[str, List[str]]:
        """
        L√§dt Job-Ketten-Definitionen aus der Standard-Pipeline-Datenbank
        """
        try:
            with get_robust_pipeline_connection() as conn:
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT source_job_type, target_job_type, priority, active, description
                    FROM job_chains 
                    WHERE active = 1
                    ORDER BY priority DESC
                ''')
                
                chains = {}
                chain_descriptions = {}
                
                for source_job, target_job, priority, active, description in cursor.fetchall():
                    if source_job not in chains:
                        chains[source_job] = []
                        chain_descriptions[source_job] = {}
                    
                    chains[source_job].append(target_job)
                    chain_descriptions[source_job][target_job] = {
                        'priority': priority,
                        'description': description
                    }
            
            self.chain_descriptions = chain_descriptions
            logger.info(f"üìã {len(chains)} Standard-Pipeline-Ketten geladen")
            
            # Logge die Pipeline-Sequenz
            if chains:
                logger.info("üîó Standard-Pipeline-Sequenz:")
                for source, targets in chains.items():
                    for target in targets:
                        desc_info = chain_descriptions.get(source, {}).get(target, {})
                        priority = desc_info.get('priority', 'unknown')
                        desc = desc_info.get('description', 'Keine Beschreibung')
                        logger.info(f"   {priority:4s}: {source:20s} ‚Üí {target:20s}")
            
            return chains
            
        except Exception as e:
            logger.error(f"‚ùå Standard-Pipeline-Chains-Loading fehlgeschlagen: {e}")
            return {}
    
    def _load_backend_integrations_from_db(self) -> Dict[str, Dict[str, str]]:
        """
        L√§dt Backend-Integration-Definitionen aus der Standard-Pipeline-Datenbank
        """
        try:
            with get_robust_pipeline_connection() as conn:
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT job_type, backend_type, storage_path, config, 
                           compression, encryption, active, description
                    FROM backend_integrations 
                    WHERE active = 1
                    ORDER BY job_type
                ''')
                
                integrations = {}
                
                for row in cursor.fetchall():
                    job_type, backend_type, storage_path, config, compression, encryption, active, description = row
                    
                    integrations[job_type] = {
                        'backend_type': backend_type,
                        'storage_path': storage_path,
                        'config': json.loads(config) if config else {},
                        'compression': bool(compression),
                        'encryption': bool(encryption),
                        'description': description
                    }
            
            logger.info(f"üîß {len(integrations)} Standard-Backend-Integrationen geladen")
            
            # Logge die Backend-Integrationen
            if integrations:
                logger.info("üîå Standard-Backend-Integrationen:")
                for job_type, config in integrations.items():
                    backend = config['backend_type']
                    features = []
                    if config['compression']:
                        features.append("komprimiert")
                    if config['encryption']:
                        features.append("verschl√ºsselt")
                    feature_str = f" ({', '.join(features)})" if features else ""
                    logger.info(f"   {job_type:20s} ‚Üí {backend:30s}{feature_str}")
            
            return integrations
            
        except Exception as e:
            logger.error(f"‚ùå Standard-Backend-Integration-Loading fehlgeschlagen: {e}")
            return {}
    
    def start_standard_pipeline_orchestration(self):
        """
        Startet die Standard-VERITAS-Pipeline-Orchestrierung
        """
        if self.orchestration_active:
            logger.warning("‚ö†Ô∏è Standard-Pipeline-Orchestrierung bereits aktiv")
            return False
        
        self.orchestration_active = True
        self.orchestration_thread = threading.Thread(target=self._standard_orchestration_loop, daemon=True)
        self.orchestration_thread.start()
        
        logger.info("üöÄ Standard-VERITAS-Pipeline-Orchestrierung gestartet")
        return True
    
    def stop_standard_pipeline_orchestration(self):
        """
        Stoppt die Standard-VERITAS-Pipeline-Orchestrierung
        """
        if not self.orchestration_active:
            logger.warning("‚ö†Ô∏è Standard-Pipeline-Orchestrierung bereits gestoppt")
            return False
        
        self.orchestration_active = False
        
        if self.orchestration_thread:
            self.orchestration_thread.join(timeout=5.0)
        
        logger.info("üõë Standard-VERITAS-Pipeline-Orchestrierung gestoppt")
        return True
    
    def _standard_orchestration_loop(self):
        """
        Haupt-Loop f√ºr Standard-VERITAS-Pipeline-Orchestrierung
        """
        logger.info("üîÑ Standard-Pipeline-Orchestration-Loop gestartet")
        
        while self.orchestration_active:
            try:
                # 1. Verarbeite abgeschlossene Jobs f√ºr Standard-Pipeline-Chaining
                self._process_completed_jobs_for_chaining()
                
                # 2. Verarbeite Backend-Integration-Kandidaten
                self._process_backend_integration_candidates()
                
                # 3. Aktualisiere Pipeline-Metriken
                self._update_pipeline_metrics()
                
                # Kurze Pause zwischen Iterations
                time.sleep(3.0)
                
            except Exception as e:
                logger.error(f"‚ùå Standard-Pipeline-Orchestration-Loop Fehler: {e}")
                time.sleep(5.0)
        
        logger.info("üèÅ Standard-Pipeline-Orchestration-Loop beendet")
    
    def _process_completed_jobs_for_chaining(self):
        """
        Verarbeitet abgeschlossene Jobs f√ºr Standard-Pipeline-Ketten-Ausl√∂sung
        """
        try:
            with get_robust_pipeline_connection() as conn:
                cursor = conn.cursor()
                
                # Finde COMPLETED Jobs die noch nicht f√ºr Chaining verarbeitet wurden
                cursor.execute('''
                    SELECT job_id, job_type, collection_name, document_id, file_path, metadata, quality_score
                    FROM pipelines 
                    WHERE status = 'COMPLETED' 
                    AND (chained_processed IS NULL OR chained_processed = 0)
                    LIMIT 25
                ''')
                
                completed_jobs = cursor.fetchall()
                
                for job_id, job_type, collection_name, document_id, file_path, metadata, quality_score in completed_jobs:
                    
                    # Pr√ºfe ob dieser Job-Typ Folge-Jobs in der Standard-Pipeline ausl√∂sen soll
                    if job_type in self.job_chains:
                        target_job_types = self.job_chains[job_type]
                        
                        for target_job_type in target_job_types:
                            # Pr√ºfe Quality-basierte Bedingungen
                            if self._check_chain_conditions(job_type, target_job_type, quality_score, metadata):
                                success = self._trigger_standard_chained_job(
                                    job_id, job_type, target_job_type, 
                                    collection_name, document_id, file_path, metadata
                                )
                                
                                if success:
                                    logger.info(f"üîó Standard-Pipeline-Chain: {job_type} ‚Üí {target_job_type} (source: {job_id})")
                    
                    # Markiere als f√ºr Chaining verarbeitet
                    cursor.execute('''
                        UPDATE pipelines 
                        SET chained_processed = 1, updated_at = ?
                        WHERE job_id = ?
                    ''', (time.strftime('%Y-%m-%d %H:%M:%S'), job_id))
                
                conn.commit()
                
                if completed_jobs:
                    logger.info(f"üîÑ {len(completed_jobs)} Jobs f√ºr Standard-Pipeline-Chaining verarbeitet")
                
        except Exception as e:
            logger.error(f"‚ùå Standard-Pipeline-Chain-Processing fehlgeschlagen: {e}")
    
    def _check_chain_conditions(self, source_job_type: str, target_job_type: str, 
                              quality_score: float, metadata: str) -> bool:
        """
        Pr√ºft ob Chain-Bedingungen f√ºr Standard-Pipeline erf√ºllt sind
        """
        try:
            # Spezielle Bedingung: quality ‚Üí backend nur bei ausreichender Qualit√§t
            if source_job_type == "quality" and target_job_type == "backend":
                if quality_score is None or quality_score < 0.7:
                    logger.debug(f"‚ùå Quality-Score zu niedrig f√ºr Backend-Integration: {quality_score}")
                    return False
                else:
                    logger.debug(f"‚úÖ Quality-Score ausreichend f√ºr Backend-Integration: {quality_score}")
            
            # Weitere Standard-Bedingungen k√∂nnen hier hinzugef√ºgt werden
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Chain-Conditions-Check fehlgeschlagen: {e}")
            return False
    
    def _trigger_standard_chained_job(self, source_job_id: int, source_job_type: str, 
                                    target_job_type: str, collection_name: str, 
                                    document_id: str, file_path: str, metadata: str) -> bool:
        """
        L√∂st einen Folge-Job in der Standard-Pipeline-Chain aus
        """
        try:
            with get_robust_pipeline_connection() as conn:
                cursor = conn.cursor()
                
                # Erstelle neuen Job f√ºr die Standard-Pipeline-Chain
                chain_metadata = {
                    'chained_from': source_job_id,
                    'chain_source_type': source_job_type,
                    'pipeline_step': target_job_type,
                    'original_metadata': json.loads(metadata) if metadata else {}
                }
                
                cursor.execute('''
                    INSERT INTO pipelines (
                        status, job_type, collection_name, document_id, file_path, 
                        metadata, chain_source_job_id, created_at, updated_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    'PENDING',
                    target_job_type,
                    collection_name,
                    document_id,
                    file_path,
                    json.dumps(chain_metadata),
                    source_job_id,
                    time.strftime('%Y-%m-%d %H:%M:%S'),
                    time.strftime('%Y-%m-%d %H:%M:%S')
                ))
                
                chained_job_id = cursor.lastrowid
                conn.commit()
                
            logger.info(f"‚úÖ Standard-Pipeline-Chain-Job erstellt: {target_job_type} (ID: {chained_job_id})")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Standard-Pipeline-Chain-Job Creation fehlgeschlagen: {e}")
            return False
    
    def _process_backend_integration_candidates(self):
        """
        Verarbeitet Jobs die f√ºr Standard-Backend-Integration bereit sind
        """
        try:
            with get_robust_pipeline_connection() as conn:
                cursor = conn.cursor()
                
                # Finde COMPLETED Jobs die noch nicht backend-integriert wurden
                cursor.execute('''
                    SELECT job_id, job_type, collection_name, document_id, file_path, metadata
                    FROM pipelines 
                    WHERE status = 'COMPLETED' 
                    AND (backend_integrated IS NULL OR backend_integrated = 0)
                    LIMIT 20
                ''')
                
                integration_candidates = cursor.fetchall()
                
                for job_id, job_type, collection_name, document_id, file_path, metadata in integration_candidates:
                    
                    # Pr√ºfe ob Standard-Backend-Integration f√ºr diesen Job-Typ definiert ist
                    if job_type in self.backend_integrations:
                        integration_config = self.backend_integrations[job_type]
                        
                        success = self._integrate_job_to_standard_backend(
                            job_id, job_type, integration_config,
                            file_path, collection_name, document_id, metadata
                        )
                        
                        if success:
                            # Markiere als backend-integriert
                            cursor.execute('''
                                UPDATE pipelines 
                                SET backend_integrated = 1, updated_at = ?
                                WHERE job_id = ?
                            ''', (time.strftime('%Y-%m-%d %H:%M:%S'), job_id))
                            
                            logger.info(f"üîå Standard-Backend-Integration: Job {job_id} ‚Üí {integration_config['backend_type']}")
                
                conn.commit()
                
                if integration_candidates:
                    logger.info(f"üîå {len(integration_candidates)} Jobs f√ºr Standard-Backend-Integration verarbeitet")
                
        except Exception as e:
            logger.error(f"‚ùå Standard-Backend-Integration-Processing fehlgeschlagen: {e}")
    
    def _integrate_job_to_standard_backend(self, job_id: int, job_type: str, 
                                         integration_config: Dict[str, Any],
                                         file_path: str, collection_name: str, 
                                         document_id: str, metadata: str) -> bool:
        """
        Integriert Job-Result ins entsprechende Standard-Backend
        """
        try:
            storage_path = integration_config['storage_path'].format(
                collection_name=collection_name,
                document_id=document_id,
                job_id=job_id
            )
            
            integration_data = {
                'job_id': job_id,
                'job_type': job_type,
                'backend_type': integration_config['backend_type'],
                'storage_path': storage_path,
                'collection_name': collection_name,
                'document_id': document_id,
                'compression': integration_config['compression'],
                'encryption': integration_config['encryption'],
                'metadata': json.loads(metadata) if metadata else {},
                'integration_timestamp': time.time()
            }
            
            # Standard-Backend-Integration (hier als Platzhalter)
            logger.info(f"üíæ Standard-Backend-Integration: {storage_path}")
            
            if integration_config['compression']:
                logger.debug(f"üóúÔ∏è  Komprimierung aktiviert f√ºr {job_type}")
            
            if integration_config['encryption']:
                logger.debug(f"üîê Verschl√ºsselung aktiviert f√ºr {job_type}")
            
            return True
                
        except Exception as e:
            logger.error(f"Standard-Backend-Integration fehlgeschlagen f√ºr Job {job_id}: {e}")
            return False
    
    def _update_pipeline_metrics(self):
        """
        Aktualisiert Pipeline-Metriken f√ºr Standard-Pipeline
        """
        try:
            with get_robust_pipeline_connection() as conn:
                cursor = conn.cursor()
                
                # Aktualisiere Metriken f√ºr jeden Standard-Job-Type
                for job_type in self.pipeline_metrics.keys():
                    cursor.execute('''
                        SELECT 
                            COUNT(*) as total,
                            SUM(CASE WHEN status = 'COMPLETED' THEN 1 ELSE 0 END) as completed,
                            SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) as failed,
                            AVG(CASE WHEN processing_time IS NOT NULL THEN processing_time ELSE 0 END) as avg_time,
                            AVG(CASE WHEN quality_score IS NOT NULL THEN quality_score ELSE 0 END) as avg_quality
                        FROM pipelines 
                        WHERE job_type = ?
                    ''', (job_type,))
                    
                    result = cursor.fetchone()
                    if result:
                        total, completed, failed, avg_time, avg_quality = result
                        self.pipeline_metrics[job_type] = {
                            'total_jobs': total or 0,
                            'completed_jobs': completed or 0,
                            'failed_jobs': failed or 0,
                            'avg_processing_time': avg_time or 0.0,
                            'avg_quality_score': avg_quality or 0.0
                        }
                        
        except Exception as e:
            logger.error(f"‚ùå Pipeline-Metriken-Update fehlgeschlagen: {e}")
    
    def get_standard_pipeline_stats(self) -> Dict[str, Any]:
        """
        Gibt Standard-Pipeline-Statistiken zur√ºck
        """
        try:
            with get_robust_pipeline_connection() as conn:
                cursor = conn.cursor()
                
                cursor.execute('SELECT COUNT(*) FROM pipelines WHERE chained_processed = 1')
                chained_jobs_count = cursor.fetchone()[0]
                
                cursor.execute('SELECT COUNT(*) FROM pipelines WHERE backend_integrated = 1')
                integrated_jobs_count = cursor.fetchone()[0]
                
                cursor.execute('SELECT COUNT(*) FROM pipelines WHERE status = "PENDING"')
                pending_jobs_count = cursor.fetchone()[0]
                
                cursor.execute('SELECT COUNT(*) FROM pipelines WHERE status = "COMPLETED"')
                completed_jobs_count = cursor.fetchone()[0]
            
            return {
                'orchestration_active': self.orchestration_active,
                'pipeline_type': 'standard_veritas_pipeline',
                'job_chains_defined': len(self.job_chains),
                'backend_integrations_defined': len(self.backend_integrations),
                'chained_jobs_processed': chained_jobs_count,
                'backend_integrated_jobs': integrated_jobs_count,
                'pending_jobs': pending_jobs_count,
                'completed_jobs': completed_jobs_count,
                'pipeline_metrics': self.pipeline_metrics
            }
            
        except Exception as e:
            logger.error(f"‚ùå Standard-Pipeline-Stats-Abruf fehlgeschlagen: {e}")
            return {
                'orchestration_active': self.orchestration_active,
                'pipeline_type': 'standard_veritas_pipeline',
                'job_chains_defined': len(self.job_chains),
                'backend_integrations_defined': len(self.backend_integrations),
                'error': str(e)
            }

def main():
    """
    Test-Funktion f√ºr den Standard-VERITAS-Pipeline-Orchestrator
    """
    logging.basicConfig(level=logging.INFO)
    
    print("üöÄ Standard VERITAS Pipeline-Orchestrator Test")
    print("="*60)
    
    orchestrator = VeritasPipelineOrchestrator()
    
    print("\nüìä Standard-Pipeline-Statistiken:")
    stats = orchestrator.get_standard_pipeline_stats()
    for key, value in stats.items():
        if key != 'pipeline_metrics':
            print(f"   {key}: {value}")
    
    print("\n‚úÖ Standard-Pipeline-Orchestrator-Test erfolgreich!")

if __name__ == "__main__":
    main()
