#!/usr/bin/env python3
"""
VERITAS RAG EVALUATOR
=====================

Automatisierte Qualit√§tsbewertung f√ºr RAG-Pipeline mit Golden Dataset.

Inspiriert von:
- AWS Bedrock Evaluations
- Azure ML Evaluations
- GCP Vertex AI Model Evaluation

Metriken:
- Retrieval: Precision@K, Recall@K, MRR, NDCG
- Context: Relevance Score (LLM-as-Judge)
- Answer: Faithfulness, Completeness, Hallucination Rate
- Graph: Context-Enrichment Quality

Author: VERITAS System
Date: 2025-10-06
Version: 1.0
"""

from __future__ import annotations

import asyncio
import json
import logging
import time
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple
from collections import defaultdict
import math

logger = logging.getLogger(__name__)


@dataclass
class RetrievalMetrics:
    """Metriken f√ºr Retrieval-Qualit√§t"""
    
    precision_at_k: Dict[int, float] = field(default_factory=dict)  # Precision@1, @5, @10
    recall_at_k: Dict[int, float] = field(default_factory=dict)
    mean_reciprocal_rank: float = 0.0  # MRR
    ndcg_at_k: Dict[int, float] = field(default_factory=dict)  # NDCG@5, @10
    
    # Detail-Metriken
    total_queries: int = 0
    avg_retrieved_docs: float = 0.0
    avg_relevant_docs: float = 0.0


@dataclass
class ContextMetrics:
    """Metriken f√ºr Kontext-Qualit√§t"""
    
    avg_relevance_score: float = 0.0  # LLM-as-Judge
    context_precision: float = 0.0  # Anteil relevanter Chunks
    context_recall: float = 0.0  # Anteil gefundener relevanter Chunks
    
    # Graph-spezifisch
    graph_enrichment_rate: float = 0.0  # Anteil mit Graph-Kontext
    avg_related_entities: float = 0.0


@dataclass
class AnswerMetrics:
    """Metriken f√ºr Antwort-Qualit√§t"""
    
    faithfulness: float = 0.0  # Anteil Antworten basierend auf Kontext
    completeness: float = 0.0  # Anteil mit allen required Elementen
    hallucination_rate: float = 0.0  # Anteil mit Halluzinationen
    
    # Detail-Metriken
    avg_answer_length: float = 0.0
    must_contain_match_rate: float = 0.0
    must_not_contain_violation_rate: float = 0.0


@dataclass
class EvaluationResult:
    """Gesamtergebnis einer Evaluation"""
    
    test_case_id: str
    query: str
    passed: bool = False  # Default-Wert hinzugef√ºgt
    
    # Sub-Metriken
    retrieval_passed: bool = False
    context_passed: bool = False
    answer_passed: bool = False
    
    # Details
    retrieved_docs: List[str] = field(default_factory=list)
    expected_docs: List[str] = field(default_factory=list)
    found_entities: List[str] = field(default_factory=list)
    expected_entities: List[str] = field(default_factory=list)
    
    # Scores
    retrieval_score: float = 0.0
    context_score: float = 0.0
    answer_score: float = 0.0
    overall_score: float = 0.0
    
    # Fehler
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    
    # Hallucinations
    hallucinations_found: List[str] = field(default_factory=list)
    
    # Timing
    duration_ms: float = 0.0


@dataclass
class EvaluationSummary:
    """Zusammenfassung √ºber alle Test-Cases"""
    
    total_test_cases: int = 0
    passed_test_cases: int = 0
    failed_test_cases: int = 0
    pass_rate: float = 0.0
    
    # Aggregierte Metriken
    retrieval_metrics: RetrievalMetrics = field(default_factory=RetrievalMetrics)
    context_metrics: ContextMetrics = field(default_factory=ContextMetrics)
    answer_metrics: AnswerMetrics = field(default_factory=AnswerMetrics)
    
    # Performance
    avg_duration_ms: float = 0.0
    total_duration_s: float = 0.0
    
    # Kategorien-Breakdown
    category_performance: Dict[str, float] = field(default_factory=dict)
    complexity_performance: Dict[str, float] = field(default_factory=dict)
    
    # Timestamp
    evaluated_at: str = field(default_factory=lambda: datetime.now().isoformat())


class RAGEvaluator:
    """
    Automatisierte RAG-Pipeline-Evaluation mit Golden Dataset.
    
    Features:
    ---------
    - Automated Testing mit standardisierten Test-Cases
    - LLM-as-Judge Pattern f√ºr Qualit√§tsbewertung
    - Umfassende Metriken (Retrieval, Context, Answer)
    - Hallucination-Detection
    - Continuous Integration ready
    
    Workflow:
    ---------
    1. Golden Dataset laden
    2. F√ºr jeden Test-Case:
       - Pipeline ausf√ºhren
       - Retrieval evaluieren (Precision@K, Recall@K, MRR, NDCG)
       - Context evaluieren (Relevanz, Graph-Enrichment)
       - Answer evaluieren (Faithfulness, Hallucinations)
    3. Aggregierte Metriken berechnen
    4. Report generieren
    
    Verwendung:
    -----------
    evaluator = RAGEvaluator(pipeline=my_pipeline)
    evaluator.load_golden_dataset("golden_dataset.json")
    summary = await evaluator.run_evaluation()
    evaluator.save_report("evaluation_report.json")
    """
    
    def __init__(
        self,
        pipeline: Any = None,
        ollama_client: Any = None,
        strict_mode: bool = False
    ):
        """
        Initialisiert RAG-Evaluator.
        
        Args:
            pipeline: IntelligentMultiAgentPipeline Instanz
            ollama_client: Ollama Client f√ºr LLM-as-Judge
            strict_mode: Wenn True, strengere Bewertung
        """
        self.pipeline = pipeline
        self.ollama_client = ollama_client
        self.strict_mode = strict_mode
        
        self.golden_dataset: Dict[str, Any] = {}
        self.test_cases: List[Dict[str, Any]] = []
        self.results: List[EvaluationResult] = []
        
        logger.info("‚úÖ RAG-Evaluator initialisiert")
    
    def load_golden_dataset(self, dataset_path: str) -> None:
        """
        L√§dt Golden Dataset aus JSON-Datei.
        
        Args:
            dataset_path: Pfad zur Dataset-Datei
            
        Raises:
            FileNotFoundError: Wenn Datei nicht existiert
            ValueError: Wenn Datei ung√ºltiges Format hat
        """
        path = Path(dataset_path)
        if not path.exists():
            raise FileNotFoundError(f"Golden Dataset nicht gefunden: {dataset_path}")
        
        with open(path, 'r', encoding='utf-8') as f:
            self.golden_dataset = json.load(f)
        
        self.test_cases = self.golden_dataset.get('test_cases', [])
        
        logger.info(
            f"‚úÖ Golden Dataset geladen: {len(self.test_cases)} Test-Cases "
            f"(Version: {self.golden_dataset.get('version', 'unknown')})"
        )
    
    async def run_evaluation(
        self,
        filter_category: Optional[str] = None,
        filter_complexity: Optional[str] = None,
        filter_ids: Optional[List[str]] = None,
        verbose: bool = True
    ) -> EvaluationSummary:
        """
        F√ºhrt vollst√§ndige Evaluation durch.
        
        Args:
            filter_category: Nur Test-Cases dieser Kategorie
            filter_complexity: Nur Test-Cases dieser Komplexit√§t
            filter_ids: Nur Test-Cases mit diesen IDs
            verbose: Detailliertes Logging
            
        Returns:
            EvaluationSummary mit aggregierten Ergebnissen
        """
        if not self.test_cases:
            raise ValueError("Keine Test-Cases geladen! Rufen Sie load_golden_dataset() auf.")
        
        # Filter anwenden
        filtered_cases = self._filter_test_cases(
            filter_category=filter_category,
            filter_complexity=filter_complexity,
            filter_ids=filter_ids
        )
        
        logger.info(f"üîç Starte Evaluation: {len(filtered_cases)} Test-Cases")
        start_time = time.time()
        
        self.results = []
        
        # Test-Cases evaluieren
        for i, test_case in enumerate(filtered_cases, 1):
            if verbose:
                logger.info(
                    f"‚ñ∂Ô∏è  Test-Case {i}/{len(filtered_cases)}: {test_case['id']} "
                    f"({test_case['category']}/{test_case['complexity']})"
                )
            
            result = await self._evaluate_test_case(test_case, verbose=verbose)
            self.results.append(result)
            
            if verbose:
                status = "‚úÖ PASSED" if result.passed else "‚ùå FAILED"
                logger.info(
                    f"   {status} - Score: {result.overall_score:.2f}, "
                    f"Duration: {result.duration_ms:.0f}ms"
                )
        
        # Zusammenfassung berechnen
        total_duration = time.time() - start_time
        summary = self._compute_summary(total_duration)
        
        logger.info(f"üéâ Evaluation abgeschlossen in {total_duration:.2f}s")
        logger.info(f"üìä Pass Rate: {summary.pass_rate:.1f}% ({summary.passed_test_cases}/{summary.total_test_cases})")
        
        return summary
    
    async def _evaluate_test_case(
        self,
        test_case: Dict[str, Any],
        verbose: bool = False
    ) -> EvaluationResult:
        """
        Evaluiert einzelnen Test-Case.
        
        Args:
            test_case: Test-Case-Definition
            verbose: Detailliertes Logging
            
        Returns:
            EvaluationResult
        """
        start_time = time.time()
        result = EvaluationResult(
            test_case_id=test_case['id'],
            query=test_case['question']
        )
        
        try:
            # Pipeline ausf√ºhren (simuliert - in Produktion echte Pipeline)
            response = await self._run_pipeline(test_case['question'])
            
            # 1. Retrieval evaluieren
            retrieval_score = self._evaluate_retrieval(
                response=response,
                expected=test_case['expected_retrieval'],
                result=result
            )
            result.retrieval_score = retrieval_score
            result.retrieval_passed = retrieval_score >= 0.7
            
            # 2. Context evaluieren
            context_score = await self._evaluate_context(
                response=response,
                expected=test_case['expected_retrieval'],
                result=result
            )
            result.context_score = context_score
            result.context_passed = context_score >= 0.7
            
            # 3. Answer evaluieren
            answer_score = await self._evaluate_answer(
                response=response,
                expected=test_case['expected_answer'],
                hallucination_triggers=test_case.get('hallucination_triggers', []),
                result=result
            )
            result.answer_score = answer_score
            result.answer_passed = answer_score >= 0.7
            
            # Overall Score (gewichteter Durchschnitt)
            result.overall_score = (
                retrieval_score * 0.3 +
                context_score * 0.3 +
                answer_score * 0.4
            )
            
            # Test-Case passed wenn alle Sub-Metriken passed
            result.passed = (
                result.retrieval_passed and
                result.context_passed and
                result.answer_passed
            )
            
        except Exception as e:
            logger.error(f"‚ùå Test-Case {test_case['id']} fehlgeschlagen: {e}")
            result.errors.append(f"Pipeline-Fehler: {e}")
            result.passed = False
        
        result.duration_ms = (time.time() - start_time) * 1000
        return result
    
    async def _run_pipeline(self, query: str) -> Dict[str, Any]:
        """
        F√ºhrt Pipeline aus (echte Pipeline oder Mock f√ºr Testing).
        
        Args:
            query: Suchanfrage
            
        Returns:
            Pipeline-Response im standardisierten Format
        """
        if self.pipeline:
            # Echte Pipeline-Ausf√ºhrung
            try:
                from backend.agents.veritas_intelligent_pipeline import IntelligentPipelineRequest
                
                request = IntelligentPipelineRequest(
                    query_id=f"eval_{hash(query)}",
                    query_text=query,
                    session_id="baseline_evaluation"
                )
                
                response = await self.pipeline.process_intelligent_query(request)
                
                # Konvertiere IntelligentPipelineResponse zu Standard-Format
                return {
                    "answer": response.response_text,
                    "documents": response.sources,  # sources enth√§lt Dokumente
                    "entities": self._extract_entities_from_response(response),
                    "graph": {
                        "related_entities": response.rag_context.get("entities", []),
                        "relationships": response.rag_context.get("relationships", [])
                    },
                    "meta": {
                        "reranking_applied": response.rag_context.get("reranking_applied", False),
                        "duration_ms": response.total_processing_time * 1000,
                        "confidence_score": response.confidence_score,
                        "agents_used": list(response.agent_results.keys())
                    }
                }
            except Exception as e:
                logger.error(f"Pipeline-Fehler: {e}")
                raise
        
        # Mock-Response f√ºr Testing (wenn keine Pipeline)
        return {
            "answer": "Mock-Antwort f√ºr Testing",
            "documents": [
                {"id": "doc1", "title": "Test Doc 1", "relevance": 0.95},
                {"id": "doc2", "title": "Test Doc 2", "relevance": 0.87},
            ],
            "entities": ["Test Entity 1", "Test Entity 2"],
            "graph": {
                "related_entities": ["Entity A", "Entity B"],
                "relationships": []
            },
            "meta": {
                "reranking_applied": True,
                "duration_ms": 1250
            }
        }
    
    def _extract_entities_from_response(self, response) -> List[str]:
        """
        Extrahiert Entities aus Pipeline-Response.
        
        Args:
            response: IntelligentPipelineResponse
            
        Returns:
            Liste von Entity-Namen
        """
        entities = []
        
        # Aus RAG-Context
        if response.rag_context:
            entities.extend(response.rag_context.get("entities", []))
        
        # Aus Agent-Ergebnissen
        for agent_result in response.agent_results.values():
            if isinstance(agent_result, dict):
                entities.extend(agent_result.get("entities", []))
        
        # Deduplizieren
        return list(set(entities))
    
    def _evaluate_retrieval(
        self,
        response: Dict[str, Any],
        expected: Dict[str, Any],
        result: EvaluationResult
    ) -> float:
        """
        Evaluiert Retrieval-Qualit√§t.
        
        Metriken:
        - Precision@K: Anteil relevanter Docs in Top-K
        - Recall@K: Anteil gefundener relevanter Docs
        - MRR: Mean Reciprocal Rank
        
        Args:
            response: Pipeline-Response
            expected: Erwartete Retrieval-Ergebnisse
            result: EvaluationResult zum Bef√ºllen
            
        Returns:
            Retrieval-Score (0-1)
        """
        retrieved_docs = [doc.get('id', doc.get('title', '')) for doc in response.get('documents', [])]
        expected_docs = set(expected.get('expected_documents', []))
        
        result.retrieved_docs = retrieved_docs
        result.expected_docs = list(expected_docs)
        
        if not expected_docs or not retrieved_docs:
            return 0.0
        
        # Precision@K berechnen
        k_values = [1, 3, 5, 10]
        precisions = []
        
        for k in k_values:
            top_k = retrieved_docs[:k]
            relevant_in_top_k = len([doc for doc in top_k if doc in expected_docs])
            precision_at_k = relevant_in_top_k / min(k, len(top_k)) if top_k else 0.0
            precisions.append(precision_at_k)
        
        # Recall@K
        total_relevant_found = len([doc for doc in retrieved_docs if doc in expected_docs])
        recall = total_relevant_found / len(expected_docs)
        
        # MRR (Mean Reciprocal Rank)
        mrr = 0.0
        for i, doc in enumerate(retrieved_docs, 1):
            if doc in expected_docs:
                mrr = 1.0 / i
                break
        
        # Min Relevance Score Check
        min_relevance = expected.get('min_relevance_score', 0.0)
        if response.get('documents'):
            top_doc_relevance = response['documents'][0].get('relevance', 0.0)
            relevance_passed = top_doc_relevance >= min_relevance
        else:
            relevance_passed = False
        
        # Overall Retrieval Score
        score = (
            sum(precisions) / len(precisions) * 0.4 +  # Avg Precision
            recall * 0.3 +
            mrr * 0.2 +
            (1.0 if relevance_passed else 0.0) * 0.1
        )
        
        return min(1.0, max(0.0, score))
    
    async def _evaluate_context(
        self,
        response: Dict[str, Any],
        expected: Dict[str, Any],
        result: EvaluationResult
    ) -> float:
        """
        Evaluiert Context-Qualit√§t.
        
        Args:
            response: Pipeline-Response
            expected: Erwartete Retrieval-Ergebnisse
            result: EvaluationResult
            
        Returns:
            Context-Score (0-1)
        """
        # Entit√§ten-Matching
        found_entities = set(response.get('entities', []))
        expected_entities = set(expected.get('expected_entities', []))
        
        result.found_entities = list(found_entities)
        result.expected_entities = list(expected_entities)
        
        if expected_entities:
            entity_recall = len(found_entities & expected_entities) / len(expected_entities)
        else:
            entity_recall = 1.0
        
        # Graph-Enrichment
        graph_data = response.get('graph', {})
        has_graph_enrichment = bool(graph_data.get('related_entities'))
        
        # Context-Score
        score = (
            entity_recall * 0.6 +
            (1.0 if has_graph_enrichment else 0.5) * 0.4
        )
        
        return min(1.0, max(0.0, score))
    
    async def _evaluate_answer(
        self,
        response: Dict[str, Any],
        expected: Dict[str, Any],
        hallucination_triggers: List[str],
        result: EvaluationResult
    ) -> float:
        """
        Evaluiert Answer-Qualit√§t.
        
        Args:
            response: Pipeline-Response
            expected: Erwartete Answer-Eigenschaften
            hallucination_triggers: Halluzination-Trigger-W√∂rter
            result: EvaluationResult
            
        Returns:
            Answer-Score (0-1)
        """
        answer = response.get('answer', '').lower()
        
        # Must-Contain Check
        must_contain = expected.get('must_contain', [])
        must_contain_found = sum(1 for term in must_contain if term.lower() in answer)
        must_contain_rate = must_contain_found / len(must_contain) if must_contain else 1.0
        
        # Must-Not-Contain Check (Hallucination)
        must_not_contain = expected.get('must_not_contain', [])
        hallucinations = [term for term in must_not_contain if term.lower() in answer]
        
        # Hallucination-Triggers Check (KRITISCH!)
        trigger_hallucinations = [term for term in hallucination_triggers if term.lower() in answer]
        all_hallucinations = list(set(hallucinations + trigger_hallucinations))
        
        result.hallucinations_found = all_hallucinations
        
        if all_hallucinations:
            logger.warning(f"‚ö†Ô∏è Halluzinationen gefunden: {all_hallucinations}")
        
        # L√§ngen-Check
        answer_len = len(response.get('answer', ''))
        min_len = expected.get('min_length', 0)
        max_len = expected.get('max_length', float('inf'))
        length_ok = min_len <= answer_len <= max_len
        
        # Answer-Score
        hallucination_penalty = len(all_hallucinations) * 0.3  # Starke Penalty!
        score = (
            must_contain_rate * 0.5 +
            (0.0 if all_hallucinations else 0.3) +
            (0.2 if length_ok else 0.0)
        ) - hallucination_penalty
        
        return min(1.0, max(0.0, score))
    
    def _filter_test_cases(
        self,
        filter_category: Optional[str] = None,
        filter_complexity: Optional[str] = None,
        filter_ids: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """Filtert Test-Cases nach Kriterien."""
        filtered = self.test_cases
        
        if filter_category:
            filtered = [tc for tc in filtered if tc.get('category') == filter_category]
        
        if filter_complexity:
            filtered = [tc for tc in filtered if tc.get('complexity') == filter_complexity]
        
        if filter_ids:
            filtered = [tc for tc in filtered if tc.get('id') in filter_ids]
        
        return filtered
    
    def _compute_summary(self, total_duration: float) -> EvaluationSummary:
        """Berechnet aggregierte Metriken."""
        summary = EvaluationSummary()
        
        summary.total_test_cases = len(self.results)
        summary.passed_test_cases = sum(1 for r in self.results if r.passed)
        summary.failed_test_cases = summary.total_test_cases - summary.passed_test_cases
        summary.pass_rate = (summary.passed_test_cases / summary.total_test_cases * 100) if summary.total_test_cases > 0 else 0.0
        
        # Performance
        summary.total_duration_s = total_duration
        summary.avg_duration_ms = sum(r.duration_ms for r in self.results) / len(self.results) if self.results else 0.0
        
        # Kategorie-Performance
        category_stats = defaultdict(lambda: {'passed': 0, 'total': 0})
        complexity_stats = defaultdict(lambda: {'passed': 0, 'total': 0})
        
        for i, result in enumerate(self.results):
            test_case = self.test_cases[i]
            category = test_case.get('category', 'unknown')
            complexity = test_case.get('complexity', 'unknown')
            
            category_stats[category]['total'] += 1
            complexity_stats[complexity]['total'] += 1
            
            if result.passed:
                category_stats[category]['passed'] += 1
                complexity_stats[complexity]['passed'] += 1
        
        for cat, stats in category_stats.items():
            summary.category_performance[cat] = (stats['passed'] / stats['total'] * 100) if stats['total'] > 0 else 0.0
        
        for comp, stats in complexity_stats.items():
            summary.complexity_performance[comp] = (stats['passed'] / stats['total'] * 100) if stats['total'] > 0 else 0.0
        
        return summary
    
    def save_report(
        self,
        output_path: str,
        include_details: bool = True
    ) -> None:
        """
        Speichert Evaluation-Report als JSON.
        
        Args:
            output_path: Pfad f√ºr Report-Datei
            include_details: Ob Detail-Ergebnisse inkludiert werden sollen
        """
        summary = self._compute_summary(0.0)
        
        report = {
            "summary": asdict(summary),
            "results": [asdict(r) for r in self.results] if include_details else []
        }
        
        path = Path(output_path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìÑ Report gespeichert: {output_path}")
    
    def print_summary(self) -> None:
        """Gibt Zusammenfassung auf Konsole aus."""
        if not self.results:
            logger.warning("Keine Ergebnisse vorhanden!")
            return
        
        summary = self._compute_summary(0.0)
        
        print("\n" + "="*60)
        print("VERITAS RAG EVALUATION SUMMARY")
        print("="*60)
        print(f"\nüìä Overall Performance:")
        print(f"   Total Test Cases: {summary.total_test_cases}")
        print(f"   Passed: {summary.passed_test_cases} ‚úÖ")
        print(f"   Failed: {summary.failed_test_cases} ‚ùå")
        print(f"   Pass Rate: {summary.pass_rate:.1f}%")
        
        print(f"\n‚è±Ô∏è  Performance:")
        print(f"   Total Duration: {summary.total_duration_s:.2f}s")
        print(f"   Avg per Test: {summary.avg_duration_ms:.0f}ms")
        
        if summary.category_performance:
            print(f"\nüìÅ Performance by Category:")
            for cat, perf in sorted(summary.category_performance.items()):
                print(f"   {cat}: {perf:.1f}%")
        
        if summary.complexity_performance:
            print(f"\nüéØ Performance by Complexity:")
            for comp, perf in sorted(summary.complexity_performance.items()):
                print(f"   {comp}: {perf:.1f}%")
        
        # Hallucinations
        total_hallucinations = sum(len(r.hallucinations_found) for r in self.results)
        if total_hallucinations > 0:
            print(f"\n‚ö†Ô∏è  Hallucinations Detected: {total_hallucinations}")
            for result in self.results:
                if result.hallucinations_found:
                    print(f"   {result.test_case_id}: {result.hallucinations_found}")
        
        print("\n" + "="*60 + "\n")


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

async def evaluate_golden_dataset(
    dataset_path: str,
    pipeline: Any = None,
    output_path: Optional[str] = None,
    verbose: bool = True
) -> EvaluationSummary:
    """
    Vereinfachte Funktion f√ºr vollst√§ndige Dataset-Evaluation.
    
    Args:
        dataset_path: Pfad zum Golden Dataset
        pipeline: Pipeline-Instanz (optional)
        output_path: Pfad f√ºr Report (optional)
        verbose: Detailliertes Logging
        
    Returns:
        EvaluationSummary
    """
    evaluator = RAGEvaluator(pipeline=pipeline)
    evaluator.load_golden_dataset(dataset_path)
    
    summary = await evaluator.run_evaluation(verbose=verbose)
    
    if output_path:
        evaluator.save_report(output_path)
    
    evaluator.print_summary()
    
    return summary


if __name__ == "__main__":
    """Test-Beispiel f√ºr RAG-Evaluator"""
    import asyncio
    
    # Logging konfigurieren
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    async def test_evaluator():
        """Test-Funktion"""
        print("\nüß™ VERITAS RAG EVALUATOR - Test\n")
        
        # Golden Dataset Pfad
        dataset_path = "backend/evaluation/golden_dataset_examples.json"
        
        # Evaluator erstellen
        evaluator = RAGEvaluator()
        
        # Dataset laden
        try:
            evaluator.load_golden_dataset(dataset_path)
        except FileNotFoundError:
            print(f"‚ö†Ô∏è Dataset nicht gefunden: {dataset_path}")
            print("   Erstelle Mock-Dataset f√ºr Test...")
            
            # Mock-Dataset f√ºr Demo
            mock_dataset = {
                "version": "1.0",
                "test_cases": [
                    {
                        "id": "test_001",
                        "category": "legal",
                        "complexity": "simple",
                        "question": "Test-Frage",
                        "expected_retrieval": {
                            "expected_documents": ["doc1"],
                            "expected_entities": ["Entity 1"],
                            "min_relevance_score": 0.8
                        },
                        "expected_answer": {
                            "must_contain": ["Test"],
                            "must_not_contain": ["Fake"],
                            "min_length": 10
                        },
                        "hallucination_triggers": ["Halluzination"]
                    }
                ]
            }
            evaluator.golden_dataset = mock_dataset
            evaluator.test_cases = mock_dataset['test_cases']
        
        # Evaluation durchf√ºhren
        print("‚ñ∂Ô∏è  Starte Evaluation...\n")
        summary = await evaluator.run_evaluation(verbose=True)
        
        # Summary ausgeben
        evaluator.print_summary()
        
        # Report speichern
        report_path = "backend/evaluation/test_evaluation_report.json"
        evaluator.save_report(report_path)
        print(f"‚úÖ Report gespeichert: {report_path}\n")
    
    # Test ausf√ºhren
    asyncio.run(test_evaluator())
