#!/usr/bin/env python3
"""
VERITAS API Backend - Mit Streaming Progress System
=================================================
Erweiterte Version mit Real-time Progress Updates f√ºr Frontend

Features:
- Server-Sent Events (SSE) f√ºr Progress Updates
- Agent Deep-thinking Zwischenergebnisse
- WebSocket-√§hnliche Real-time Kommunikation
- Frontend Integration f√ºr veritas_app.py

Port: 5000
Dokumentation: http://localhost:5000/docs
"""
import asyncio
import logging
from datetime import datetime
import json
import os
import sys
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any, AsyncGenerator
import time
import uuid

# Import Streaming Progress System
try:
    from shared.pipelines.veritas_streaming_progress import (
        create_progress_manager, create_progress_streamer,
        ProgressStage, ProgressType, VeritasProgressManager, VeritasProgressStreamer
    )
    STREAMING_AVAILABLE = True
except ImportError:
    STREAMING_AVAILABLE = False

# Import UDS3 Core Integration
try:
    import uds3
    from uds3 import (
        create_secure_document_light, 
        get_optimized_unified_strategy,
        UnifiedDatabaseStrategy,
        MULTI_DB_DISTRIBUTION_AVAILABLE
    )
    # Versuche Security-Imports einzeln
    try:
        from uds3.uds3_security_quality import SecurityLevel, QualityMetric
        QualityLevel = QualityMetric  # Alias f√ºr Kompatibilit√§t
    except ImportError:
        try:
            from uds3.uds3_core import SecurityLevel
            QualityLevel = None
        except ImportError:
            SecurityLevel = None
            QualityLevel = None
    
    UDS3_AVAILABLE = True
    logging.info("‚úÖ UDS3 Core erfolgreich importiert - Erweiterte Database Features verf√ºgbar")
except ImportError as e:
    UDS3_AVAILABLE = False
    SecurityLevel = None
    QualityLevel = None
    logging.warning(f"‚ö†Ô∏è UDS3 nicht verf√ºgbar, Fallback zu Standard-Backend: {e}")

# Import Intelligent Multi-Agent Pipeline
try:
    from backend.agents.veritas_intelligent_pipeline import (
        IntelligentMultiAgentPipeline, get_intelligent_pipeline,
        IntelligentPipelineRequest, IntelligentPipelineResponse
    )
    from backend.agents.veritas_ollama_client import VeritasOllamaClient, get_ollama_client
    INTELLIGENT_PIPELINE_AVAILABLE = True
except ImportError:
    try:
        repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
        if repo_root not in sys.path:
            sys.path.append(repo_root)
        from backend.agents.veritas_intelligent_pipeline import (
            IntelligentMultiAgentPipeline, get_intelligent_pipeline,
            IntelligentPipelineRequest, IntelligentPipelineResponse
        )
        from backend.agents.veritas_ollama_client import VeritasOllamaClient, get_ollama_client
        INTELLIGENT_PIPELINE_AVAILABLE = True
    except ImportError as e:
        INTELLIGENT_PIPELINE_AVAILABLE = False
        logging.warning(f"‚ö†Ô∏è Intelligent Pipeline nicht verf√ºgbar: {e}")

# Logging Configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===== PYDANTIC MODELS =====

class VeritasRAGRequest(BaseModel):
    question: str = Field(..., description="Frage f√ºr das RAG-System")
    mode: str = Field(default="VERITAS", description="System-Modus")
    model: Optional[str] = Field(default=None, description="LLM-Modell")
    temperature: float = Field(default=0.7, description="LLM-Temperatur")
    max_tokens: int = Field(default=1000, description="Max. Tokens")
    session_id: Optional[str] = None
    chat_history: Optional[List[Dict[str, str]]] = Field(
        default=None,
        description="Optionale Chat-History f√ºr kontextuelle Antworten. Format: [{'role': 'user'|'assistant', 'content': '...'}]"
    )

class VeritasRAGResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    session_id: str
    mode: str
    quality_score: float
    processing_time: float
    tokens_used: Optional[int] = None
    model_used: Optional[str] = None
    request_id: str

class VeritasStreamingQueryRequest(BaseModel):
    query: str = Field(..., description="Query f√ºr Streaming-Verarbeitung")
    session_id: Optional[str] = None
    enable_streaming: bool = Field(default=True, description="Aktiviere Progress Streaming")
    enable_intermediate_results: bool = Field(default=True, description="Zeige Zwischenergebnisse")
    enable_llm_thinking: bool = Field(default=True, description="Zeige LLM Deep-thinking")

class VeritasAgentQueryRequest(BaseModel):
    query: str = Field(..., description="Agent-Query")
    agent_types: List[str] = Field(default=[], description="Gew√ºnschte Agent-Typen")
    complexity: str = Field(default="standard", description="Query-Komplexit√§t")
    external_sources: bool = Field(default=True, description="Externe Datenquellen nutzen")
    quality_level: str = Field(default="high", description="Qualit√§tslevel")
    session_id: Optional[str] = None

class VeritasAgentQueryResponse(BaseModel):
    answer: str
    agent_results: List[Dict[str, Any]]
    external_data: List[Dict[str, Any]]
    quality_metrics: Dict[str, Any]
    processing_details: Dict[str, Any]
    session_id: str
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())

class StartSessionRequest(BaseModel):
    mode: str = Field(default="VERITAS", description="System-Modus")
    
class StartSessionResponse(BaseModel):
    session_id: str
    mode: str
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())

# ===== UDS3-SPECIFIC MODELS =====

class UDS3SecureDocumentRequest(BaseModel):
    file_path: str = Field(..., description="Pfad zur Quelldatei")
    content: str = Field(..., description="Dokumenteninhalt")
    chunks: List[str] = Field(default=[], description="Text-Chunks f√ºr Vektorisierung")
    security_level: Optional[str] = Field(default="INTERNAL", description="Sicherheitsstufe")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Zus√§tzliche Metadaten")

class UDS3SecureDocumentResponse(BaseModel):
    success: bool
    document_id: Optional[str] = None
    operation_type: str
    timestamp: str
    security_info: Dict[str, Any] = Field(default_factory=dict)
    quality_score: Dict[str, Any] = Field(default_factory=dict)
    validation_results: Dict[str, Any] = Field(default_factory=dict)
    database_operations: Dict[str, Any] = Field(default_factory=dict)
    issues: List[str] = Field(default_factory=list)
    processing_time: float = 0.0

class UDS3QueryRequest(BaseModel):
    query: str = Field(..., description="UDS3-Query")
    query_type: str = Field(default="unified", description="Art der Query (unified, vector, graph, relational)")
    filters: Dict[str, Any] = Field(default_factory=dict, description="Query-Filter")
    security_context: Optional[str] = Field(default=None, description="Sicherheitskontext")

class UDS3QueryResponse(BaseModel):
    success: bool
    results: List[Dict[str, Any]] = Field(default_factory=list)
    total_results: int = 0
    query_info: Dict[str, Any] = Field(default_factory=dict)
    processing_time: float = 0.0
    quality_metrics: Dict[str, Any] = Field(default_factory=dict)

# ===== STREAMING PROGRESS SETUP =====

# Global Progress Manager
progress_manager = None  # Type: Optional[VeritasProgressManager] 
progress_streamer = None  # Type: Optional[VeritasProgressStreamer]

# Global Components
intelligent_pipeline = None
ollama_client = None
uds3_strategy = None

# ===== INITIALIZATION FUNCTIONS =====

def initialize_streaming_system():
    """Initialisiert das Streaming Progress System"""
    global progress_manager, progress_streamer
    
    if STREAMING_AVAILABLE:
        try:
            progress_manager = create_progress_manager()
            progress_streamer = create_progress_streamer(progress_manager)
            logger.info("‚úÖ Streaming Progress System initialisiert")
            return True
        except Exception as e:
            logger.error(f"‚ùå Streaming Initialisierung fehlgeschlagen: {e}")
            return False
    else:
        logger.warning("‚ö†Ô∏è Streaming System nicht verf√ºgbar")
        return False

async def initialize_intelligent_pipeline():
    """Initialisiert die Intelligent Multi-Agent Pipeline"""
    global intelligent_pipeline, ollama_client
    
    if INTELLIGENT_PIPELINE_AVAILABLE:
        try:
            intelligent_pipeline = await get_intelligent_pipeline()
            ollama_client = await get_ollama_client()
            logger.info("‚úÖ Intelligent Multi-Agent Pipeline initialisiert")
            return True
        except Exception as e:
            logger.error(f"‚ùå Intelligent Pipeline Initialisierung fehlgeschlagen: {e}")
            return False
    else:
        logger.warning("‚ö†Ô∏è Intelligent Pipeline nicht verf√ºgbar")
        return False

def initialize_uds3_system():
    """Initialisiert das UDS3 Strategy System"""
    global uds3_strategy
    
    if UDS3_AVAILABLE:
        try:
            uds3_strategy = get_optimized_unified_strategy()
            logger.info("‚úÖ UDS3 Strategy System initialisiert")
            return True
        except Exception as e:
            logger.error(f"‚ùå UDS3 Strategy Initialisierung fehlgeschlagen: {e}")
            return False
    else:
        logger.warning("‚ö†Ô∏è UDS3 System nicht verf√ºgbar")
        return False

# ===== LIFESPAN CONTEXT MANAGER =====

@asynccontextmanager
async def lifespan(app: FastAPI):
    """App Lifespan Management - Ersetzt on_event startup/shutdown
    
    Raises:
        RuntimeError: Wenn kritische Systeme (UDS3, Pipeline) nicht verf√ºgbar sind
    """
    # Startup
    logger.info("üöÄ Veritas API Backend (Streaming + Intelligent Pipeline + UDS3) wird gestartet...")
    
    # Streaming System initialisieren (optional)
    streaming_initialized = initialize_streaming_system()
    
    # UDS3 System initialisieren - ERFORDERLICH!
    uds3_initialized = initialize_uds3_system()
    if not uds3_initialized:
        raise RuntimeError(
            "‚ùå KRITISCHER FEHLER: UDS3 System konnte nicht initialisiert werden!\n"
            "Das Backend kann nicht ohne UDS3-Backend arbeiten.\n"
            "Bitte √ºberpr√ºfen Sie die UDS3-Installation und Konfiguration."
        )
    
    # PHASE 5: Hybrid Search initialisieren (UDS3 Adapter + BM25 + RRF)
    try:
        from backend.api.veritas_phase5_integration import (
            initialize_phase5_hybrid_search, DEMO_CORPUS
        )
        phase5_initialized = await initialize_phase5_hybrid_search(demo_corpus=DEMO_CORPUS)
        if phase5_initialized:
            logger.info("   ‚úÖ Phase 5 Hybrid Search: OK")
        else:
            logger.warning("   ‚ö†Ô∏è Phase 5 Hybrid Search: Disabled (check config)")
    except Exception as e:
        logger.warning(f"   ‚ö†Ô∏è Phase 5 Hybrid Search initialization failed: {e}")
        phase5_initialized = False
    
    # Intelligent Pipeline initialisieren - ERFORDERLICH!
    pipeline_initialized = await initialize_intelligent_pipeline()
    if not pipeline_initialized:
        raise RuntimeError(
            "‚ùå KRITISCHER FEHLER: Intelligent Pipeline konnte nicht initialisiert werden!\n"
            "Das Backend ben√∂tigt die Pipeline f√ºr Query-Verarbeitung.\n"
            "M√∂gliche Ursachen:\n"
            "  - UDS3 nicht verf√ºgbar (bereits gepr√ºft: OK)\n"
            "  - Ollama nicht erreichbar\n"
            "  - Agent-Module fehlen"
        )
    
    # Ollama-Check
    if not ollama_client:
        raise RuntimeError(
            "‚ùå KRITISCHER FEHLER: Ollama Client nicht verf√ºgbar!\n"
            "Das Backend ben√∂tigt Ollama f√ºr LLM-Funktionalit√§t.\n"
            "Bitte stellen Sie sicher, dass Ollama l√§uft (http://localhost:11434)."
        )
    
    logger.info(f"üìä System Status:")
    logger.info(f"   ‚úÖ Streaming Progress: {'OK' if streaming_initialized else 'Nicht verf√ºgbar (optional)'}")
    logger.info(f"   ‚úÖ UDS3 Strategy: OK (ERFORDERLICH)")
    logger.info(f"   ‚úÖ Intelligent Pipeline: OK (ERFORDERLICH)")
    logger.info(f"   ‚úÖ Ollama Client: OK (ERFORDERLICH)")
    logger.info(f"üéâ Backend erfolgreich gestartet - Bereit f√ºr Queries mit ECHTEN Daten (kein Mock-Modus)")
    
    yield  # Server l√§uft
    
    # Shutdown (optional - cleanup code)
    logger.info("üõë Veritas API Backend wird heruntergefahren...")

# ===== FASTAPI APP =====

app = FastAPI(
    title="VERITAS API Backend",
    description="Erweiterte API mit Streaming Progress System, Intelligent Pipeline und UDS3",
    version="1.0.0-streaming-uds3",
    lifespan=lifespan
)

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===== ROUTER INCLUDES =====

# ‚ú® NEW v3.17.0: IMMI Geodaten-Router (Immissionsschutz)
try:
    from backend.api.immi_endpoints import router as immi_router
    app.include_router(immi_router)
    logger.info("‚úÖ IMMI-Router integriert: /api/immi/* (BImSchG + WKA Geodaten)")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è IMMI-Router nicht verf√ºgbar: {e}")

# ‚ú® NEW v3.16.0: Feedback System Router
try:
    from backend.api.feedback_routes import router as feedback_router
    app.include_router(feedback_router)
    logger.info("‚úÖ Feedback-Router integriert: /api/feedback/*")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Feedback-Router nicht verf√ºgbar: {e}")

# ===== CORE ENDPOINTS =====

@app.get("/")
async def root():
    """Root Endpoint - API Status"""
    return {
        "message": "Veritas API Backend (Streaming + UDS3 + IMMI + Feedback)",
        "version": "1.0.0-streaming-uds3-immi-feedback",
        "status": "active",
        "streaming_available": STREAMING_AVAILABLE,
        "uds3_available": UDS3_AVAILABLE,
        "intelligent_pipeline_available": INTELLIGENT_PIPELINE_AVAILABLE,
        "endpoints": {
            "chat": "/v2/query",
            "streaming_chat": "/v2/query/stream",
            "intelligent_chat": "/v2/intelligent/query",
            "uds3_create": "/uds3/documents",
            "uds3_query": "/uds3/query",
            "progress": "/progress/{session_id}",
            "rag": "/ask", 
            "agents": "/agents/ask",
            "immi_bimschg": "/api/immi/markers/bimschg",  # NEW
            "immi_wka": "/api/immi/markers/wka",  # NEW
            "immi_search": "/api/immi/search",  # NEW
            "feedback": "/api/feedback/submit",
            "feedback_stats": "/api/feedback/stats",
            "docs": "/docs"
        }
    }

@app.get("/health")
async def health_check():
    """Health Check Endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "streaming_available": STREAMING_AVAILABLE,
        "intelligent_pipeline_available": INTELLIGENT_PIPELINE_AVAILABLE,
        "uds3_available": UDS3_AVAILABLE,
        "uds3_multi_db_distribution": MULTI_DB_DISTRIBUTION_AVAILABLE if UDS3_AVAILABLE else False,
        "ollama_available": ollama_client is not None if INTELLIGENT_PIPELINE_AVAILABLE else False
    }

@app.get("/capabilities")
async def get_capabilities():
    """
    System Capabilities Endpoint f√ºr Frontend
    Gibt detaillierte Informationen √ºber verf√ºgbare Features zur√ºck
    """
    # Pr√ºfe Ollama-Status
    ollama_status = {
        "available": False,
        "models": [],
        "endpoint": "http://localhost:11434"
    }
    
    if INTELLIGENT_PIPELINE_AVAILABLE and ollama_client:
        try:
            # Hole verf√ºgbare Modelle aus dem Client
            models_dict = ollama_client.available_models
            model_names = list(models_dict.keys()) if models_dict else []
            
            ollama_status = {
                "available": len(model_names) > 0 and not ollama_client.offline_mode,
                "models": model_names,
                "model_count": len(model_names),
                "endpoint": "http://localhost:11434",
                "default_model": ollama_client.default_model,
                "offline_mode": ollama_client.offline_mode
            }
        except Exception as e:
            logger.warning(f"Ollama-Abfrage fehlgeschlagen: {e}")
            ollama_status["available"] = False
            ollama_status["error"] = str(e)
    
    # Pr√ºfe UDS3-Status
    uds3_capabilities = {
        "available": UDS3_AVAILABLE,
        "multi_db_distribution": MULTI_DB_DISTRIBUTION_AVAILABLE if UDS3_AVAILABLE else False,
        "databases": []
    }
    
    if UDS3_AVAILABLE and uds3_strategy:
        try:
            # Hole verf√ºgbare Datenbanken
            if hasattr(uds3_strategy, 'get_available_databases'):
                uds3_capabilities["databases"] = uds3_strategy.get_available_databases()
            else:
                uds3_capabilities["databases"] = ["vector", "graph", "relational"]  # Standard
        except Exception as e:
            logger.warning(f"UDS3-Abfrage fehlgeschlagen: {e}")
    
    # Pr√ºfe Intelligent Pipeline Status
    pipeline_capabilities = {
        "available": INTELLIGENT_PIPELINE_AVAILABLE,
        "initialized": intelligent_pipeline is not None,
        "features": []
    }
    
    if INTELLIGENT_PIPELINE_AVAILABLE and intelligent_pipeline:
        pipeline_capabilities["features"] = [
            "multi_agent_orchestration",
            "rag_based_agent_selection",
            "llm_commentary",
            "parallel_execution",
            "confidence_scoring",
            "follow_up_suggestions"
        ]
        
        # Hole Agent-Informationen
        try:
            if hasattr(intelligent_pipeline, 'get_available_agents'):
                pipeline_capabilities["available_agents"] = intelligent_pipeline.get_available_agents()
            else:
                pipeline_capabilities["available_agents"] = [
                    "geo_context",
                    "legal_framework", 
                    "document_retrieval",
                    "financial_analysis",
                    "environmental_assessment",
                    "social_impact",
                    "construction_management",
                    "traffic_planning"
                ]
        except Exception as e:
            logger.warning(f"Agent-Abfrage fehlgeschlagen: {e}")
    
    # Streaming Capabilities
    streaming_capabilities = {
        "available": STREAMING_AVAILABLE,
        "endpoints": ["/v2/query/stream", "/v2/intelligent/query"] if STREAMING_AVAILABLE else [],
        "features": ["progress_updates", "intermediate_results", "llm_thinking"] if STREAMING_AVAILABLE else []
    }
    
    # System-weite Capabilities
    return {
        "system": {
            "version": "1.0.0-production",
            "environment": "production",
            "timestamp": datetime.now().isoformat(),
            "uptime_seconds": (datetime.now() - datetime.fromtimestamp(0)).total_seconds()  # Placeholder
        },
        "endpoints": {
            "chat": {
                "path": "/v2/query",
                "available": True,
                "production_ready": True,
                "uses_intelligent_pipeline": INTELLIGENT_PIPELINE_AVAILABLE
            },
            "streaming_chat": {
                "path": "/v2/query/stream",
                "available": STREAMING_AVAILABLE,
                "production_ready": STREAMING_AVAILABLE
            },
            "intelligent_query": {
                "path": "/v2/intelligent/query",
                "available": INTELLIGENT_PIPELINE_AVAILABLE,
                "production_ready": INTELLIGENT_PIPELINE_AVAILABLE
            },
            "rag": {
                "path": "/ask",
                "available": True,
                "production_ready": INTELLIGENT_PIPELINE_AVAILABLE
            },
            "uds3_documents": {
                "path": "/uds3/documents",
                "available": UDS3_AVAILABLE,
                "production_ready": UDS3_AVAILABLE
            },
            "uds3_query": {
                "path": "/uds3/query",
                "available": UDS3_AVAILABLE,
                "production_ready": UDS3_AVAILABLE
            }
        },
        "features": {
            "ollama": ollama_status,
            "uds3": uds3_capabilities,
            "intelligent_pipeline": pipeline_capabilities,
            "streaming": streaming_capabilities
        },
        "modes": {
            "veritas": {
                "available": True,
                "requires": ["intelligent_pipeline"],
                "optimal": INTELLIGENT_PIPELINE_AVAILABLE
            },
            "chat": {
                "available": True,
                "requires": ["ollama"],
                "optimal": ollama_status["available"]
            },
            "vpb": {
                "available": True,
                "requires": ["intelligent_pipeline", "uds3"],
                "optimal": INTELLIGENT_PIPELINE_AVAILABLE and UDS3_AVAILABLE
            },
            "covina": {
                "available": UDS3_AVAILABLE,
                "requires": ["uds3", "intelligent_pipeline"],
                "optimal": UDS3_AVAILABLE and INTELLIGENT_PIPELINE_AVAILABLE,
                "status": "experimental"
            }
        },
        "recommendations": _generate_recommendations(
            ollama_status["available"],
            UDS3_AVAILABLE,
            INTELLIGENT_PIPELINE_AVAILABLE
        )
    }


def _generate_recommendations(ollama_available: bool, uds3_available: bool, pipeline_available: bool) -> list:
    """Generiert Empfehlungen basierend auf System-Status"""
    recommendations = []
    
    if not ollama_available:
        recommendations.append({
            "type": "warning",
            "message": "Ollama nicht verf√ºgbar - LLM-Features eingeschr√§nkt",
            "action": "Starten Sie Ollama: http://localhost:11434"
        })
    
    if not pipeline_available:
        recommendations.append({
            "type": "error",
            "message": "Intelligent Pipeline nicht initialisiert",
            "action": "Backend neu starten oder Logs pr√ºfen"
        })
    
    if not uds3_available:
        recommendations.append({
            "type": "info",
            "message": "UDS3 nicht verf√ºgbar - Erweiterte Datenbank-Features deaktiviert",
            "action": "Optional: UDS3 installieren f√ºr Multi-DB Support"
        })
    
    if ollama_available and pipeline_available and uds3_available:
        recommendations.append({
            "type": "success",
            "message": "Alle Features verf√ºgbar - System voll funktionsf√§hig",
            "action": "Keine Aktion erforderlich"
        })
    
    return recommendations

# ===== INTELLIGENT MULTI-AGENT PIPELINE ENDPOINTS =====

@app.post("/v2/intelligent/query")
async def veritas_intelligent_query(request: VeritasStreamingQueryRequest):
    """
    üß† INTELLIGENT MULTI-AGENT PIPELINE ENDPOINT
    
    Verarbeitet Query durch intelligente Pipeline mit:
    - Real-time LLM-Kommentaren f√ºr jeden Step
    - RAG-basierte Agent-Selektion  
    - Parallele Agent-Execution
    - LLM-basierte Result-Synthesis
    
    Features:
    - Ollama LLM Integration f√ºr Kommentierung
    - Multi-Agent Orchestration
    - Confidence Scoring
    - Follow-up Suggestions
    """
    
    if not INTELLIGENT_PIPELINE_AVAILABLE or not intelligent_pipeline:
        raise HTTPException(
            status_code=503, 
            detail="Intelligent Multi-Agent Pipeline nicht verf√ºgbar"
        )
    
    query_id = f"intelligent_query_{int(time.time())}_{uuid.uuid4().hex[:8]}"
    session_id = request.session_id or str(uuid.uuid4())
    
    try:
        # Intelligent Pipeline Request erstellen
        pipeline_request = IntelligentPipelineRequest(
            query_id=query_id,
            query_text=request.query,
            user_context={"session_id": session_id},
            session_id=session_id,
            enable_llm_commentary=request.enable_llm_thinking,
            enable_real_time_updates=request.enable_streaming,
            max_parallel_agents=5,
            timeout=60
        )
        
        # Pipeline ausf√ºhren
        start_time = time.time()
        pipeline_response = await intelligent_pipeline.process_intelligent_query(pipeline_request)
        processing_time = time.time() - start_time
        
        # Response formatieren
        response = {
            "query_id": query_id,
            "session_id": session_id,
            "answer": pipeline_response.response_text,
            "confidence_score": pipeline_response.confidence_score,
            "processing_time": processing_time,
            "model_used": "Ollama Multi-Agent Pipeline",
            "mode": "INTELLIGENT_PIPELINE",
            
            # Multi-Agent Details
            "agent_results": pipeline_response.agent_results,
            "agents_used": len(pipeline_response.agent_results),
            "sources": pipeline_response.sources,
            "rag_context": pipeline_response.rag_context,
            
            # LLM Commentary
            "llm_commentary": pipeline_response.llm_commentary,
            "pipeline_steps": len(pipeline_response.llm_commentary),
            
            # Suggestions & Metadata  
            "follow_up_suggestions": pipeline_response.follow_up_suggestions,
            "processing_metadata": pipeline_response.processing_metadata,
            
            # Quality Metrics
            "quality_metrics": {
                "confidence_score": pipeline_response.confidence_score,
                "processing_time": processing_time,
                "agents_success_rate": 1.0,  # Mock f√ºr jetzt
                "sources_found": len(pipeline_response.sources)
            },
            
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"‚úÖ Intelligent Query verarbeitet: {query_id} ({processing_time:.2f}s)")
        return response
        
    except Exception as e:
        logger.error(f"‚ùå Intelligent Query fehlgeschlagen: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Intelligent Pipeline Fehler: {str(e)}"
        )

@app.get("/v2/intelligent/status")
async def intelligent_pipeline_status():
    """Status der Intelligent Multi-Agent Pipeline"""
    
    if not INTELLIGENT_PIPELINE_AVAILABLE:
        return {"status": "unavailable", "reason": "Pipeline nicht geladen"}
    
    if not intelligent_pipeline:
        return {"status": "not_initialized", "reason": "Pipeline nicht initialisiert"}
    
    # Pipeline Statistics
    stats = intelligent_pipeline.get_pipeline_statistics()
    
    # Ollama Client Status
    ollama_stats = {}
    if ollama_client:
        ollama_stats = ollama_client.get_client_statistics()
    
    return {
        "status": "active",
        "pipeline_stats": stats,
        "ollama_stats": ollama_stats,
        "timestamp": datetime.now().isoformat()
    }

# ===== STREAMING ENDPOINTS =====

@app.post("/v2/query/stream")
async def veritas_streaming_query(request: VeritasStreamingQueryRequest):
    """
    STREAMING ENDPOINT f√ºr Real-time Progress Updates
    Startet Verarbeitung und gibt Stream-URL zur√ºck
    """
    session_id = request.session_id or str(uuid.uuid4())
    query_id = f"stream_query_{int(time.time())}_{uuid.uuid4().hex[:8]}"
    
    if not STREAMING_AVAILABLE or not progress_manager:
        raise HTTPException(status_code=503, detail="Streaming System nicht verf√ºgbar")
    
    # Progress Session starten
    progress_manager.start_session(
        session_id=session_id,
        query_id=query_id,
        query_text=request.query
    )
    
    # Starte Async Processing
    asyncio.create_task(
        _process_streaming_query(session_id, query_id, request)
    )
    
    return {
        "session_id": session_id,
        "query_id": query_id,
        "stream_url": f"/progress/{session_id}",
        "message": "Verarbeitung gestartet - verbinde mit Stream f√ºr Updates",
        "estimated_time": "5-15 Sekunden"
    }

@app.get("/progress/{session_id}")
async def get_progress_stream(session_id: str):
    """
    Server-Sent Events Stream f√ºr Progress Updates
    Frontend kann hier f√ºr Real-time Updates subscriben
    """
    if not STREAMING_AVAILABLE or not progress_streamer:
        raise HTTPException(status_code=503, detail="Streaming System nicht verf√ºgbar")
    
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
    }
    
    return StreamingResponse(
        progress_streamer.create_progress_stream(session_id),
        media_type="text/event-stream",
        headers=headers
    )

async def _process_streaming_query(session_id: str, query_id: str, request: VeritasStreamingQueryRequest):
    """
    Asynchrone Query-Verarbeitung mit Progress Updates
    Simuliert komplexes Agent-Processing mit Real-time Updates
    """
    try:
        # 1. Query Analysis Stage
        progress_manager.update_stage(
            session_id, 
            ProgressStage.ANALYZING_QUERY,
            {"query_length": len(request.query), "complexity_detection": "in_progress"}
        )
        await asyncio.sleep(1.0)
        
        # Analysiere Query
        complexity = _analyze_query_complexity(request.query)
        domain = _analyze_query_domain(request.query)
        
        # 2. Agent Selection Stage
        selected_agents = _select_agents_for_query(request.query, complexity, domain)
        progress_manager.update_stage(
            session_id,
            ProgressStage.SELECTING_AGENTS,
            {
                "selected_agents": selected_agents,
                "complexity": complexity,
                "domain": domain
            }
        )
        await asyncio.sleep(0.5)
        
        # 3. Agent Processing Stage
        progress_manager.update_stage(session_id, ProgressStage.AGENT_PROCESSING)
        
        agent_results = {}
        for i, agent_type in enumerate(selected_agents):
            # Pr√ºfe Cancellation vor jedem Agent
            if progress_manager.is_session_cancelled(session_id):
                logger.info(f"üõë Session {session_id} abgebrochen - stoppe Agent-Verarbeitung")
                return
            
            # Agent startet
            progress_manager.update_agent_progress(
                session_id, agent_type, ProgressType.AGENT_START
            )
            
            # Simuliere Agent-Verarbeitung mit Cancellation-Checks
            sleep_duration = 1.0 + (i * 0.5)
            sleep_steps = int(sleep_duration / 0.2)  # Check every 200ms
            
            for step in range(sleep_steps):
                if progress_manager.is_session_cancelled(session_id):
                    logger.info(f"üõë Session {session_id} abgebrochen w√§hrend {agent_type}")
                    return
                await asyncio.sleep(0.2)
            
            # Agent-Result generieren
            agent_result = _generate_agent_result(agent_type, request.query, complexity)
            agent_results[agent_type] = agent_result
            
            # Zwischenergebnis hinzuf√ºgen (falls aktiviert)
            if request.enable_intermediate_results:
                progress_manager.add_intermediate_result(
                    session_id=session_id,
                    result_type=f"{agent_type}_analysis",
                    content=agent_result.get('summary', 'Analyse abgeschlossen'),
                    confidence=agent_result.get('confidence_score', 0.8),
                    sources=agent_result.get('sources', [])
                )
            
            # Agent abgeschlossen
            progress_manager.update_agent_progress(
                session_id, agent_type, ProgressType.AGENT_COMPLETE,
                result=agent_result
            )
        
        # 4. Context Gathering Stage
        if progress_manager.is_session_cancelled(session_id):
            return
            
        progress_manager.update_stage(session_id, ProgressStage.GATHERING_CONTEXT)
        await asyncio.sleep(1.0)
        
        # 5. LLM Reasoning Stage (falls aktiviert)
        if request.enable_llm_thinking and not progress_manager.is_session_cancelled(session_id):
            progress_manager.update_stage(session_id, ProgressStage.LLM_REASONING)
            
            thinking_steps = [
                "Analysiere gesammelte Informationen",
                "Bewerte Relevanz und Vertrauensw√ºrdigkeit", 
                "Identifiziere Wissensl√ºcken",
                "Strukturiere finale Antwort",
                "√úberpr√ºfe Konsistenz und Vollst√§ndigkeit"
            ]
            
            for step in thinking_steps:
                if progress_manager.is_session_cancelled(session_id):
                    return
                    
                progress_manager.add_llm_thinking_step(
                    session_id, step, f"LLM verarbeitet: {step}"
                )
                await asyncio.sleep(0.8)
        
        # 6. Synthesis Stage
        if progress_manager.is_session_cancelled(session_id):
            return
            
        progress_manager.update_stage(session_id, ProgressStage.SYNTHESIZING)
        await asyncio.sleep(1.0)
        
        # 7. Final Check vor Completion
        if progress_manager.is_session_cancelled(session_id):
            return
        
        # Generate final response
        final_response = _synthesize_final_response(request.query, agent_results, complexity, domain)
        
        # 7. Finalization
        progress_manager.update_stage(session_id, ProgressStage.FINALIZING)
        await asyncio.sleep(0.5)
        
        # Complete session
        progress_manager.complete_session(session_id, final_response)
        
    except Exception as e:
        logger.error(f"‚ùå Streaming Query Error: {e}")
        progress_manager.update_stage(session_id, ProgressStage.ERROR)
        progress_manager.complete_session(session_id, {"error": str(e)})

def _select_agents_for_query(query: str, complexity: str, domain: str) -> List[str]:
    """W√§hlt Agenten basierend auf Query aus"""
    base_agents = ['geo_context', 'legal_framework']
    
    # Domain-spezifische Agenten
    domain_agents = {
        'building': ['construction', 'document_retrieval'],
        'environmental': ['environmental', 'external_api'],
        'transport': ['traffic', 'external_api'],
        'business': ['financial', 'document_retrieval'],
        'general': ['document_retrieval']
    }
    
    selected = base_agents + domain_agents.get(domain, ['document_retrieval'])
    
    # Komplexit√§ts-basierte Erweiterung
    if complexity == 'advanced':
        selected.append('financial')
        selected.append('social')
    
    return list(set(selected))  # Remove duplicates

def _generate_agent_result(agent_type: str, query: str, complexity: str) -> Dict[str, Any]:
    """Generiert simuliertes Agent-Ergebnis"""
    
    base_confidence = 0.8 if complexity == 'basic' else 0.75 if complexity == 'standard' else 0.7
    
    agent_specialties = {
        'geo_context': {
            'summary': 'Geografischer Kontext und lokale Bestimmungen identifiziert',
            'details': 'Relevante Gebiets- und Standortinformationen gesammelt',
            'sources': ['OpenStreetMap', 'Gemeinde-DB', 'Geoportal']
        },
        'legal_framework': {
            'summary': 'Rechtliche Rahmenbedingungen und Vorschriften analysiert',
            'details': 'Aktuelle Gesetze und Verordnungen ausgewertet',
            'sources': ['BauGB', 'VwVfG', 'GemO', 'Landesrecht']
        },
        'construction': {
            'summary': 'Bautechnische Aspekte und Genehmigungsverfahren bewertet',
            'details': 'Bauvorschriften und technische Anforderungen gepr√ºft',
            'sources': ['DIN-Normen', 'Bauordnung', 'Technische Richtlinien']
        },
        'environmental': {
            'summary': 'Umweltaspekte und Emissionsbestimmungen untersucht',
            'details': 'Umweltschutzauflagen und Grenzwerte ermittelt',
            'sources': ['Umweltbundesamt', 'Luftreinhaltepl√§ne', 'EU-Richtlinien']
        },
        'financial': {
            'summary': 'Kostenstrukturen und finanzielle Aspekte kalkuliert',
            'details': 'Geb√ºhren, Kosten und F√∂rderungsm√∂glichkeiten analysiert',
            'sources': ['Geb√ºhrenordnung', 'F√∂rderdatenbank', 'Kostensch√§tzungen']
        },
        'traffic': {
            'summary': 'Verkehrsrechtliche Bestimmungen und Infrastruktur bewertet',
            'details': 'Verkehrsregeln und Infrastrukturanforderungen gepr√ºft',
            'sources': ['StVO', 'Verkehrsbeh√∂rde', '√ñPNV-Pl√§ne']
        },
        'document_retrieval': {
            'summary': 'Relevante Dokumente und Formulare gefunden',
            'details': 'Antragsformulare und Informationsmaterialien identifiziert',
            'sources': ['Verwaltungsportal', 'Formulardatenbank', 'FAQ-Sammlung']
        },
        'external_api': {
            'summary': 'Aktuelle externe Daten abgerufen',
            'details': 'Live-Daten und externe Informationsquellen ausgewertet',
            'sources': ['API-Services', 'Open-Data-Portale', 'Echtzeitdaten']
        }
    }
    
    specialty = agent_specialties.get(agent_type, {
        'summary': f'{agent_type} Analyse abgeschlossen',
        'details': f'Spezifische {agent_type} Verarbeitung durchgef√ºhrt',
        'sources': ['Standard-Quellen']
    })
    
    return {
        'agent_type': agent_type,
        'confidence_score': base_confidence + (0.1 * hash(query + agent_type) % 3 / 10),
        'processing_time': 1.0 + (hash(agent_type) % 10 / 10),
        'summary': specialty['summary'],
        'details': specialty['details'],
        'sources': specialty['sources'],
        'status': 'completed'
    }

def _synthesize_final_response(query: str, agent_results: Dict[str, Any], complexity: str, domain: str) -> Dict[str, Any]:
    """Generiert finale synthetisierte Antwort"""
    
    # Sammle beste Ergebnisse
    high_confidence_results = [
        result for result in agent_results.values() 
        if result.get('confidence_score', 0) > 0.75
    ]
    
    # Generiere Hauptantwort
    main_response = f"""
**Antwort auf Ihre Frage**: {query}

**Zusammenfassung der Analyse** ({domain.title()}, {complexity.title()}):

"""
    
    for agent_type, result in agent_results.items():
        confidence = result.get('confidence_score', 0)
        confidence_icon = "üü¢" if confidence > 0.8 else "üü°" if confidence > 0.7 else "üî¥"
        main_response += f"{confidence_icon} **{agent_type.replace('_', ' ').title()}**: {result.get('summary', 'Verarbeitung abgeschlossen')}\n\n"
    
    # Sammle alle Quellen
    all_sources = []
    for result in agent_results.values():
        all_sources.extend(result.get('sources', []))
    
    unique_sources = list(set(all_sources))[:10]  # Limitiere auf 10
    
    main_response += f"""
**N√§chste Schritte**: Basierend auf der Analyse empfehlen wir Ihnen, sich zun√§chst √ºber die spezifischen Anforderungen zu informieren und die entsprechenden Antragsformulare zu beschaffen.

**Hinweis**: Diese Antwort wurde durch {len(agent_results)} spezialisierte Agenten erstellt und mit einem durchschnittlichen Vertrauenswert von {sum(r.get('confidence_score', 0) for r in agent_results.values()) / len(agent_results):.0%} bewertet.
"""
    
    return {
        'response_text': main_response.strip(),
        'confidence_score': sum(r.get('confidence_score', 0) for r in agent_results.values()) / len(agent_results),
        'sources': [{'title': source, 'url': f'test://{source.lower().replace(" ", "_")}', 'relevance': 0.8} for source in unique_sources],
        'agent_results': agent_results,
        'processing_metadata': {
            'complexity': complexity,
            'domain': domain,
            'agent_count': len(agent_results),
            'high_confidence_count': len(high_confidence_results),
            'processing_method': 'streaming_synthesis'
        }
    }

# ===== VERITAS CHAT-APP INTEGRATION (Legacy-kompatibel) =====

@app.post("/v2/query")
async def veritas_chat_query(query_data: Dict[str, Any]):
    """
    MAIN ENDPOINT f√ºr Veritas Chat-App Integration
    PRODUKTIV: Nutzt IntelligentMultiAgentPipeline mit echten LLM-Anfragen
    """
    start_time = time.time()
    
    try:
        query_text = query_data.get('query', '')
        if not query_text:
            raise HTTPException(status_code=400, detail="Query ist erforderlich")
        
        session_id = query_data.get('session_id', str(uuid.uuid4()))
        enable_streaming = query_data.get('enable_streaming', False)
        mode = query_data.get('mode', 'veritas')
        
        # Falls Streaming gew√ºnscht, delegiere an Streaming-Endpoint
        if enable_streaming and STREAMING_AVAILABLE:
            streaming_request = VeritasStreamingQueryRequest(
                query=query_text,
                session_id=session_id,
                enable_streaming=True,
                enable_intermediate_results=True,
                enable_llm_thinking=True
            )
            return await veritas_streaming_query(streaming_request)
        
        # PRODUKTIV: Nutze Intelligent Pipeline statt Mock-Daten
        if INTELLIGENT_PIPELINE_AVAILABLE and intelligent_pipeline:
            logger.info(f"üß† Nutze Intelligent Pipeline f√ºr Query: {query_text[:50]}...")
            
            # Erstelle Pipeline Request
            query_id = f"query_{uuid.uuid4().hex[:8]}"
            pipeline_request = IntelligentPipelineRequest(
                query_id=query_id,
                query_text=query_text,
                user_context={
                    "session_id": session_id,
                    "mode": mode,
                    "frontend_version": "3.5.0"
                },
                session_id=session_id,
                enable_llm_commentary=False,  # Schnellere Antworten
                enable_real_time_updates=False,
                max_parallel_agents=5,
                timeout=60
            )
            
            try:
                # Pipeline ausf√ºhren - ECHTE LLM-Integration!
                pipeline_response = await intelligent_pipeline.process_intelligent_query(pipeline_request)
                processing_time = time.time() - start_time
                
                # Response im erwarteten Frontend-Format
                chat_response = {
                    'response_text': pipeline_response.response_text,
                    'confidence_score': pipeline_response.confidence_score,
                    'sources': pipeline_response.sources,
                    'worker_results': pipeline_response.agent_results,
                    'agent_results': pipeline_response.agent_results,
                    'rag_context': pipeline_response.rag_context,
                    'follow_up_suggestions': pipeline_response.follow_up_suggestions,
                    'processing_metadata': {
                        'query_id': query_id,
                        'complexity': 'intelligent',
                        'processing_time': processing_time,
                        'agent_count': len(pipeline_response.agent_results),
                        'successful_agents': len(pipeline_response.agent_results),
                        'system_mode': 'intelligent_pipeline_production',
                        'streaming_available': STREAMING_AVAILABLE,
                        'ollama_available': ollama_client is not None,
                        'timestamp': datetime.now().isoformat()
                    }
                }
                
                logger.info(f"‚úÖ Intelligent Pipeline Response: {processing_time:.2f}s, "
                           f"{len(pipeline_response.agent_results)} agents, "
                           f"confidence: {pipeline_response.confidence_score:.2%}")
                
                return chat_response
                
            except Exception as pipeline_error:
                logger.error(f"‚ùå Intelligent Pipeline Fehler: {pipeline_error}")
                # Fallback zu Basic Response
                return _generate_basic_response(query_text, session_id, str(pipeline_error))
        
        else:
            # FALLBACK: Wenn Pipeline nicht verf√ºgbar
            logger.warning("‚ö†Ô∏è Intelligent Pipeline nicht verf√ºgbar - nutze Basic Response")
            return _generate_basic_response(query_text, session_id, "Pipeline nicht initialisiert")
        
    except Exception as e:
        logger.error(f"‚ùå Fehler bei Chat-Query: {e}")
        return _generate_error_response(query_text, session_id, str(e))


def _generate_basic_response(query_text: str, session_id: str, reason: str) -> Dict[str, Any]:
    """Generiert Basic Response wenn Pipeline nicht verf√ºgbar"""
    processing_time = 0.01
    
    return {
        'response_text': f"""Ihre Anfrage wurde empfangen: "{query_text}"

‚ö†Ô∏è **Hinweis**: Die intelligente Pipeline ist derzeit nicht verf√ºgbar.
Grund: {reason}

Bitte stellen Sie sicher, dass:
‚Ä¢ Ollama l√§uft (http://localhost:11434)
‚Ä¢ LLM-Modelle geladen sind (llama3.1:latest)
‚Ä¢ Die Intelligent Pipeline initialisiert ist

Kontaktieren Sie den Administrator f√ºr Unterst√ºtzung.""",
        'confidence_score': 0.0,
        'sources': [],
        'worker_results': {},
        'agent_results': {},
        'rag_context': {'query_type': 'fallback', 'pipeline_available': False},
        'follow_up_suggestions': [
            'Pr√ºfen Sie die Ollama-Installation',
            '√úberpr√ºfen Sie die Backend-Logs',
            'Kontaktieren Sie den Support'
        ],
        'processing_metadata': {
            'complexity': 'fallback',
            'processing_time': processing_time,
            'agent_count': 0,
            'successful_agents': 0,
            'system_mode': 'fallback_basic',
            'streaming_available': STREAMING_AVAILABLE,
            'pipeline_error': reason,
            'timestamp': datetime.now().isoformat()
        }
    }


def _generate_error_response(query_text: str, session_id: str, error_message: str) -> Dict[str, Any]:
    """Generiert Error Response"""
    return {
        'response_text': f"""‚ùå **Fehler bei der Verarbeitung**

Ein Fehler ist aufgetreten: {error_message}

Ihre Anfrage: "{query_text[:100]}..."

Bitte versuchen Sie es erneut oder kontaktieren Sie den Support.""",
        'confidence_score': 0.0,
        'sources': [],
        'worker_results': {},
        'processing_metadata': {
            'error': error_message,
            'processing_time': 0.0,
            'system_mode': 'error_fallback',
            'timestamp': datetime.now().isoformat()
        }
    }

def _analyze_query_complexity(query_text: str) -> str:
    """Einfache Query-Komplexit√§ts-Analyse"""
    query_lower = query_text.lower()
    
    if any(word in query_lower for word in ['vergleichen', 'analysieren', 'bewerten', 'wahrscheinlichkeit']):
        return 'advanced'
    elif any(word in query_lower for word in ['wie', 'welche', 'genehmigung', 'kosten']):
        return 'standard'
    else:
        return 'basic'

def _analyze_query_domain(query_text: str) -> str:
    """Einfache Query-Domain-Analyse"""
    query_lower = query_text.lower()
    
    if any(word in query_lower for word in ['bau', 'genehmigung', 'planung', 'geb√§ude']):
        return 'building'
    elif any(word in query_lower for word in ['verkehr', 'parken', 'stra√üe']):
        return 'transport'
    elif any(word in query_lower for word in ['umwelt', 'l√§rm', 'luft']):
        return 'environmental'
    elif any(word in query_lower for word in ['gewerbe', 'gesch√§ft', 'betrieb']):
        return 'business'
    else:
        return 'general'

# ===== PROGRESS ENDPOINTS =====

@app.post("/cancel/{session_id}")
async def cancel_streaming_session(session_id: str):
    """
    Bricht eine aktive Streaming-Session ab
    """
    try:
        if not STREAMING_AVAILABLE or not progress_manager:
            raise HTTPException(status_code=503, detail="Streaming System nicht verf√ºgbar")
        
        # Pr√ºfe ob Session existiert
        session_progress = progress_manager.get_session_progress(session_id)
        
        if not session_progress:
            # Session existiert nicht (mehr)
            return {
                "success": True,
                "message": "Session nicht gefunden oder bereits beendet",
                "session_id": session_id,
                "status": "not_found"
            }
        
        # Pr√ºfe aktuellen Status
        current_stage = session_progress.get('current_stage', 'unknown')
        
        if current_stage in ['completed', 'error', 'cancelled']:
            return {
                "success": True,
                "message": f"Session bereits beendet (Status: {current_stage})",
                "session_id": session_id,
                "status": current_stage
            }
        
        # Session abbrechen
        progress_manager.cancel_session(session_id, "user_cancelled")
        
        logger.info(f"üõë Session {session_id} wurde vom Benutzer abgebrochen")
        
        return {
            "success": True,
            "message": "Session erfolgreich abgebrochen",
            "session_id": session_id,
            "status": "cancelled",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Fehler beim Abbrechen der Session {session_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Fehler beim Abbrechen: {str(e)}")

@app.get("/progress/status/{session_id}")
async def get_session_progress(session_id: str):
    """Holt aktuellen Progress-Status f√ºr Session"""
    if not STREAMING_AVAILABLE or not progress_manager:
        raise HTTPException(status_code=503, detail="Streaming System nicht verf√ºgbar")
    
    progress_status = progress_manager.get_session_progress(session_id)
    
    if not progress_status:
        raise HTTPException(status_code=404, detail="Session nicht gefunden")
    
    return progress_status

# ===== RAG & AGENT ENDPOINTS (Legacy-kompatibel) =====

@app.post("/ask", response_model=VeritasRAGResponse)
async def veritas_rag_query(request: VeritasRAGRequest):
    """
    Standard RAG Query - PRODUKTIV
    Nutzt IntelligentMultiAgentPipeline f√ºr echte RAG-Queries
    
    üÜï Unterst√ºtzt Chat-History f√ºr kontextuelle Antworten
    """
    session_id = request.session_id or str(uuid.uuid4())
    request_id = str(uuid.uuid4())
    start_time = time.time()
    
    try:
        # üÜï CONTEXT-INTEGRATION: Chat-History verarbeiten
        enriched_question = request.question
        context_metadata = {}
        
        if request.chat_history and len(request.chat_history) > 0:
            try:
                from backend.agents.context_manager import ConversationContextManager
                
                # Mock ChatSession aus History erstellen
                from shared.chat_schema import ChatSession, ChatMessage
                mock_session = ChatSession(
                    session_id=session_id,
                    llm_model=request.model or "llama3.1:8b"
                )
                
                # History-Messages hinzuf√ºgen
                for msg in request.chat_history:
                    mock_session.add_message(
                        role=msg.get('role', 'user'),
                        content=msg.get('content', '')
                    )
                
                # Context erstellen
                context_manager = ConversationContextManager(max_tokens=2000)
                context_result = context_manager.build_conversation_context(
                    chat_session=mock_session,
                    current_query=request.question,
                    strategy="sliding_window",
                    max_messages=10
                )
                
                conversation_context = context_result.get('context', '')
                
                if conversation_context:
                    # Erweiterte Frage mit Context
                    enriched_question = f"""Bisherige Konversation:
{conversation_context}

Aktuelle Frage:
{request.question}"""
                    
                    context_metadata = {
                        'context_enabled': True,
                        'context_messages': context_result.get('message_count', 0),
                        'context_tokens': context_result.get('token_count', 0),
                        'context_strategy': context_result.get('strategy_used', 'none')
                    }
                    
                    logger.info(
                        f"üìù Chat-Context integriert: {context_result.get('message_count', 0)} Messages, "
                        f"{context_result.get('token_count', 0)} Tokens"
                    )
                else:
                    context_metadata = {'context_enabled': False, 'reason': 'no_context_generated'}
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Context-Integration fehlgeschlagen: {e}")
                context_metadata = {'context_enabled': False, 'error': str(e)}
        else:
            context_metadata = {'context_enabled': False, 'reason': 'no_history_provided'}
        
        # PRODUKTIV: Nutze Intelligent Pipeline
        if INTELLIGENT_PIPELINE_AVAILABLE and intelligent_pipeline:
            logger.info(f"üß† RAG Query via Intelligent Pipeline: {request.question[:50]}...")
            
            pipeline_request = IntelligentPipelineRequest(
                query_id=request_id,
                query_text=enriched_question,  # üÜï Verwende angereicherte Frage
                user_context={
                    "session_id": session_id,
                    "mode": request.mode,
                    "model": request.model,
                    "temperature": request.temperature,
                    **context_metadata  # üÜï Context-Metadaten hinzuf√ºgen
                },
                session_id=session_id,
                enable_llm_commentary=False,
                enable_real_time_updates=False,
                max_parallel_agents=5,
                timeout=60
            )
            
            pipeline_response = await intelligent_pipeline.process_intelligent_query(pipeline_request)
            processing_time = time.time() - start_time
            
            # Konvertiere Pipeline Response zu VeritasRAGResponse Format
            return VeritasRAGResponse(
                answer=pipeline_response.response_text,
                sources=pipeline_response.sources,
                metadata={
                    "mode": request.mode,
                    "model": request.model or "intelligent-pipeline",
                    "temperature": request.temperature,
                    "max_tokens": request.max_tokens,
                    "streaming_available": STREAMING_AVAILABLE,
                    "agent_count": len(pipeline_response.agent_results),
                    "pipeline_mode": "production",
                    **context_metadata  # üÜï Context-Info in Response
                },
                session_id=session_id,
                mode=request.mode,
                quality_score=pipeline_response.confidence_score,
                processing_time=processing_time,
                tokens_used=0,  # TODO: Token counting implementieren
                model_used=request.model or "intelligent-pipeline",
                request_id=request_id
            )
        
        else:
            # FALLBACK: Wenn Pipeline nicht verf√ºgbar
            logger.warning("‚ö†Ô∏è Intelligent Pipeline nicht verf√ºgbar - Basic Fallback")
            
            answer = f"""‚ö†Ô∏è **Pipeline nicht verf√ºgbar**

Ihre Frage: {request.question}

Die intelligente RAG-Pipeline ist derzeit nicht verf√ºgbar.
Bitte pr√ºfen Sie:
‚Ä¢ Ollama Installation
‚Ä¢ LLM-Modelle
‚Ä¢ Backend-Konfiguration

Kontaktieren Sie den Administrator."""
            
            processing_time = time.time() - start_time
            
            return VeritasRAGResponse(
                answer=answer,
                sources=[],
                metadata={
                    "mode": request.mode,
                    "model": "fallback",
                    "temperature": request.temperature,
                    "max_tokens": request.max_tokens,
                    "streaming_available": STREAMING_AVAILABLE,
                    "pipeline_error": "Not initialized"
                },
                session_id=session_id,
                mode=request.mode,
                quality_score=0.0,
                processing_time=processing_time,
                tokens_used=0,
                model_used="fallback",
                request_id=request_id
            )
        
    except Exception as e:
        logger.error(f"‚ùå Fehler bei RAG-Query: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===== UDS3 ENDPOINTS =====

@app.post("/uds3/documents", response_model=UDS3SecureDocumentResponse)
async def create_secure_document(request: UDS3SecureDocumentRequest):
    """Erstellt ein neues sicheres Dokument mit UDS3"""
    start_time = time.time()
    
    if not UDS3_AVAILABLE:
        raise HTTPException(status_code=503, detail="UDS3 System nicht verf√ºgbar")
    
    if not uds3_strategy:
        raise HTTPException(status_code=500, detail="UDS3 Strategy nicht initialisiert")
    
    try:
        # Security Level konvertieren
        security_level = None
        if SecurityLevel and request.security_level:
            try:
                security_level = SecurityLevel[request.security_level.upper()]
            except (KeyError, AttributeError):
                security_level = SecurityLevel.INTERNAL if SecurityLevel else None
        
        # UDS3 secure document creation
        result = uds3_strategy.create_secure_document(
            file_path=request.file_path,
            content=request.content,
            chunks=request.chunks,
            security_level=security_level,
            **request.metadata
        )
        
        processing_time = time.time() - start_time
        
        # JSON-sichere Datenextraktion
        def make_json_safe(data):
            """Konvertiert Daten zu JSON-serialisierbarem Format"""
            if isinstance(data, dict):
                return {k: make_json_safe(v) for k, v in data.items() 
                       if not callable(v)}
            elif isinstance(data, (list, tuple)):
                return [make_json_safe(item) for item in data]
            elif callable(data):
                return str(data)
            elif hasattr(data, '__dict__'):
                return str(data)
            else:
                return data
        
        safe_result = make_json_safe(result)
        
        return UDS3SecureDocumentResponse(
            success=safe_result.get("success", False),
            document_id=safe_result.get("security_info", {}).get("document_id"),
            operation_type=safe_result.get("operation_type", "CREATE_SECURE_DOCUMENT"),
            timestamp=safe_result.get("timestamp", datetime.now().isoformat()),
            security_info=safe_result.get("security_info", {}),
            quality_score=safe_result.get("quality_score", {}),
            validation_results=safe_result.get("validation_results", {}),
            database_operations=safe_result.get("database_operations", {}),
            issues=safe_result.get("issues", []),
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"Fehler bei UDS3 Dokument-Erstellung: {e}")
        processing_time = time.time() - start_time
        return UDS3SecureDocumentResponse(
            success=False,
            operation_type="CREATE_SECURE_DOCUMENT",
            timestamp=datetime.now().isoformat(),
            issues=[str(e)],
            processing_time=processing_time
        )

@app.post("/uds3/query", response_model=UDS3QueryResponse)
async def uds3_query(request: UDS3QueryRequest):
    """F√ºhrt eine UDS3-Query aus"""
    start_time = time.time()
    
    if not UDS3_AVAILABLE:
        raise HTTPException(status_code=503, detail="UDS3 System nicht verf√ºgbar")
    
    if not uds3_strategy:
        raise HTTPException(status_code=500, detail="UDS3 Strategy nicht initialisiert")
    
    try:
        # Fallback auf create_secure_document_light f√ºr einfache Queries
        if request.query_type == "light":
            result = create_secure_document_light({
                "file_path": "query_temp.txt",
                "content": request.query,
                "chunks": [request.query],
                **request.filters
            })
            
            processing_time = time.time() - start_time
            
            # JSON-sichere Konvertierung
            def make_json_safe(data):
                if isinstance(data, dict):
                    return {k: make_json_safe(v) for k, v in data.items() 
                           if not callable(v)}
                elif isinstance(data, (list, tuple)):
                    return [make_json_safe(item) for item in data]
                elif callable(data):
                    return str(data)
                else:
                    return data
            
            safe_result = make_json_safe(result)
            
            return UDS3QueryResponse(
                success=safe_result.get("success", False),
                results=[safe_result] if safe_result.get("success") else [],
                total_results=1 if safe_result.get("success") else 0,
                query_info={"query_type": request.query_type, "query": request.query},
                processing_time=processing_time,
                quality_metrics=safe_result.get("quality_score", {})
            )
        
        # F√ºr andere Query-Typen: Mock-Implementation
        processing_time = time.time() - start_time
        
        return UDS3QueryResponse(
            success=True,
            results=[{
                "document_id": f"mock_{hash(request.query)}",
                "content_preview": request.query[:100],
                "relevance_score": 0.85,
                "source": "uds3_mock"
            }],
            total_results=1,
            query_info={
                "query_type": request.query_type,
                "query": request.query,
                "filters_applied": len(request.filters)
            },
            processing_time=processing_time,
            quality_metrics={"confidence": 0.85, "coverage": 0.90}
        )
        
    except Exception as e:
        logger.error(f"Fehler bei UDS3 Query: {e}")
        processing_time = time.time() - start_time
        return UDS3QueryResponse(
            success=False,
            results=[],
            total_results=0,
            query_info={"error": str(e)},
            processing_time=processing_time
        )

@app.get("/uds3/status")
async def uds3_status():
    """UDS3 System Status"""
    status = {
        "uds3_available": UDS3_AVAILABLE,
        "strategy_initialized": uds3_strategy is not None,
        "multi_db_distribution": MULTI_DB_DISTRIBUTION_AVAILABLE if UDS3_AVAILABLE else False,
        "timestamp": datetime.now().isoformat()
    }
    
    if UDS3_AVAILABLE and uds3_strategy:
        try:
            # UDS3 Health Check
            health_result = getattr(uds3_strategy, 'health_check', lambda: {"status": "unknown"})()
            status["health_check"] = health_result
        except Exception as e:
            status["health_check"] = {"status": "error", "message": str(e)}
    
    return status

# ===== SESSION MANAGEMENT =====

@app.post("/session/start", response_model=StartSessionResponse)
async def start_session(request: StartSessionRequest):
    """Startet eine neue Session"""
    session_id = str(uuid.uuid4())
    
    return StartSessionResponse(
        session_id=session_id,
        mode=request.mode
    )

# ===== SYSTEM INFO ENDPOINTS =====

@app.get("/modes")
async def get_available_modes():
    """Verf√ºgbare Frage-Modi mit Details"""
    return {
        "modes": {
            "veritas": {
                "display_name": "Standard RAG",
                "description": "Retrieval-Augmented Generation mit Dokumenten-Suche",
                "status": "implemented",
                "endpoints": ["/v2/query", "/v2/query/stream"],
                "capabilities": ["rag", "streaming", "citations"]
            },
            "chat": {
                "display_name": "Allgemeiner Chat",
                "description": "Direkter Chat mit LLM ohne Dokument-Retrieval",
                "status": "implemented",
                "endpoints": ["/v2/query"],
                "capabilities": ["chat", "conversation_history"]
            },
            "vpb": {
                "display_name": "VPB Verwaltung",
                "description": "Spezialisiert auf Verwaltungsprozesse und Beh√∂rden",
                "status": "implemented",
                "endpoints": ["/v2/query"],
                "capabilities": ["rag", "administrative_context"]
            },
            "covina": {
                "display_name": "COVINA Analyse",
                "description": "COVID-19 Verwaltungsanalyse (experimentell)",
                "status": "experimental",
                "endpoints": ["/v2/query"],
                "capabilities": ["rag", "covid_context"]
            }
        },
        "default_mode": "veritas",
        "streaming_available": STREAMING_AVAILABLE
    }

@app.get("/agents/types")
async def get_available_agent_types():
    """Verf√ºgbare Agent-Typen"""
    return {
        "agent_types": [
            "legal_framework",
            "document_retrieval", 
            "geo_context",
            "external_api",
            "cost_analysis",
            "environmental",
            "construction", 
            "traffic",
            "financial",
            "social"
        ],
        "system_mode": "streaming_backend",
        "streaming_features": {
            "progress_updates": STREAMING_AVAILABLE,
            "intermediate_results": STREAMING_AVAILABLE,
            "llm_thinking": STREAMING_AVAILABLE
        },
        "phase5_hybrid_search": {
            "available": phase5_initialized if 'phase5_initialized' in locals() else False,
            "features": [
                "UDS3 Vector Search Adapter",
                "BM25 Sparse Retrieval",
                "Reciprocal Rank Fusion (RRF)",
                "Graceful Degradation"
            ],
            "current_mode": "BM25-Hybrid (Dense=0.0)" if phase5_initialized else "Disabled"
        }
    }

# ===== PHASE 5 HYBRID SEARCH ENDPOINT =====

@app.post("/v2/hybrid/search")
async def hybrid_search_endpoint(
    query: str,
    top_k: int = 10,
    enable_monitoring: bool = True
):
    """
    Phase 5 Hybrid Search Endpoint
    
    Kombiniert Dense (UDS3) + Sparse (BM25) Retrieval mit RRF Fusion.
    
    Args:
        query: Suchanfrage
        top_k: Anzahl Ergebnisse (default: 10)
        enable_monitoring: Performance-Monitoring (default: True)
    
    Returns:
        Hybrid search results mit Scores
    """
    try:
        from backend.api.veritas_phase5_integration import get_hybrid_retriever
        import time
        
        start_time = time.time()
        
        # Get hybrid retriever
        hybrid_retriever = get_hybrid_retriever()
        
        # Execute search
        results = await hybrid_retriever.retrieve(query, top_k=top_k)
        
        elapsed_ms = (time.time() - start_time) * 1000
        
        # Format results
        formatted_results = []
        for result in results:
            formatted_results.append({
                "doc_id": result.doc_id,
                "content": result.content,
                "score": float(result.score),
                "dense_score": float(result.dense_score),
                "sparse_score": float(result.sparse_score),
                "dense_rank": result.dense_rank,
                "sparse_rank": result.sparse_rank,
                "metadata": result.metadata
            })
        
        response = {
            "query": query,
            "results": formatted_results,
            "total_results": len(formatted_results),
            "latency_ms": elapsed_ms,
            "mode": "BM25-Hybrid" if all(r["dense_score"] == 0.0 for r in formatted_results) else "Full Hybrid",
            "timestamp": datetime.now().isoformat()
        }
        
        if enable_monitoring:
            logger.info(f"üîç Hybrid Search: '{query[:50]}...' ‚Üí {len(results)} results in {elapsed_ms:.0f}ms")
        
        return response
        
    except RuntimeError as e:
        raise HTTPException(status_code=503, detail=str(e))
    except Exception as e:
        logger.error(f"‚ùå Hybrid search error: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Hybrid search failed: {str(e)}")

@app.get("/get_models")
async def get_available_models():
    """Verf√ºgbare LLM-Modelle"""
    models = []
    
    # Versuche Modelle vom Ollama Client zu holen
    if ollama_client and hasattr(ollama_client, 'list_models'):
        try:
            ollama_models = await ollama_client.list_models()
            if ollama_models:
                models.extend(ollama_models)
        except Exception as e:
            logger.debug(f"Ollama Modelle nicht verf√ºgbar: {e}")
    
    # Fallback auf Standard-Modelle
    if not models:
        models = [
            {"name": "llama3.1:latest", "size": "4.7GB", "provider": "ollama"},
            {"name": "llama3.1:8b", "size": "4.7GB", "provider": "ollama"},
            {"name": "mistral:latest", "size": "4.1GB", "provider": "ollama"},
            {"name": "codellama:latest", "size": "3.8GB", "provider": "ollama"},
        ]
    
    return {
        "models": models,
        "total": len(models),
        "default_model": "llama3.1:latest"
    }

# ===== MAIN ENTRY POINT =====

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=5000)