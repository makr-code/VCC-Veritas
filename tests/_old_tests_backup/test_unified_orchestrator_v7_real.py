"""
End-to-End Test f√ºr UnifiedOrchestratorV7 mit REAL UDS3 + Ollama + SUPERVISOR

Test Query: "Brauche ich eine Baugenehmigung f√ºr einen Carport mit PV in M√ºnchen?"

Testet:
- Real UDS3 Hybrid Search (ChromaDB + Neo4j)
- Real Ollama LLM Calls (llama3.2)
- Supervisor Integration (Phases 1.5, 1.6, 6.5)
- Agent Selection & Execution (Mock-Modus)
- Alle 9 wissenschaftlichen Phasen (mit Supervisor)
- JSON Schema Validierung
- Streaming Events

Author: VERITAS v7.0
Date: 12. Oktober 2025 (Updated for Supervisor Integration)
"""

import asyncio
import sys
import logging
from pathlib import Path

# Add project root to path
REPO_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(REPO_ROOT))

from backend.orchestration.unified_orchestrator_v7 import UnifiedOrchestratorV7, StreamEvent
from backend.agents.veritas_ollama_client import VeritasOllamaClient

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def test_real_query_processing():
    """
    End-to-End Test mit Real UDS3 + Ollama + Supervisor Integration
    """
    
    # Updated query to test supervisor agent selection
    test_query = "Brauche ich eine Baugenehmigung f√ºr einen Carport mit PV in M√ºnchen?"
    
    logger.info("=" * 80)
    logger.info("üß™ VERITAS v7.0 - End-to-End Test (Real UDS3 + Ollama + Supervisor)")
    logger.info("=" * 80)
    logger.info(f"Test Query: {test_query}")
    logger.info("Expected: Construction + Weather + Financial agents should be selected")
    logger.info("=" * 80)
    
    try:
        # 1. Initialize Ollama Client
        logger.info("\nüìã Step 1: Initialize Ollama Client")
        ollama_client = VeritasOllamaClient(
            base_url="http://localhost:11434",
            timeout=60,  # L√§ngeres Timeout f√ºr wissenschaftliche Phasen
            max_retries=2
        )
        
        # Health check
        await ollama_client.initialize()
        logger.info("‚úÖ Ollama Client initialized")
        
        # 2. Initialize UnifiedOrchestratorV7 (UDS3 auto-initialized)
        logger.info("\nüìã Step 2: Initialize UnifiedOrchestratorV7 with Supervisor")
        orchestrator = UnifiedOrchestratorV7(
            config_dir="config",
            method_id="default_method",  # supervisor_enabled=true in config
            ollama_client=ollama_client,
            uds3_strategy=None,  # Auto-initialize
            agent_orchestrator=None,  # Mock mode (no real agents yet)
            enable_streaming=True
        )
        logger.info("‚úÖ Orchestrator initialized (Supervisor Mode: Mock Agents)")
        
        # Check if supervisor is enabled
        supervisor_enabled = orchestrator._is_supervisor_enabled()
        logger.info(f"   Supervisor Enabled: {supervisor_enabled}")
        if supervisor_enabled:
            logger.info("   ‚ö†Ô∏è Note: Agent execution will use MOCK results (no real agent_orchestrator)")
        else:
            logger.warning("   ‚ö†Ô∏è WARNING: Supervisor NOT enabled - will skip agent phases!")
        
        # 3. Process Query with Streaming
        logger.info("\nüìã Step 3: Process Query (Streaming Mode)")
        logger.info("-" * 80)
        
        events_collected = []
        phase_results = {}
        
        async for event in orchestrator.process_query_stream(user_query=test_query):
            events_collected.append(event)
            
            # Log event
            if event.type == "progress":
                logger.info(f"‚è≥ Progress: {event.data.get('progress')}% - {event.data.get('message')}")
            
            elif event.type == "processing_step":
                logger.info(f"üîÑ {event.data.get('step')}: {event.data.get('message')}")
            
            elif event.type == "phase_complete":
                phase_id = event.data.get('phase_id')
                status = event.data.get('status')
                confidence = event.data.get('confidence', 0.0)
                execution_time = event.data.get('execution_time_ms', 0)
                
                logger.info(
                    f"‚úÖ Phase Complete: {phase_id} | "
                    f"Status: {status} | "
                    f"Confidence: {confidence:.2f} | "
                    f"Time: {execution_time:.0f}ms"
                )
                
                phase_results[phase_id] = event.data
            
            elif event.type == "final_result":
                logger.info("\n" + "=" * 80)
                logger.info("üéØ FINAL RESULT")
                logger.info("=" * 80)
                
                final_answer = event.data.get('final_answer', {})
                overall_confidence = event.data.get('confidence', 0.0)
                total_time = event.data.get('execution_time_ms', 0)
                
                logger.info(f"\nüìù Final Answer:")
                logger.info(f"   Main Answer: {final_answer.get('main_answer', 'N/A')[:200]}...")
                logger.info(f"   Confidence: {overall_confidence:.2f}")
                logger.info(f"   Total Execution Time: {total_time:.0f}ms ({total_time/1000:.1f}s)")
                
                if final_answer.get('action_recommendations'):
                    logger.info(f"\nüí° Action Recommendations:")
                    for rec in final_answer.get('action_recommendations', [])[:3]:
                        logger.info(f"   - [{rec.get('priority', 'N/A')}] {rec.get('action', 'N/A')}")
            
            elif event.type == "error":
                logger.error(f"‚ùå Error: {event.data.get('message')}")
        
        # 4. Summary Statistics
        logger.info("\n" + "=" * 80)
        logger.info("üìä TEST SUMMARY")
        logger.info("=" * 80)
        
        event_type_counts = {}
        for event in events_collected:
            event_type_counts[event.type] = event_type_counts.get(event.type, 0) + 1
        
        logger.info(f"Total Events: {len(events_collected)}")
        for event_type, count in event_type_counts.items():
            logger.info(f"  - {event_type}: {count}")
        
        logger.info(f"\nPhases Completed: {len(phase_results)}/9 (with supervisor)")
        for phase_id, result in phase_results.items():
            status_icon = "‚úÖ" if result.get('status') == 'success' else "‚ö†Ô∏è"
            logger.info(
                f"  {status_icon} {phase_id}: {result.get('status')} "
                f"(confidence={result.get('confidence', 0.0):.2f})"
            )
        
        # Check if supervisor phases were executed
        supervisor_phases = ['supervisor_agent_selection', 'agent_execution', 'agent_result_synthesis']
        supervisor_executed = [p for p in supervisor_phases if p in phase_results]
        
        logger.info(f"\nSupervisor Phases Executed: {len(supervisor_executed)}/3")
        if supervisor_executed:
            for phase_id in supervisor_executed:
                logger.info(f"  ‚úÖ {phase_id}")
        else:
            logger.warning("  ‚ö†Ô∏è No supervisor phases executed (supervisor_enabled=false or skipped)")
        
        # 5. Validation Checks
        logger.info("\n" + "=" * 80)
        logger.info("‚úÖ VALIDATION CHECKS")
        logger.info("=" * 80)
        
        checks = {
            "Supervisor enabled in config": orchestrator._is_supervisor_enabled(),
            "Expected phases executed (6-9)": len(phase_results) >= 6,
            "Final result received": any(e.type == "final_result" for e in events_collected),
            "No errors": not any(e.type == "error" for e in events_collected),
            "All phases successful": all(
                r.get('status') in ['success', 'partial', 'skipped'] 
                for r in phase_results.values()
            ),
            "Confidence > 0.5": all(
                r.get('confidence', 0.0) > 0.5 
                for r in phase_results.values() 
                if r.get('status') == 'success'  # Only check successful phases
            )
        }
        
        for check_name, passed in checks.items():
            status = "‚úÖ PASS" if passed else "‚ùå FAIL"
            logger.info(f"{status}: {check_name}")
        
        # Overall Result
        all_passed = all(checks.values())
        logger.info("\n" + "=" * 80)
        if all_passed:
            logger.info("üéâ TEST PASSED - All validation checks successful!")
        else:
            logger.warning("‚ö†Ô∏è TEST INCOMPLETE - Some checks failed")
        logger.info("=" * 80)
        
        return all_passed
    
    except Exception as e:
        logger.error(f"\n‚ùå TEST FAILED: {e}", exc_info=True)
        return False
    
    finally:
        # Cleanup
        if 'ollama_client' in locals():
            await ollama_client.close()
            logger.info("\nüßπ Ollama Client closed")


async def test_non_streaming():
    """
    Test Non-Streaming Mode mit Supervisor (f√ºr Performance-Vergleich)
    """
    
    # Updated query
    test_query = "Brauche ich eine Baugenehmigung f√ºr einen Carport mit PV in M√ºnchen?"
    
    logger.info("\n" + "=" * 80)
    logger.info("üß™ VERITAS v7.0 - Non-Streaming Test (Supervisor)")
    logger.info("=" * 80)
    
    try:
        # Initialize
        ollama_client = VeritasOllamaClient(
            base_url="http://localhost:11434",
            timeout=60,
            max_retries=2
        )
        await ollama_client.initialize()
        
        orchestrator = UnifiedOrchestratorV7(
            config_dir="config",
            method_id="default_method",
            ollama_client=ollama_client,
            uds3_strategy=None,
            agent_orchestrator=None,  # Mock mode
            enable_streaming=False  # Non-streaming
        )
        
        # Check supervisor mode
        supervisor_enabled = orchestrator._is_supervisor_enabled()
        logger.info(f"Supervisor Enabled: {supervisor_enabled}")
        
        # Process query
        logger.info("‚è≥ Processing query (non-streaming)...")
        import time
        start_time = time.time()
        
        result = await orchestrator.process_query(user_query=test_query)
        
        duration = time.time() - start_time
        
        # Results
        logger.info(f"\n‚úÖ Query processed in {duration:.1f}s")
        logger.info(f"Confidence: {result.confidence:.2f}")
        logger.info(f"Final Answer: {result.final_answer.get('main_answer', 'N/A')[:200]}...")
        
        return True
    
    except Exception as e:
        logger.error(f"‚ùå Non-streaming test failed: {e}", exc_info=True)
        return False
    
    finally:
        if 'ollama_client' in locals():
            await ollama_client.close()


async def main():
    """
    Main test runner
    """
    
    logger.info("\n" + "=" * 80)
    logger.info("üöÄ VERITAS v7.0 - End-to-End Test Suite")
    logger.info("=" * 80)
    
    # Test 1: Streaming Mode
    logger.info("\nüìã Test 1: Streaming Mode")
    test1_passed = await test_real_query_processing()
    
    # Test 2: Non-Streaming Mode
    logger.info("\nüìã Test 2: Non-Streaming Mode")
    test2_passed = await test_non_streaming()
    
    # Final Summary
    logger.info("\n" + "=" * 80)
    logger.info("üèÅ FINAL TEST SUMMARY")
    logger.info("=" * 80)
    logger.info(f"Test 1 (Streaming):     {'‚úÖ PASSED' if test1_passed else '‚ùå FAILED'}")
    logger.info(f"Test 2 (Non-Streaming): {'‚úÖ PASSED' if test2_passed else '‚ùå FAILED'}")
    
    if test1_passed and test2_passed:
        logger.info("\nüéâ ALL TESTS PASSED - v7.0 PRODUCTION READY!")
    else:
        logger.warning("\n‚ö†Ô∏è SOME TESTS FAILED - Review logs above")
    
    logger.info("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())
