# DUAL-MODE PROMPT INTEGRATION - FINALE VERSION

## üéØ Problem-Diagnose

**Test-Ergebnis 1 (FEHLGESCHLAGEN):**
```
‚ùå Zitationen: 0/3
‚ùå Direkte Zitate: 0/2
‚ùå Legal Refs: 0/3
üìä GESAMT: 0/3 Kriterien erf√ºllt (0.0%)
```

**Root Cause:**
- Backend nutzt `SupervisorAgent.synthesize_results()`
- SupervisorAgent hatte **kein Enhanced Prompt System**
- VerwaltungsrechtPrompts Integration in `veritas_api_module.py` wurde ignoriert

## ‚úÖ L√∂sung: Dual-Mode Prompt System

### Was ist das Dual-Mode System?

Das System in `backend/agents/veritas_enhanced_prompts.py` hat **3 Modi**:

1. **INTERNAL_RAG** - Query-Enrichment f√ºr bessere Retrieval-Qualit√§t
2. **USER_FACING** - Nat√ºrliche Antworten mit **IEEE-Zitationen** ‚≠ê
3. **HYBRID** - Kombiniert beide Modi

### USER_FACING_RESPONSE Template (Das richtige!)

**System-Prompt Features:**
```python
PERS√ñNLICHKEIT:
- Freundlich, zug√§nglich
- Pr√§zise, aber nicht steif

ZITATIONSREGELN (ZWINGEND!):
- JEDER Fakt MUSS mit [N] zitiert werden
- [N] = Position in Quellenliste (1-basiert)
- Zitation DIREKT nach Fakt: "¬ß 58 regelt...[1]"

FORMAT:
1. Direkte Antwort (MIT [N] Zitationen!)
2. Details (MIT [N] Zitationen!)
3. Quellen (automatisch)
4. N√§chste Schritte
5. üí° Vorschl√§ge (3-5 Follow-ups)
```

**User-Prompt Variables:**
- `{query}` - User-Frage
- `{rag_context}` - Kontext aus Dokumenten
- `{agent_results}` - Agent-Erkenntnisse
- `{source_list}` - **Verf√ºgbare Quellen f√ºr [N] Zitationen**

**Beispiel Output (EXZELLENT):**
```
F√ºr eine Baugenehmigung in Brandenburg ben√∂tigen Sie folgende Unterlagen[1]:

‚Ä¢ Bauantrag (amtliches Formular)
‚Ä¢ Lageplan mit Grundst√ºcksgrenzen
‚Ä¢ Bauvorlagen[2]

Die Bearbeitungsdauer betr√§gt 2-3 Monate[3].

üí° Vorschl√§ge:
‚Ä¢ Welche Kosten fallen an?
‚Ä¢ Welche Fristen muss ich beachten?
```

## üîß Implementierte √Ñnderungen

### Datei 1: `backend/agents/veritas_supervisor_agent.py`

**Import ge√§ndert (Zeile 41):**
```python
# ALT:
from backend.agents.veritas_enhanced_prompts import VerwaltungsrechtPrompts

# NEU:
from backend.agents.veritas_enhanced_prompts import EnhancedPromptTemplates, PromptMode
```

**synthesize_results() ersetzt (Zeile 668):**

**ALT (Kein Enhanced Prompt):**
```python
synthesis_prompt = self.SYNTHESIS_PROMPT.format(
    original_query=original_query,
    agent_results=agent_results_json
)
```

**NEU (Dual-Mode System):**
```python
# 1. Bereite Daten auf
rag_context = "\n\n".join([
    f"[{i}] {agent.agent_type}: {agent.response_text}"
    for i, agent in enumerate(deduplicated, 1)
])

source_list = "\n".join([
    f"[{i}] Agent: {agent.agent_type} (Conf: {agent.confidence_score:.2f})"
    for i, agent in enumerate(deduplicated, 1)
])

# 2. Nutze USER_FACING_RESPONSE
system_prompt = EnhancedPromptTemplates.get_system_prompt(
    mode=PromptMode.USER_FACING,
    domain="general"
)

user_prompt = EnhancedPromptTemplates.get_user_prompt(
    mode=PromptMode.USER_FACING,
    domain="general",
    query=original_query,
    rag_context=rag_context,
    agent_results=agent_results_text,
    source_list=source_list  # ‚≠ê F√ºr IEEE-Zitationen!
)

full_prompt = f"{system_prompt}\n\n{user_prompt}"
```

## üìä Erwartete Verbesserung

**Vorher (Baseline):**
- 0% IEEE-Zitationen
- 1.6% Direkte Zitate
- 32% Aspect Coverage
- 0% Follow-up Suggestions

**Nachher (USER_FACING_RESPONSE):**
- **60-80%** IEEE-Zitationen ‚úÖ (Prompt erzwingt [N] nach jedem Fakt)
- **40-60%** Direkte Zitate (nat√ºrlicher Output)
- **50-70%** Aspect Coverage (strukturierte Antworten)
- **80-100%** Follow-up Suggestions (Template fordert 3-5 Vorschl√§ge)

## üß™ Test-Anweisungen

### Schritt 1: Backend neu starten
```bash
# Backend-Terminal (Strg+C falls l√§uft)
python start_backend.py
```

### Schritt 2: Prompt-Verbesserungstest
```bash
cd tests
python test_prompt_improvement.py
```

**Erwartung:**
```
‚úÖ Zitationen: ‚â•3/3 (IEEE [1], [2], [3])
‚úÖ Direkte Zitate: ‚â•2/2 (falls vorhanden)
‚úÖ Legal Refs: ‚â•3/3 (¬ß Referenzen)
üìä GESAMT: 3/3 Kriterien erf√ºllt (100%)
```

### Schritt 3: Bei Erfolg - Full Golden Dataset v2
```bash
python test_rag_quality_v3_19_0.py
# Teste alle 10 Modelle mit Enhanced Prompts
# Vergleiche mit Baseline (baseline in rag_test_results_1760099947.json)
```

## üîç Warum funktioniert das jetzt?

**Problem vorher:**
1. Backend ‚Üí Intelligent Pipeline ‚Üí SupervisorAgent
2. SupervisorAgent.SYNTHESIS_PROMPT = **basic template ohne Zitationen**
3. Kein `source_list` Parameter ‚Üí LLM wei√ü nicht was [1], [2] bedeutet

**L√∂sung jetzt:**
1. Backend ‚Üí Intelligent Pipeline ‚Üí SupervisorAgent
2. SupervisorAgent ‚Üí **EnhancedPromptTemplates.USER_FACING_RESPONSE**
3. **`source_list` wird √ºbergeben:** `[1] Agent: BuildingPermitAgent (Conf: 0.85)`
4. LLM sieht: "F√ºr Zitationen nutze [1], [2], [3] aus source_list"
5. LLM generiert: "¬ß 58 LBO BW regelt...[1]" ‚úÖ

## üìÅ Ge√§nderte Dateien (Final)

1. ‚úÖ `backend/agents/veritas_ollama_client.py`
   - Neue Methode: `list_models()` (10 Modelle statt 4)

2. ‚úÖ `backend/api/veritas_api_module.py`
   - Import: VerwaltungsrechtPrompts (wird nicht genutzt, aber bereit)
   - Enhanced Prompt (wird von Supervisor √ºberschrieben)

3. ‚úÖ `backend/agents/veritas_supervisor_agent.py` ‚≠ê **KRITISCH**
   - Import: EnhancedPromptTemplates, PromptMode
   - synthesize_results(): Nutzt USER_FACING_RESPONSE mit source_list

4. ‚úÖ `tests/test_rag_quality_v3_19_0.py`
   - Bugfix: start_time ‚Üí query_start_time

5. ‚úÖ `tests/analyze_golden_dataset.py` (NEU)
   - Analyse-Tool

6. ‚úÖ `tests/test_prompt_improvement.py` (NEU)
   - Schnelltest (1 Modell, 1 Frage)

## üéØ Kritische Unterschiede

### VerwaltungsrechtPrompts vs. EnhancedPromptTemplates

| Feature | VerwaltungsrechtPrompts | EnhancedPromptTemplates |
|---------|------------------------|------------------------|
| **Ziel** | Verwaltungsrecht-spezifisch | Multi-Domain, flexibel |
| **Modi** | Single-Mode | **3 Modi** (RAG, User, Hybrid) |
| **Zitate** | ¬ß Referenzen + "..." Quotes | **IEEE [N] Citations** ‚≠ê |
| **Follow-ups** | Template-basiert | **Explizit gefordert** (3-5) |
| **Backend-Integration** | Nicht verwendet | **Aktiv in Supervisor** ‚úÖ |

**Warum EnhancedPromptTemplates?**
- ‚úÖ Bereits im Backend integriert
- ‚úÖ USER_FACING_RESPONSE hat **alle Features**
- ‚úÖ IEEE-Zitationen = Standard in RAG-Systemen
- ‚úÖ Multi-Domain (building, environmental, etc.)

## ‚ö†Ô∏è Fallback-Plan

Falls Test wieder fehlschl√§gt:

1. **Pr√ºfe ob Supervisor wirklich genutzt wird:**
   ```python
   # In Backend-Logs suchen:
   "[ENHANCED SYNTHESIS] Nutze EnhancedPromptTemplates.USER_FACING_RESPONSE"
   ```

2. **Pr√ºfe ob source_list √ºbergeben wird:**
   ```python
   # Log sollte zeigen:
   "Prompt: XXXX Zeichen, Quellen: 3, Mode: USER_FACING"
   ```

3. **Wenn immer noch 0% Zitationen:**
   - Problem liegt am LLM (ignoriert Anweisungen)
   - L√∂sung: Few-Shot Examples direkt in Prompt einbauen
   - Alternative: Constraint-based Generation (Force [N] nach jedem Satz)

## üìä Success Criteria

**Minimum Viable Success:**
- ‚â• 50% Zitationen (1.5/3)
- ‚â• 30% Direkte Zitate (0.6/2)
- ‚â• 50% Legal Refs (1.5/3)

**Target Success:**
- ‚â• 80% Zitationen (2.4/3)
- ‚â• 60% Direkte Zitate (1.2/2)
- ‚â• 70% Legal Refs (2.1/3)

**Excellent:**
- 100% Zitationen (3/3) ‚úÖ
- 100% Direkte Zitate (2/2) ‚úÖ
- 100% Legal Refs (3/3) ‚úÖ

---

**Status:** ‚úÖ Dual-Mode Integration abgeschlossen
**N√§chster Schritt:** Backend-Neustart ‚Üí test_prompt_improvement.py
**Expected Result:** 60-80% Improvement (Baseline: 0% ‚Üí Target: 60-80%)
