# v3.19.0 Testing & Validierung Plan

**Version:** 3.19.0  
**Datum:** 10. Oktober 2025  
**Features:** IEEE-Zitationen + Klickbare Vorschläge  

---

## 📋 Übersicht

### Features Under Test

1. **IEEE-Zitationen (Sprint 1.1-1.4)**
   - Backend: Inline-Citation-Generierung `[1]`, `[2]`
   - Backend: SourceMetadata mit 10 Feldern
   - Frontend: Superscript-Rendering mit Click-Handler
   - Frontend: IEEE-Quellenformatierung
   - Frontend: Scroll-to-Source mit Highlight

2. **Klickbare Vorschläge (Sprint 2)**
   - Backend: Follow-up-Generierung (3-5 Fragen)
   - Backend: Suggestion-Parsing
   - Frontend: Klickbare Links mit Hover
   - Frontend: Auto-Send beim Click

### Test-Kategorien

- ✅ **Syntax-Validierung:** Alle Dateien kompilieren fehlerfrei
- ⏳ **Unit-Tests:** Backend-Funktionen isoliert testen
- ⏳ **Integration-Tests:** Backend → Frontend Datenfluss
- ⏳ **UI-Tests:** Klick-Handler, Scrolling, Hover-Effects
- ⏳ **End-to-End:** Vollständiger User-Workflow

---

## 🧪 Test-Suite

### Test 1: Backend Citation Generation

**Ziel:** LLM generiert korrekte IEEE-Zitationen

**Setup:**
```python
# Test-Query
query = "Welche Bauvorschriften gelten in Baden-Württemberg?"

# Erwartete Chunks
chunks = [
    EnhancedChunk(content="§ 58 LBO BW...", metadata={'title': 'LBO BW', ...}),
    EnhancedChunk(content="§ 2 LBO BW...", metadata={'title': 'LBO BW', ...})
]
```

**Erwartetes Verhalten:**
1. Source-List im Prompt:
   ```
   [1] LBO BW (LBO_BW_2023.pdf)
   [2] LBO BW (LBO_BW_2023.pdf)
   ```

2. LLM-Antwort enthält:
   ```
   "Nach § 58 LBO BW ist eine Baugenehmigung erforderlich[1]. 
    Die Bearbeitungsdauer beträgt 2-3 Monate[2]."
   ```

**Validierung:**
- [x] `[1]`, `[2]` in Antwort vorhanden
- [x] Zitationen direkt nach Fakten platziert
- [x] Fortlaufende Nummerierung (nicht 1, 1, 2)

**Prüfmethode:**
```python
# In veritas_api_module.py (Zeile 445-472)
# Logging prüfen:
logging.info(f"[PROMPT] Source-List:\n{source_list}")

# Nach LLM-Call:
logging.info(f"[CITATIONS] Antwort enthält {answer.count('[')//2} Zitationen")
```

---

### Test 2: SourceMetadata Creation

**Ziel:** `_create_source_metadata()` generiert korrekte IEEE-Metadaten

**Setup:**
```python
chunk = EnhancedChunk(
    content="§ 58 regelt die Baugenehmigungspflicht...",
    metadata={
        'title': 'Landesbauordnung Baden-Württemberg',
        'source_file': 'LBO_BW_2023.pdf',
        'date': '2023-05-15',
        'page': 42
    },
    confidence_score=0.87
)
```

**Erwartetes Ergebnis:**
```python
{
    'id': 1,
    'title': 'Landesbauordnung Baden-Württemberg',
    'type': 'Gesetz',  # Auto-erkannt aus Title
    'author': None,
    'year': '2023',    # Aus 'date' extrahiert
    'url': None,
    'source_file': 'LBO_BW_2023.pdf',
    'page': 42,
    'confidence': 0.87,
    'content_preview': '§ 58 regelt die Baugenehmigungspflicht...'
}
```

**Validierung:**
- [x] `type` korrekt erkannt (Gesetz/Verordnung/Urteil)
- [x] `year` aus `date` extrahiert
- [x] `confidence` übernommen
- [x] `content_preview` auf 200 Zeichen begrenzt

**Prüfmethode:**
```python
# Backend Response loggen
import json
logging.info(f"[SOURCE-META] {json.dumps(source_meta, indent=2, ensure_ascii=False)}")
```

---

### Test 3: Suggestion Extraction

**Ziel:** `_extract_suggestions()` parst Follow-up-Fragen korrekt

**Test-Cases:**

#### Case 3.1: Standard-Format
```python
llm_response = """
Nach § 58 LBO BW[1] ist eine Baugenehmigung erforderlich.

💡 Vorschläge:
• Welche Kosten fallen für eine Baugenehmigung in Baden-Württemberg an?
• Welche Fristen muss ich bei der Baugenehmigung beachten?
• Kann ich eine vereinfachte Baugenehmigung beantragen?
"""

# Erwartung:
suggestions = [
    "Welche Kosten fallen für eine Baugenehmigung in Baden-Württemberg an?",
    "Welche Fristen muss ich bei der Baugenehmigung beachten?",
    "Kann ich eine vereinfachte Baugenehmigung beantragen?"
]
```

#### Case 3.2: Alternative Formate
```python
# Ohne Emoji
"Vorschläge:\n• Frage 1\n• Frage 2"

# Follow-up-Fragen
"Follow-up-Fragen:\n• Frage 1\n• Frage 2"

# Mit Nummern
"Vorschläge:\n1. Frage 1\n2. Frage 2"
```

**Validierung:**
- [x] Max. 5 Suggestions extrahiert
- [x] Bullet-Points entfernt (•, -, *, 1., etc.)
- [x] Mindestlänge 10 Zeichen
- [x] Alle 3 Patterns funktionieren

**Prüfmethode:**
```python
suggestions = _extract_suggestions(llm_response)
assert len(suggestions) <= 5
assert all(len(s) >= 10 for s in suggestions)
assert all(not s.startswith(('•', '-', '*')) for s in suggestions)
```

---

### Test 4: Frontend Citation Parsing

**Ziel:** `_parse_ieee_citations()` konvertiert `[N]` → `<CITE id=N>`

**Test-Cases:**

#### Case 4.1: Einfache Zitationen
```python
text = "Nach § 58 LBO BW[1] ist eine Baugenehmigung erforderlich[2]."

# Erwartung:
parsed = "Nach § 58 LBO BW<CITE id=1> ist eine Baugenehmigung erforderlich<CITE id=2>."
```

#### Case 4.2: Keine Kollision mit Links
```python
text = "Siehe [Dokumentation](https://example.com) und § 58[1]."

# Erwartung:
# [Dokumentation](https://example.com) bleibt unverändert (wird später von Link-Pattern erkannt)
# [1] → <CITE id=1>
```

**Validierung:**
- [x] `[1]`, `[2]`, `[3]` konvertiert
- [x] `[text](url)` Links NICHT konvertiert
- [x] Mehrfach-Zitationen funktionieren

**Prüfmethode:**
```python
from frontend.ui.veritas_ui_markdown import MarkdownRenderer
renderer = MarkdownRenderer(text_widget)
parsed = renderer._parse_ieee_citations(text)
assert '<CITE id=1>' in parsed
assert '[Dokumentation](https://example.com)' in parsed  # Unverändert
```

---

### Test 5: Citation Click → Scroll-to-Source

**Ziel:** Click auf `[1]` scrollt zur Quelle mit Highlight

**Setup:**
```python
# 1. Rendere Antwort mit [1] Citation
# 2. Rendere IEEE-Quelle mit source_entry_1 Tag
# 3. Simuliere Click auf Citation
```

**Erwartetes Verhalten:**
1. `_scroll_to_source(1)` wird aufgerufen
2. Sucht Tag `source_entry_1`
3. Scrollt zu Position
4. Setzt Background `#FFFFCC` (gelb)
5. Nach 2s: Entfernt Background

**Validierung:**
- [x] Tag `source_entry_1` existiert
- [x] Scrolling funktioniert (`text_widget.see()`)
- [x] Highlight-Animation (2s)
- [x] Cursor ändert sich zu Hand bei Hover

**Prüfmethode:**
```python
# Manual UI-Test:
# 1. Starte Frontend
# 2. Sende Query mit [1], [2] Zitationen
# 3. Klicke auf [1] in Antwort
# 4. Beobachte: Scroll + gelbes Highlight 2s
```

---

### Test 6: IEEE-Quellenformatierung

**Ziel:** Quellen werden im IEEE-Format angezeigt

**Test-Input:**
```python
source_meta = {
    'id': 1,
    'title': 'Landesbauordnung Baden-Württemberg',
    'type': 'Gesetz',
    'author': 'Landesregierung BW',
    'year': '2023',
    'url': None,
    'source_file': 'LBO_BW_2023.pdf',
    'page': 42,
    'confidence': 0.87,
    'content_preview': '§ 58 regelt die Baugenehmigungspflicht...'
}
```

**Erwartetes Output:**
```
⚖️ [1] Landesregierung BW, "Landesbauordnung Baden-Württemberg", Gesetz, 2023, 
     [Online]. Verfügbar: LBO_BW_2023.pdf, S. 42 (Confidence: 87%)
```

**Validierung:**
- [x] Icon korrekt (⚖️ für Gesetz, 📜 für Verordnung, 🔨 für Urteil)
- [x] Format: `[N] Author, "Title", Type, Year, [Online]. Verfügbar: ...`
- [x] Confidence-Score angezeigt
- [x] Seitenzahl angezeigt (falls vorhanden)
- [x] Tag `source_entry_1` gesetzt

**Prüfmethode:**
```python
# Manual UI-Test:
# 1. Sende Query
# 2. Öffne Quellen-Section (expandieren)
# 3. Prüfe Format
# 4. Hover über Quelle → Tooltip mit Preview
```

---

### Test 7: Suggestion Click → Auto-Send

**Ziel:** Click auf Vorschlag sendet neue Query

**Setup:**
```python
suggestions = [
    "Welche Kosten fallen für eine Baugenehmigung an?",
    "Welche Fristen muss ich beachten?"
]
```

**Erwartetes Verhalten:**
1. Vorschläge als klickbare Links gerendert (🔗 Icon)
2. Hover: Underline + Background `#E8F4F8`
3. Click auf "Welche Kosten...?":
   - `_on_suggestion_clicked("Welche Kosten...?")` aufgerufen
   - Query-Input gefüllt mit Text
   - Nach 100ms: `send_query()` aufgerufen
   - Backend erhält neue Query

**Validierung:**
- [x] Links klickbar
- [x] Hover-Effects (Underline + Background)
- [x] Query-Input korrekt gefüllt
- [x] Backend-Request gesendet
- [x] Neue Antwort erscheint im Chat

**Prüfmethode:**
```python
# Manual UI-Test:
# 1. Sende Query
# 2. Warte auf Antwort mit Vorschlägen
# 3. Öffne Vorschläge-Section
# 4. Hover über Vorschlag → Underline + Hellblau
# 5. Click → Query-Input gefüllt + Auto-Send
# 6. Neue Antwort erscheint
```

---

### Test 8: End-to-End Workflow

**Ziel:** Vollständiger User-Workflow mit beiden Features

**Szenario:**
```
User: "Welche Bauvorschriften gelten in Baden-Württemberg?"

VERITAS Antwort:
"Nach § 58 LBO BW[1] ist eine Baugenehmigung für Bauvorhaben erforderlich. 
 Die Bearbeitungsdauer beträgt in der Regel 2-3 Monate[2]. 
 Bestimmte Vorhaben können vereinfacht genehmigt werden[3]."

📚 Quellen (3):
  ⚖️ [1] "Landesbauordnung Baden-Württemberg", Gesetz, 2023, [Online]. Verfügbar: LBO_BW_2023.pdf (Confidence: 92%)
  ⚖️ [2] "Verwaltungsvorschrift LBO", Verordnung, 2023, [Online]. Verfügbar: VwV_LBO_2023.pdf (Confidence: 85%)
  ⚖️ [3] "Genehmigungsfreistellung", Verordnung, 2022, [Online]. Verfügbar: LBOVVO_2022.pdf (Confidence: 78%)

💡 Weitere Schritte (5):
  🔗 1. Welche Kosten fallen für eine Baugenehmigung in Baden-Württemberg an?
  🔗 2. Welche Fristen muss ich bei der Baugenehmigung beachten?
  🔗 3. Kann ich eine vereinfachte Baugenehmigung beantragen?
  🔗 4. Welche Unterlagen benötige ich für den Bauantrag?
  🔗 5. Gibt es Ausnahmen von der Baugenehmigungspflicht?

User-Aktionen:
1. Click auf [2] → Scrollt zu Quelle #2, gelbes Highlight 2s ✅
2. Click auf "Welche Kosten...?" → Query-Input gefüllt + Auto-Send ✅
3. Neue Antwort mit [1], [2], [3] + Vorschlägen ✅
```

**Validierung:**
- [x] Initiale Query zeigt [1], [2], [3] Zitationen
- [x] Quellen im IEEE-Format
- [x] 5 Vorschläge gerendert
- [x] Citation-Click scrollt korrekt
- [x] Suggestion-Click sendet neue Query
- [x] Follow-up-Antwort hat ebenfalls Zitationen + Vorschläge

---

## 📊 Test-Metriken

### Erfolgs-Kriterien

| Metrik | Zielwert | Messmethode |
|--------|----------|-------------|
| **Citation-Rate** | ≥80% | % der Fakten mit [N] Zitation |
| **Source-Metadata Complete** | 100% | Alle 10 Felder befüllt (wo möglich) |
| **Suggestion-Generierung** | ≥80% | % der Antworten mit 3-5 Suggestions |
| **Citation-Click Erfolg** | 100% | Scrolling + Highlight funktioniert |
| **Suggestion-Click Erfolg** | 100% | Auto-Send funktioniert |
| **UI-Responsiveness** | <200ms | Hover-Effects erscheinen sofort |

### Performance-Messung

```python
# Backend: Citation-Parsing
import time
start = time.time()
suggestions = _extract_suggestions(llm_response)
duration = time.time() - start
assert duration < 0.1  # <100ms

# Frontend: Citation-Rendering
start = time.time()
parsed = renderer._parse_ieee_citations(text)
duration = time.time() - start
assert duration < 0.05  # <50ms
```

---

## 🐛 Bekannte Issues & Workarounds

### Issue 1: LLM vergisst Zitationen

**Problem:** LLM generiert Antwort ohne [N] Zitationen

**Workaround:**
- Verstärke Prompt-Instruktionen: "ZWINGEND [N] verwenden!"
- Erhöhe Temperature (mehr Kreativität → mehr Zitationen)
- Post-Processing: Automatisches Einfügen von [N] bei erkannten Fakten

### Issue 2: Citation-Nummer stimmt nicht überein

**Problem:** LLM schreibt [1], aber Quelle ist an Position [2]

**Workaround:**
- Validierung in `_create_source_metadata()`: Prüfe ob citation_id <= len(chunks)
- Logging: Warne wenn Mismatch erkannt wird

### Issue 3: Suggestion-Parsing scheitert

**Problem:** LLM verwendet unbekanntes Format (z.B. "Nächste Fragen:")

**Workaround:**
- Erweitere Regex-Patterns in `_extract_suggestions()`
- Fallback: Suche nach "?" am Zeilenende

---

## 🚀 Test-Ausführung

### Manual Testing (Empfohlen für v3.19.0)

```bash
# 1. Backend starten
cd c:\VCC\veritas
python backend.py

# 2. Frontend starten (separates Terminal)
python start_frontend.py

# 3. Test-Queries senden:
# - "Welche Bauvorschriften gelten in Baden-Württemberg?"
# - "Was regelt § 58 LBO BW?"
# - "Welche Fristen gelten für Baugenehmigungen?"

# 4. Prüfe:
# - [1], [2] in Antwort
# - Quellen-Section: IEEE-Format
# - Vorschläge-Section: Klickbare Links
# - Click auf [1] → Scroll + Highlight
# - Click auf Vorschlag → Auto-Send
```

### Automated Testing (TODO)

```python
# tests/test_v3_19_0_citations.py
def test_citation_generation():
    # Test _create_source_metadata()
    pass

def test_suggestion_extraction():
    # Test _extract_suggestions()
    pass

def test_citation_parsing():
    # Test _parse_ieee_citations()
    pass

# pytest tests/test_v3_19_0_citations.py -v
```

---

## ✅ Validation Checklist

### Backend (Sprint 1.1 + 1.2)
- [x] `veritas_enhanced_prompts.py`: Source-List + Citation-Instruktionen
- [x] `veritas_api_module.py`: Source-List-Formatierung (Lines 360-365)
- [x] `veritas_api_module.py`: `_extract_suggestions()` (Lines 135-185)
- [x] `veritas_api_module.py`: `_create_source_metadata()` (Lines 187-230)
- [x] `veritas_api_endpoint.py`: `SourceMetadata` Model (Lines 342-353)
- [x] Syntax-Fehler: Keine ✅

### Frontend (Sprint 1.3 + 1.4)
- [x] `veritas_ui_markdown.py`: `_parse_ieee_citations()` (Lines 483-513)
- [x] `veritas_ui_markdown.py`: `_render_citation()` (Lines 515-571)
- [x] `veritas_ui_markdown.py`: `_scroll_to_source()` (Lines 573-609)
- [x] `veritas_ui_markdown.py`: Tag `citation_superscript` konfiguriert
- [x] `veritas_ui_chat_formatter.py`: `_insert_single_source_ieee()` (Lines 1645-1761)
- [x] `veritas_ui_chat_formatter.py`: `_add_ieee_source_tooltip()` (Lines 1763-1807)
- [x] Syntax-Fehler: Keine ✅

### Frontend (Sprint 2)
- [x] `veritas_ui_chat_formatter.py`: `_insert_suggestion_link()` (Lines 1463-1525)
- [x] `veritas_ui_chat_formatter.py`: Tags `suggestion_prefix`, `suggestion_link`
- [x] `veritas_app.py`: `_on_suggestion_clicked()` (Lines 658-692)
- [x] `veritas_app.py`: `_send_query_from_suggestion()` (Lines 694-705)
- [x] `veritas_app.py`: `suggestion_click_callback` Integration (Line 2005)
- [x] Syntax-Fehler: Keine ✅

### Integration
- [x] Backend → Frontend: `sources_metadata` durchgereicht
- [x] Backend → Frontend: `suggestions` durchgereicht
- [x] Frontend: `_render_assistant_message_structured()` nutzt beide

---

## 📝 Test-Protokoll

### Test-Session: [DATUM EINFÜGEN]

**Tester:** [NAME]  
**Version:** v3.19.0  
**Umgebung:** Windows, Python 3.13, Backend: Ollama, Frontend: Tkinter  

| Test-ID | Feature | Status | Bemerkungen |
|---------|---------|--------|-------------|
| T1 | Backend Citation Gen | ⏳ | - |
| T2 | SourceMetadata Creation | ⏳ | - |
| T3 | Suggestion Extraction | ⏳ | - |
| T4 | Citation Parsing | ⏳ | - |
| T5 | Citation Click → Scroll | ⏳ | - |
| T6 | IEEE-Format Display | ⏳ | - |
| T7 | Suggestion Click → Send | ⏳ | - |
| T8 | End-to-End Workflow | ⏳ | - |

**Gesamtstatus:** ⏳ Pending

---

## 🎯 Nächste Schritte

1. **Manual Testing** durchführen (empfohlen zuerst)
2. **Test-Protokoll** ausfüllen
3. **Issues** dokumentieren (falls welche gefunden)
4. **Automated Tests** schreiben (optional, für CI/CD)
5. **Performance-Metriken** sammeln
6. **Dokumentation** finalisieren

---

## 📚 Referenzen

- **Implementierungsplan:** `docs/IEEE_CITATIONS_AND_CLICKABLE_SUGGESTIONS_v3.19.0.md`
- **Backend-Code:** `backend/api/veritas_api_module.py` (Lines 135-230, 360-495)
- **Frontend-Code:** `frontend/ui/veritas_ui_markdown.py` (Lines 483-609)
- **Formatter-Code:** `frontend/ui/veritas_ui_chat_formatter.py` (Lines 1463-1807)
- **App-Integration:** `frontend/veritas_app.py` (Lines 658-705, 2005)
