# Phase 5: Advanced RAG - Bestands-Analyse

**Datum:** 6. Oktober 2025  
**Status:** ğŸ” Analyse abgeschlossen

---

## ğŸ“Š Vorhandene RAG-Komponenten

### âœ… **Bereits Implementiert**

#### 1. **Re-Ranking Service** (`backend/agents/veritas_reranking_service.py`)
**Status:** âœ… VollstÃ¤ndig implementiert (468 Zeilen)

**Features:**
- âœ… Cross-Encoder Re-Ranking (`cross-encoder/ms-marco-MiniLM-L-6-v2`)
- âœ… ReRankingConfig (top_k, initial_k, batch_size, score_threshold)
- âœ… Cache-UnterstÃ¼tzung (optional)
- âœ… Graceful Degradation (funktioniert ohne sentence-transformers)
- âœ… Performance-Optimierung (Batch-Processing)

**Metriken:**
```python
- Latenz: ~50-100ms fÃ¼r 20 Dokumente (CPU)
- Latenz: ~10-20ms fÃ¼r 20 Dokumente (GPU)
- Memory: ~500MB fÃ¼r Modell
```

**Integration:**
- âœ… Bereits in `RAGContextService` integriert
- âœ… Enable/Disable via Config (`enable_reranking: bool`)
- âœ… Two-Stage Retrieval: UDS3 (Top-20) â†’ Re-Ranking (Top-5)

**Code-Beispiel:**
```python
from backend.agents.veritas_reranking_service import (
    get_reranking_service,
    ReRankingConfig
)

# Service initialisieren
reranking_service = get_reranking_service()

# Dokumente re-ranken
reranked_docs = await reranking_service.rerank_documents(
    query="Bauvorhaben Umweltauflagen",
    documents=initial_docs,  # Top-20 von UDS3
    top_k=5
)
```

---

#### 2. **RAG Evaluator** (`backend/evaluation/veritas_rag_evaluator.py`)
**Status:** âœ… VollstÃ¤ndig implementiert (839 Zeilen)

**Features:**
- âœ… **Retrieval-Metriken:**
  - Precision@K, Recall@K
  - MRR (Mean Reciprocal Rank)
  - NDCG@K (Normalized Discounted Cumulative Gain)
- âœ… **Context-Metriken:**
  - Context Relevance Score (LLM-as-Judge)
  - Context Precision/Recall
  - Graph Enrichment Rate
- âœ… **Answer-Metriken:**
  - Faithfulness
  - Completeness
  - Hallucination Rate

**Evaluation-Workflow:**
```python
@dataclass
class RetrievalMetrics:
    precision_at_k: Dict[int, float]  # @1, @5, @10
    recall_at_k: Dict[int, float]
    mean_reciprocal_rank: float
    ndcg_at_k: Dict[int, float]

@dataclass
class EvaluationResult:
    test_case_id: str
    query: str
    passed: bool
    retrieval_passed: bool
    context_passed: bool
    answer_passed: bool
    retrieved_docs: List[str]
    expected_docs: List[str]
```

**Integration:**
- âœ… Golden Dataset Support
- âœ… Automatisierte Evaluations
- âœ… Detaillierte Metriken-Reports

---

#### 3. **RAG Context Service** (`backend/agents/rag_context_service.py`)
**Status:** âœ… VollstÃ¤ndig implementiert (359 Zeilen)

**Features:**
- âœ… Unified RAG Interface
- âœ… UDS3 Integration (Vector + Graph + Relational)
- âœ… Re-Ranking Integration (optional)
- âœ… Normalized Response Formats

**RAGQueryOptions:**
```python
@dataclass
class RAGQueryOptions:
    limit_documents: int = 5
    include_vector: bool = True
    include_graph: bool = True
    include_relational: bool = True
    
    # Re-Ranking
    enable_reranking: bool = True
    reranking_initial_k: int = 20  # Initial Retrieval
    reranking_final_k: int = 5     # Nach Re-Ranking
```

**Two-Stage Retrieval:**
```
Stage 1: UDS3 Vector Search â†’ Top-20 (Recall-optimiert)
Stage 2: Cross-Encoder Re-Ranking â†’ Top-5 (Precision-optimiert)
Stage 3: Graph Context Synthesis
```

---

### âŒ **Noch NICHT Implementiert**

#### 1. **Sparse Retrieval (BM25/SPLADE)**
**Status:** âŒ Fehlt komplett

**Was fehlt:**
- BM25 Lexikalisches Matching
- SPLADE (optional)
- Hybrid Search (Dense + Sparse)
- Reciprocal Rank Fusion (RRF)

**Impact:** 
- Kein lexikalisches Matching fÃ¼r exakte Begriffe (Â§ 242 BGB, DIN 18040-1)
- Keine Hybrid-Search-Vorteile

---

#### 2. **Semantic Chunking**
**Status:** âŒ Fehlt komplett

**Aktuell:** Vermutlich fixed-size Chunking in UDS3

**Was fehlt:**
- Semantic-basierte Chunk-Grenzen
- Sentence-Level Similarity
- Overlap-Strategien fÃ¼r Context-Preservation

**Impact:**
- Chunk-Grenzen schneiden SÃ¤tze/AbsÃ¤tze durch
- Kontext-Verlust an Chunk-Grenzen

---

#### 3. **Hierarchical Chunking**
**Status:** âŒ Fehlt komplett

**Was fehlt:**
- Multi-Level Chunks (Paragraph â†’ Section â†’ Document)
- Parent-Child-Relationen
- Context-Expansion bei Retrieval

**Impact:**
- Kein hierarchischer Kontext
- Keine automatische Context-Expansion

---

#### 4. **Query Expansion & Reformulation**
**Status:** âŒ Fehlt komplett

**Was fehlt:**
- LLM-basierte Query-Expansion
- Synonym-Expansion
- Multi-Query Generation
- Query-Rewriting

**Impact:**
- Single-Query Retrieval (keine Multi-Perspektive)
- Kein verbesserter Recall durch Query-Varianten

---

#### 5. **MMR Diversity Re-Ranking**
**Status:** âŒ Fehlt

**Aktuell:** Cross-Encoder Re-Ranking (relevance-only)

**Was fehlt:**
- Maximal Marginal Relevance (MMR)
- Diversity-basierte Re-Ranking
- Deduplication

**Impact:**
- MÃ¶gliche redundante Results
- Keine Diversity-Optimierung

---

## ğŸ¯ Phase 5 - Angepasste Roadmap

### Was wir ÃœBERSPRINGEN kÃ¶nnen:
- âœ… **Cross-Encoder Re-Ranking** - Bereits vollstÃ¤ndig implementiert
- âœ… **Evaluation Framework** - Bereits vorhanden (NDCG, MRR, Precision@K, Recall@K)
- âœ… **RAG Integration Layer** - Bereits vorhanden (RAGContextService)

### Was wir IMPLEMENTIEREN sollten:

#### **Phase 5.1: Sparse Retrieval & Hybrid Search** (PrioritÃ¤t 1)
**Zeitaufwand:** 6-8 Stunden  
**Code:** 300-400 Zeilen

**Deliverables:**
1. BM25 Sparse Retriever
2. Reciprocal Rank Fusion (RRF)
3. Hybrid Search Pipeline (Dense + Sparse)
4. Integration in RAGContextService

**Neue Features:**
```python
class HybridRetriever:
    async def retrieve_hybrid(
        query: str,
        top_k: int = 50
    ) -> List[ScoredChunk]:
        # Dense Retrieval (UDS3 Embeddings)
        dense_results = await uds3.vector_search(query, k=50)
        
        # Sparse Retrieval (BM25)
        sparse_results = await bm25.search(query, k=50)
        
        # Reciprocal Rank Fusion
        fused_results = rrf.fuse(dense_results, sparse_results)
        
        return fused_results[:top_k]
```

---

#### **Phase 5.2: Query Expansion** (PrioritÃ¤t 2)
**Zeitaufwand:** 4-5 Stunden  
**Code:** 200-300 Zeilen

**Deliverables:**
1. LLM-basierte Query-Expansion
2. Multi-Query Generation
3. Query-Rewriting
4. Integration in RAGContextService

**Neue Features:**
```python
class QueryExpander:
    async def expand_query(
        query: str,
        num_expansions: int = 3
    ) -> List[str]:
        # LLM-basierte Expansion
        expansions = await llm.expand(query, n=num_expansions)
        
        return [query] + expansions

# Usage
expanded_queries = await expander.expand_query(
    "Bauvorhaben Umweltauflagen"
)
# â†’ ["Bauvorhaben Umweltauflagen",
#    "Umweltrechtliche Genehmigungen Bauprojekte",
#    "UmweltvertrÃ¤glichkeitsprÃ¼fung Bauantrag"]
```

---

#### **Phase 5.3: MMR Diversity Re-Ranking** (PrioritÃ¤t 3)
**Zeitaufwand:** 3-4 Stunden  
**Code:** 150-250 Zeilen

**Deliverables:**
1. Maximal Marginal Relevance (MMR)
2. Integration mit bestehendem Cross-Encoder
3. Configurable Diversity-Parameter

**Neue Features:**
```python
class MMRReranker:
    async def rerank_with_diversity(
        query: str,
        candidates: List[Chunk],
        top_k: int = 10,
        lambda_param: float = 0.5  # 0.5 = Balance Relevance/Diversity
    ) -> List[Chunk]:
        # MMR Iterative Selection
        selected = []
        remaining = candidates.copy()
        
        for _ in range(top_k):
            mmr_scores = [
                lambda_param * relevance(q, c) - 
                (1 - lambda_param) * max_similarity(c, selected)
                for c in remaining
            ]
            best = remaining[argmax(mmr_scores)]
            selected.append(best)
            remaining.remove(best)
        
        return selected
```

---

#### **Phase 5.4: Semantic Chunking** (PrioritÃ¤t 4 - Optional)
**Zeitaufwand:** 4-6 Stunden  
**Code:** 300-400 Zeilen

**Deliverables:**
1. Semantic-basierte Chunking-Strategie
2. Sentence-Level Similarity
3. Overlap-Strategien
4. Integration in UDS3 Preprocessing

**Hinweis:** BenÃ¶tigt Ã„nderungen in UDS3 Document-Preprocessing

---

## ğŸ“Š Angepasste Code-SchÃ¤tzung

| Komponente | Original (Design) | Angepasst (RealitÃ¤t) | Status |
|------------|-------------------|----------------------|--------|
| **Re-Ranking (Cross-Encoder)** | 250-350 | **0** (bereits vorhanden) | âœ… SKIP |
| **Evaluation Framework** | 250-350 | **0** (bereits vorhanden) | âœ… SKIP |
| **Hybrid Search (BM25 + RRF)** | 300-400 | **350** | ğŸ”„ TODO |
| **Query Expansion** | 200-300 | **250** | ğŸ”„ TODO |
| **MMR Diversity** | 150-250 | **200** | ğŸ”„ TODO |
| **Semantic Chunking** | 300-400 | **350** (optional) | â¸ï¸ OPTIONAL |
| **Integration & Config** | 150-200 | **100** | ğŸ”„ TODO |
| **Tests** | 400-500 | **200** | ğŸ”„ TODO |
| **Dokumentation** | 500-600 | **300** | ğŸ”„ TODO |
| **GESAMT** | **2350-3100** | **1450-1750** | **-40%** |

**Zeitersparnis:** ~1 Tag (weil Re-Ranking & Evaluation bereits fertig)

---

## ğŸš€ Empfohlene NÃ¤chste Schritte

### **Option A: Minimaler Impact (Hybrid Search only)**
**Zeitaufwand:** 1 Tag  
**Code:** ~550 Zeilen

**Implementiere:**
1. âœ… Sparse Retrieval (BM25)
2. âœ… Reciprocal Rank Fusion
3. âœ… Hybrid Search Integration

**Vorteile:**
- GrÃ¶ÃŸter Impact/Aufwand-Ratio
- Lexikalisches Matching fÃ¼r exakte Begriffe
- KomplementÃ¤r zu bestehendem Dense-Retrieval

---

### **Option B: Moderate Impact (Hybrid + Query Expansion)**
**Zeitaufwand:** 1.5 Tage  
**Code:** ~850 Zeilen

**Implementiere:**
1. âœ… Sparse Retrieval + Hybrid Search
2. âœ… Query Expansion (LLM-basiert)
3. âœ… Multi-Query Generation

**Vorteile:**
- Hybrid Search + verbesserte Recall
- Multi-Perspektive Retrieval
- LLM-basierte Query-Verbesserung

---

### **Option C: VollstÃ¤ndig (All Features)**
**Zeitaufwand:** 2-2.5 Tage  
**Code:** ~1450-1750 Zeilen

**Implementiere:**
1. âœ… Hybrid Search
2. âœ… Query Expansion
3. âœ… MMR Diversity Re-Ranking
4. â¸ï¸ Semantic Chunking (optional)

**Vorteile:**
- VollstÃ¤ndiges Advanced RAG System
- Alle State-of-the-Art Techniken
- Maximale Retrieval-QualitÃ¤t

---

## ğŸ—³ï¸ Empfehlung

**Ich empfehle: Option B - Moderate Impact**

**BegrÃ¼ndung:**
1. âœ… Hybrid Search ist **Must-Have** (grÃ¶ÃŸter Impact)
2. âœ… Query Expansion ist **High-Value** (besserer Recall)
3. âŒ MMR ist **Nice-to-Have** (Cross-Encoder bereits sehr gut)
4. âŒ Semantic Chunking ist **UDS3-Dependent** (grÃ¶ÃŸerer Scope)

**NÃ¤chster Schritt:**
Sollen wir mit **Phase 5.1: Hybrid Search (BM25 + RRF)** starten?

- **Ja** â†’ Implementiere BM25 Sparse Retriever
- **Alternative** â†’ WÃ¤hle andere Option (A oder C)
- **Review** â†’ Design nochmal reviewen
