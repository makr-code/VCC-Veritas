# Phase 5: Advanced RAG Pipeline - Design-Dokument

**Version:** 1.0  
**Datum:** 6. Oktober 2025  
**Status:** ðŸ”„ In Planung  
**Autor:** VERITAS Development Team

---

## ðŸ“‹ Executive Summary

### Zielsetzung
Verbesserung der RAG-Pipeline mit state-of-the-art Retrieval-Techniken zur Steigerung der Antwort-QualitÃ¤t und Relevanz.

### Motivation
Die aktuelle RAG-Pipeline nutzt **Dense Retrieval** (Embeddings + Cosine-Similarity). Moderne RAG-Systeme kombinieren jedoch:
- **Hybrid Search** (Dense + Sparse)
- **Multi-Stage Retrieval** mit Re-Ranking
- **Semantic Chunking** statt fixed-size
- **Query Expansion** fÃ¼r besseren Recall

### Erfolgs-Kriterien

| Metrik | Aktuell (Baseline) | Ziel (Phase 5) | Verbesserung |
|--------|-------------------|----------------|--------------|
| **NDCG@10** | 0.65 (geschÃ¤tzt) | > 0.80 | +23% |
| **MRR** | 0.55 (geschÃ¤tzt) | > 0.75 | +36% |
| **Recall@10** | 0.70 (geschÃ¤tzt) | > 0.85 | +21% |
| **Retrieval-Latenz** | ~200ms | < 300ms | Max +50% |
| **Chunk-Quality** | Fixed-size | Semantic | Qualitativ |

---

## ðŸ—ï¸ Architektur-Ãœbersicht

### High-Level-Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ADVANCED RAG PIPELINE (Phase 5)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  1. QUERY PROCESSING                                   â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ Query        â”‚  â”‚ Multi-Query  â”‚  â”‚ Query       â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ Expansion    â”‚â†’ â”‚ Generation   â”‚â†’ â”‚ Rewrite     â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚         â†“                                               â”‚    â”‚
â”‚  â”‚  Original Query + Expanded Queries + Reformulations    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  2. HYBRID RETRIEVAL                                   â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚    â”‚
â”‚  â”‚  â”‚ Dense        â”‚          â”‚ Sparse       â”‚            â”‚    â”‚
â”‚  â”‚  â”‚ Retrieval    â”‚          â”‚ Retrieval    â”‚            â”‚    â”‚
â”‚  â”‚  â”‚ (Embeddings) â”‚          â”‚ (BM25/SPLADE)â”‚            â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚    â”‚
â”‚  â”‚         â”‚                         â”‚                     â”‚    â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚    â”‚
â”‚  â”‚                  â†“                                      â”‚    â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚    â”‚
â”‚  â”‚         â”‚ Result Fusion  â”‚ (Reciprocal Rank Fusion)    â”‚    â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚    â”‚
â”‚  â”‚                  â†“                                      â”‚    â”‚
â”‚  â”‚         Top-K Candidates (k=50-100)                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  3. RE-RANKING                                         â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ Cross-       â”‚â†’ â”‚ Diversity    â”‚â†’ â”‚ Relevance   â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ Encoder      â”‚  â”‚ Re-Ranking   â”‚  â”‚ Feedback    â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚         â†“                                               â”‚    â”‚
â”‚  â”‚  Top-N Final Results (n=5-10)                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  4. CONTEXT AUGMENTATION                               â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚    â”‚
â”‚  â”‚  â”‚ Chunk        â”‚  â”‚ Metadata     â”‚                    â”‚    â”‚
â”‚  â”‚  â”‚ Expansion    â”‚  â”‚ Enrichment   â”‚                    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚    â”‚
â”‚  â”‚         â†“                                               â”‚    â”‚
â”‚  â”‚  Augmented Context â†’ LLM                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”§ Komponenten-Design

### 1. Query Processing Layer

#### 1.1 Query Expansion
**Ziel:** Erweitere Query um Synonyme, verwandte Begriffe, Kontextvariationen

**Implementierung:**
```python
class QueryExpander:
    """LLM-basierte Query-Expansion fÃ¼r besseren Recall"""
    
    def __init__(self, llm_client: OllamaClient):
        self.llm = llm_client
        
    async def expand_query(
        self, 
        query: str,
        num_expansions: int = 3
    ) -> List[str]:
        """
        Generiert erweiterte Query-Varianten
        
        Returns:
            [original_query, expansion_1, expansion_2, ...]
        """
        prompt = f"""
        Original Query: "{query}"
        
        Generiere {num_expansions} semantisch Ã¤hnliche Umformulierungen:
        1. Verwende Synonyme
        2. FÃ¼ge relevante Kontextbegriffe hinzu
        3. Behalte die Kernintention bei
        
        Format: Eine Umformulierung pro Zeile
        """
        
        response = await self.llm.generate_response(prompt)
        expansions = [query]  # Original immer dabei
        expansions.extend(response.strip().split('\n'))
        
        return expansions[:num_expansions + 1]
```

**Beispiel:**
```
Original: "Bauvorhaben Umweltauflagen Berlin"
Expansion 1: "Umweltrechtliche Genehmigungen fÃ¼r Bauprojekte in Berlin"
Expansion 2: "UmweltvertrÃ¤glichkeitsprÃ¼fung Bauantrag Berlin"
Expansion 3: "Ã–kologische Auflagen Hochbau Berlin-Mitte"
```

---

#### 1.2 Multi-Query Generation
**Ziel:** Generiere multiple Perspektiven der Query fÃ¼r umfassendere Retrieval

**Implementierung:**
```python
class MultiQueryGenerator:
    """Generiert multiple Query-Perspektiven"""
    
    async def generate_multi_queries(
        self,
        query: str,
        num_queries: int = 3
    ) -> List[str]:
        """
        Generiert verschiedene Perspektiven/Aspekte der Query
        
        Returns:
            Liste von Queries aus verschiedenen Perspektiven
        """
        prompt = f"""
        User-Query: "{query}"
        
        Generiere {num_queries} verschiedene Such-Perspektiven:
        1. Rechtliche Perspektive
        2. Technische Perspektive  
        3. Prozessuale Perspektive
        
        Jede Perspektive als separate Query.
        """
        
        response = await self.llm.generate_response(prompt)
        queries = [query]  # Original
        queries.extend(response.strip().split('\n'))
        
        return queries[:num_queries + 1]
```

---

### 2. Hybrid Retrieval Layer

#### 2.1 Dense Retrieval (Embeddings)
**Aktuell:** Sentence-Transformers Embeddings + Cosine-Similarity  
**Neu:** Erweitert um Multi-Query Support

**Komponenten:**
```python
class DenseRetriever:
    """Embedding-basierte Dense Retrieval"""
    
    def __init__(
        self,
        embedding_model: str = "all-MiniLM-L6-v2",
        vector_store: VectorStore
    ):
        self.model = SentenceTransformer(embedding_model)
        self.vector_store = vector_store
        
    async def retrieve(
        self,
        queries: List[str],  # Multi-Query Support
        top_k: int = 50
    ) -> List[ScoredChunk]:
        """
        Dense Retrieval fÃ¼r multiple Queries
        
        Returns:
            Top-K Chunks mit Dense-Scores
        """
        all_results = []
        
        for query in queries:
            query_embedding = self.model.encode(query)
            results = await self.vector_store.similarity_search(
                query_embedding,
                k=top_k
            )
            all_results.extend(results)
        
        # Dedupliziere & merge Scores
        return self._merge_results(all_results, top_k)
```

---

#### 2.2 Sparse Retrieval (BM25/SPLADE)
**Ziel:** Lexikalisches Matching fÃ¼r exakte Begriffe, Akronyme, Zahlen

**BM25 Implementation:**
```python
from rank_bm25 import BM25Okapi

class SparseRetriever:
    """BM25-basierte Sparse Retrieval"""
    
    def __init__(self, corpus: List[str]):
        # Tokenize Corpus
        self.tokenized_corpus = [
            self._tokenize(doc) for doc in corpus
        ]
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        self.corpus = corpus
        
    def _tokenize(self, text: str) -> List[str]:
        """Einfache Tokenization (kann erweitert werden)"""
        return text.lower().split()
        
    async def retrieve(
        self,
        queries: List[str],
        top_k: int = 50
    ) -> List[ScoredChunk]:
        """
        BM25 Retrieval fÃ¼r multiple Queries
        
        Returns:
            Top-K Chunks mit BM25-Scores
        """
        all_scores = []
        
        for query in queries:
            tokenized_query = self._tokenize(query)
            scores = self.bm25.get_scores(tokenized_query)
            all_scores.append(scores)
        
        # Merge Scores (Max, Sum, oder Average)
        merged_scores = np.max(all_scores, axis=0)
        
        # Top-K Indizes
        top_indices = np.argsort(merged_scores)[-top_k:][::-1]
        
        return [
            ScoredChunk(
                chunk=self.corpus[idx],
                score=merged_scores[idx],
                source="bm25"
            )
            for idx in top_indices
        ]
```

**Warum BM25?**
- âœ… Gut fÃ¼r exakte Begriffe (z.B. "Â§ 242 BGB", "DIN 18040-1")
- âœ… Akronyme & AbkÃ¼rzungen (z.B. "UVP", "VOB")
- âœ… Zahlen & Daten (z.B. "2024", "50.000 EUR")
- âœ… KomplementÃ¤r zu Dense-Retrieval

---

#### 2.3 Result Fusion (Reciprocal Rank Fusion)
**Ziel:** Kombiniere Dense + Sparse Results optimal

**RRF Formula:**
```
RRF_score(d) = Î£_{r âˆˆ R} 1 / (k + rank_r(d))

Wobei:
- R = Liste der Retriever (Dense, Sparse)
- rank_r(d) = Rang von Dokument d in Retriever r
- k = Konstante (typisch 60)
```

**Implementation:**
```python
class ReciprocalRankFusion:
    """Reciprocal Rank Fusion fÃ¼r Hybrid-Results"""
    
    def __init__(self, k: int = 60):
        self.k = k
        
    def fuse(
        self,
        dense_results: List[ScoredChunk],
        sparse_results: List[ScoredChunk],
        top_k: int = 50
    ) -> List[ScoredChunk]:
        """
        Fusioniert Dense + Sparse Results via RRF
        
        Returns:
            Top-K fusionierte Results mit RRF-Scores
        """
        # Chunk-ID â†’ RRF-Score Mapping
        rrf_scores = {}
        
        # Dense Results
        for rank, chunk in enumerate(dense_results):
            chunk_id = chunk.chunk_id
            rrf_scores[chunk_id] = rrf_scores.get(chunk_id, 0) + \
                1 / (self.k + rank + 1)
        
        # Sparse Results
        for rank, chunk in enumerate(sparse_results):
            chunk_id = chunk.chunk_id
            rrf_scores[chunk_id] = rrf_scores.get(chunk_id, 0) + \
                1 / (self.k + rank + 1)
        
        # Sortiere nach RRF-Score
        sorted_chunks = sorted(
            rrf_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        return [
            ScoredChunk(
                chunk_id=chunk_id,
                score=score,
                source="rrf"
            )
            for chunk_id, score in sorted_chunks[:top_k]
        ]
```

**Warum RRF?**
- âœ… Rank-basiert (nicht score-basiert) â†’ robuster gegen unterschiedliche Score-Skalen
- âœ… Einfach & effektiv
- âœ… State-of-the-Art in Multi-Retriever-Fusion

---

### 3. Re-Ranking Layer

#### 3.1 Cross-Encoder Re-Ranker
**Ziel:** PrÃ¤zise Relevanz-Bewertung fÃ¼r Top-K Kandidaten

**Modell:** `cross-encoder/ms-marco-MiniLM-L-6-v2` (22M params, schnell)

**Implementation:**
```python
from sentence_transformers import CrossEncoder

class CrossEncoderReranker:
    """Cross-Encoder basiertes Re-Ranking"""
    
    def __init__(
        self,
        model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    ):
        self.model = CrossEncoder(model_name)
        
    async def rerank(
        self,
        query: str,
        candidates: List[ScoredChunk],
        top_k: int = 10
    ) -> List[ScoredChunk]:
        """
        Re-rankt Kandidaten via Cross-Encoder
        
        Returns:
            Top-K re-ranked Chunks
        """
        # Prepare Query-Chunk Pairs
        pairs = [
            [query, candidate.chunk_text]
            for candidate in candidates
        ]
        
        # Cross-Encoder Scores
        scores = self.model.predict(pairs)
        
        # Merge mit Original-Candidates
        for candidate, score in zip(candidates, scores):
            candidate.rerank_score = score
        
        # Sortiere nach Rerank-Score
        reranked = sorted(
            candidates,
            key=lambda x: x.rerank_score,
            reverse=True
        )
        
        return reranked[:top_k]
```

**Performance:**
- **Latenz:** ~10ms pro Chunk (GPU), ~50ms (CPU)
- **Accuracy:** +15-20% NDCG@10 vs Bi-Encoder

---

#### 3.2 Diversity Re-Ranking (MMR)
**Ziel:** Maximiere Diversity in Results (vermeide redundante Chunks)

**MMR Formula:**
```
MMR = argmax_{D_i âˆˆ R \ S} [
    Î» * Similarity(Q, D_i) - (1-Î») * max_{D_j âˆˆ S} Similarity(D_i, D_j)
]

Wobei:
- Q = Query
- R = Kandidaten-Set
- S = Bereits ausgewÃ¤hlte Chunks
- Î» = Diversity-Parameter (0.5 = Balance)
```

**Implementation:**
```python
class MaximalMarginalRelevance:
    """MMR-basierte Diversity Re-Ranking"""
    
    def __init__(self, lambda_param: float = 0.5):
        self.lambda_param = lambda_param
        
    async def rerank(
        self,
        query_embedding: np.ndarray,
        candidates: List[ScoredChunk],
        top_k: int = 10
    ) -> List[ScoredChunk]:
        """
        MMR Re-Ranking fÃ¼r Diversity
        
        Returns:
            Top-K diverse Chunks
        """
        selected = []
        remaining = candidates.copy()
        
        # Iterativ auswÃ¤hlen
        for _ in range(min(top_k, len(remaining))):
            mmr_scores = []
            
            for candidate in remaining:
                # Relevanz-Score
                relevance = self._cosine_similarity(
                    query_embedding,
                    candidate.embedding
                )
                
                # Diversity-Score (max similarity zu bereits gewÃ¤hlten)
                if selected:
                    diversity = max([
                        self._cosine_similarity(
                            candidate.embedding,
                            s.embedding
                        )
                        for s in selected
                    ])
                else:
                    diversity = 0
                
                # MMR Score
                mmr = self.lambda_param * relevance - \
                      (1 - self.lambda_param) * diversity
                
                mmr_scores.append((candidate, mmr))
            
            # WÃ¤hle besten MMR-Score
            best_candidate, best_score = max(
                mmr_scores,
                key=lambda x: x[1]
            )
            
            selected.append(best_candidate)
            remaining.remove(best_candidate)
        
        return selected
```

---

### 4. Advanced Chunking Strategies

#### 4.1 Semantic Chunking
**Problem mit Fixed-Size Chunking:**
- Schneidet SÃ¤tze/AbsÃ¤tze mitten durch
- Verliert semantischen Kontext
- Chunk-Grenzen willkÃ¼rlich

**Semantic Chunking Ansatz:**
```python
class SemanticChunker:
    """Semantic-basiertes Chunking (nicht fixed-size)"""
    
    def __init__(
        self,
        embedding_model: SentenceTransformer,
        similarity_threshold: float = 0.7
    ):
        self.model = embedding_model
        self.threshold = similarity_threshold
        
    async def chunk_document(
        self,
        text: str,
        min_chunk_size: int = 100,
        max_chunk_size: int = 500
    ) -> List[str]:
        """
        Chunked Dokument semantisch
        
        Returns:
            Liste semantisch kohÃ¤renter Chunks
        """
        # Split in SÃ¤tze
        sentences = self._split_sentences(text)
        
        # Embed Sentences
        embeddings = self.model.encode(sentences)
        
        chunks = []
        current_chunk = [sentences[0]]
        current_embedding = embeddings[0]
        
        for i in range(1, len(sentences)):
            sentence = sentences[i]
            embedding = embeddings[i]
            
            # Similarity zum aktuellen Chunk
            similarity = self._cosine_similarity(
                current_embedding,
                embedding
            )
            
            # Wenn Ã¤hnlich & nicht zu groÃŸ â†’ hinzufÃ¼gen
            if (similarity >= self.threshold and 
                len(' '.join(current_chunk)) < max_chunk_size):
                current_chunk.append(sentence)
                # Update Chunk-Embedding (Mean)
                current_embedding = np.mean([current_embedding, embedding], axis=0)
            else:
                # Neuer Chunk
                chunks.append(' '.join(current_chunk))
                current_chunk = [sentence]
                current_embedding = embedding
        
        # Letzter Chunk
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
```

**Vorteile:**
- âœ… Semantisch kohÃ¤rente Chunks
- âœ… Keine abgeschnittenen SÃ¤tze
- âœ… Bessere Kontext-Preservation

---

#### 4.2 Hierarchical Chunking
**Ziel:** Multi-Level Chunks (Paragraph â†’ Section â†’ Document)

**Implementation:**
```python
class HierarchicalChunker:
    """Hierarchical Chunking mit Parent-Child-Relationen"""
    
    async def chunk_hierarchical(
        self,
        text: str
    ) -> List[HierarchicalChunk]:
        """
        Erstellt hierarchische Chunk-Struktur
        
        Returns:
            Liste von Chunks mit Parent-Child-Links
        """
        # Level 1: Document
        doc_chunk = HierarchicalChunk(
            text=text,
            level=1,
            parent_id=None
        )
        
        # Level 2: Sections (via Headers)
        sections = self._split_by_headers(text)
        section_chunks = []
        
        for section in sections:
            section_chunk = HierarchicalChunk(
                text=section,
                level=2,
                parent_id=doc_chunk.chunk_id
            )
            section_chunks.append(section_chunk)
            
            # Level 3: Paragraphs
            paragraphs = self._split_paragraphs(section)
            for para in paragraphs:
                para_chunk = HierarchicalChunk(
                    text=para,
                    level=3,
                    parent_id=section_chunk.chunk_id
                )
                section_chunks.append(para_chunk)
        
        return [doc_chunk] + section_chunks
```

**Retrieval-Strategie:**
1. Retrieve auf Paragraph-Level (Level 3)
2. Bei Match: Hole Parent-Section fÃ¼r mehr Kontext
3. Bei Bedarf: Hole gesamtes Document

---

## ðŸ“Š Evaluation-Framework

### Retrieval-Metriken

#### 1. NDCG@K (Normalized Discounted Cumulative Gain)
```python
def ndcg_at_k(
    relevant_docs: List[str],
    retrieved_docs: List[str],
    k: int = 10
) -> float:
    """
    NDCG@K: Misst Ranking-QualitÃ¤t mit Position-Discount
    
    Returns:
        NDCG Score (0.0 - 1.0)
    """
    dcg = sum([
        1 / np.log2(i + 2) 
        for i, doc in enumerate(retrieved_docs[:k]) 
        if doc in relevant_docs
    ])
    
    idcg = sum([
        1 / np.log2(i + 2) 
        for i in range(min(k, len(relevant_docs)))
    ])
    
    return dcg / idcg if idcg > 0 else 0.0
```

---

#### 2. MRR (Mean Reciprocal Rank)
```python
def mean_reciprocal_rank(
    relevant_docs: List[str],
    retrieved_docs: List[str]
) -> float:
    """
    MRR: Position des ersten relevanten Dokuments
    
    Returns:
        MRR Score (0.0 - 1.0)
    """
    for i, doc in enumerate(retrieved_docs):
        if doc in relevant_docs:
            return 1 / (i + 1)
    return 0.0
```

---

#### 3. Recall@K
```python
def recall_at_k(
    relevant_docs: List[str],
    retrieved_docs: List[str],
    k: int = 10
) -> float:
    """
    Recall@K: Anteil relevanter Docs in Top-K
    
    Returns:
        Recall (0.0 - 1.0)
    """
    retrieved_set = set(retrieved_docs[:k])
    relevant_set = set(relevant_docs)
    
    if not relevant_set:
        return 0.0
    
    return len(retrieved_set & relevant_set) / len(relevant_set)
```

---

### Benchmark-Dataset

**Erstelle Test-Set:**
```python
class RAGBenchmark:
    """Benchmark fÃ¼r RAG-Evaluation"""
    
    def __init__(self):
        self.test_queries = [
            {
                "query": "Umweltauflagen fÃ¼r Bauvorhaben in Berlin",
                "relevant_chunks": ["chunk_123", "chunk_456", ...],
                "ideal_ranking": ["chunk_123", "chunk_456", ...]
            },
            # ... mehr Test-Queries
        ]
    
    async def evaluate_pipeline(
        self,
        pipeline: RAGPipeline
    ) -> Dict[str, float]:
        """
        Evaluiert RAG-Pipeline auf Test-Set
        
        Returns:
            {"ndcg@10": 0.78, "mrr": 0.65, "recall@10": 0.82}
        """
        ndcg_scores = []
        mrr_scores = []
        recall_scores = []
        
        for test_case in self.test_queries:
            query = test_case["query"]
            relevant = test_case["relevant_chunks"]
            
            # Retrieve
            results = await pipeline.retrieve(query, top_k=10)
            retrieved = [r.chunk_id for r in results]
            
            # Metriken
            ndcg_scores.append(ndcg_at_k(relevant, retrieved, k=10))
            mrr_scores.append(mean_reciprocal_rank(relevant, retrieved))
            recall_scores.append(recall_at_k(relevant, retrieved, k=10))
        
        return {
            "ndcg@10": np.mean(ndcg_scores),
            "mrr": np.mean(mrr_scores),
            "recall@10": np.mean(recall_scores)
        }
```

---

## ðŸ”„ Implementation-Roadmap

### Phase 5.1: Hybrid Search (Tag 1)
**Dauer:** 6-8 Stunden

**Todos:**
1. âœ… Design-Dokument finalisieren
2. â³ Sparse Retrieval (BM25) implementieren
3. â³ Reciprocal Rank Fusion implementieren
4. â³ Integration in VeritasRAGPipeline
5. â³ Unit-Tests fÃ¼r Hybrid-Retrieval

**Code-SchÃ¤tzung:** 300-400 Zeilen

---

### Phase 5.2: Re-Ranking System (Tag 1-2)
**Dauer:** 6-8 Stunden

**Todos:**
1. â³ Cross-Encoder Integration
2. â³ MMR Diversity Re-Ranking
3. â³ Konfigurierbare Re-Ranking-Pipeline
4. â³ Performance-Tests (Latenz)

**Code-SchÃ¤tzung:** 250-350 Zeilen

---

### Phase 5.3: Advanced Chunking (Tag 2)
**Dauer:** 4-6 Stunden

**Todos:**
1. â³ Semantic Chunker implementieren
2. â³ Hierarchical Chunker implementieren
3. â³ Backward-KompatibilitÃ¤t (Feature-Flag)
4. â³ Chunk-Quality Tests

**Code-SchÃ¤tzung:** 300-400 Zeilen

---

### Phase 5.4: Query Expansion (Tag 2)
**Dauer:** 4-5 Stunden

**Todos:**
1. â³ Query Expander (LLM-basiert)
2. â³ Multi-Query Generator
3. â³ Integration in Query-Pipeline
4. â³ A/B Tests (mit/ohne Expansion)

**Code-SchÃ¤tzung:** 200-300 Zeilen

---

### Phase 5.5: Evaluation & Benchmarks (Tag 3)
**Dauer:** 6-8 Stunden

**Todos:**
1. â³ Benchmark-Dataset erstellen (20-30 Test-Queries)
2. â³ Evaluation-Framework implementieren
3. â³ Baseline-Messung (aktuelles RAG)
4. â³ Advanced-RAG Messung
5. â³ Performance-Report erstellen

**Code-SchÃ¤tzung:** 250-350 Zeilen

---

### Phase 5.6: Documentation & Integration (Tag 3)
**Dauer:** 3-4 Stunden

**Todos:**
1. â³ Implementation-Report
2. â³ API-Dokumentation
3. â³ Migration-Guide
4. â³ Performance-Dashboard

**Code-SchÃ¤tzung:** Dokumentation (500-600 Zeilen)

---

## ðŸ“¦ Deliverables

| Komponente | Zeilen (geschÃ¤tzt) | Status |
|------------|-------------------|--------|
| **Hybrid Search** (Dense + Sparse + Fusion) | 300-400 | ðŸ”„ |
| **Re-Ranking** (Cross-Encoder + MMR) | 250-350 | ðŸ”„ |
| **Advanced Chunking** (Semantic + Hierarchical) | 300-400 | ðŸ”„ |
| **Query Expansion** (LLM + Multi-Query) | 200-300 | ðŸ”„ |
| **Evaluation Framework** (Benchmarks) | 250-350 | ðŸ”„ |
| **Integration & Config** | 150-200 | ðŸ”„ |
| **Tests** (Unit + Integration) | 400-500 | ðŸ”„ |
| **Dokumentation** (Implementation-Report) | 500-600 | ðŸ”„ |
| **GESAMT** | **2350-3100 Zeilen** | ðŸ”„ |

---

## ðŸŽ¯ Success-Kriterien

### Must-Have
- [x] Design-Dokument vollstÃ¤ndig
- [ ] Hybrid Search (Dense + Sparse) âœ…
- [ ] Re-Ranking (Cross-Encoder) âœ…
- [ ] Evaluation-Framework (NDCG@10, MRR, Recall@K) âœ…
- [ ] NDCG@10 > 0.80 (vs Baseline 0.65)
- [ ] Backward-KompatibilitÃ¤t 100%

### Should-Have
- [ ] MMR Diversity Re-Ranking
- [ ] Semantic Chunking
- [ ] Query Expansion (LLM-basiert)
- [ ] Performance-Dashboard

### Nice-to-Have
- [ ] Hierarchical Chunking
- [ ] Multi-Query Generation
- [ ] A/B Testing Infrastructure
- [ ] Caching-Layer fÃ¼r Embeddings

---

## ðŸš€ Getting Started

**NÃ¤chster Schritt:** Phase 5.1 - Hybrid Search Implementation

**Ready?** Lassen Sie uns mit der BM25 Sparse Retrieval Implementation beginnen! ðŸŽ¯
