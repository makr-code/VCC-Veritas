# Golden Dataset System fÃ¼r RAG Quality Improvement

**Version:** 2.0  
**Datum:** 10. Oktober 2025  
**Zweck:** Feedback-Schleife zur kontinuierlichen Verbesserung des Prompt-Designs

---

## ğŸ“‹ Ãœberblick

Das Golden Dataset System erweitert das RAG Quality Testing um:

1. **Alle Modelle testen** (nicht nur erste 3)
2. **Direkte Zitate** aus Rechtsquellen erkennen und bewerten
3. **Detaillierte Zeitmessung** (Retrieval, Generation, Network-Latenz)
4. **Feedback-Schleife** fÃ¼r iterative Prompt-Verbesserung
5. **Benchmark-Datensatz** fÃ¼r Modell-Vergleiche

---

## ğŸ¯ Zielsetzung

### Problem
Nach erstem Test (v3.19.0):
- âŒ **0% IEEE-Zitationen** (0/15 Tests)
- âŒ **0% Follow-up-VorschlÃ¤ge** (0/15 Tests)
- âŒ **32% Aspekt-Abdeckung** (zu niedrig)
- âŒ **Keine direkten Zitate** aus Rechtsquellen

### LÃ¶sung: Golden Dataset
1. **Baseline etablieren** (aktuelle QualitÃ¤t dokumentieren)
2. **Prompt optimieren** (basierend auf Daten)
3. **Re-evaluieren** (Verbesserung messen)
4. **Iterieren** bis Ziel-QualitÃ¤t erreicht

---

## ğŸ—ï¸ Architektur

### Komponenten

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GOLDEN DATASET GENERATOR (test_rag_quality_v3_19_0.py)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ALLE MODELLE TESTEN                                        â”‚
â”‚  - llama3.1:latest                                          â”‚
â”‚  - llama3.1:8b                                              â”‚
â”‚  - mistral:latest                                           â”‚
â”‚  - codellama:latest                                         â”‚
â”‚  - ... (alle verfÃ¼gbaren)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5 KOMPLEXE TESTFRAGEN (Multi-Aspekt)                      â”‚
â”‚  Q1: Baugenehmigung (Voraussetzungen, Fristen, Kosten)     â”‚
â”‚  Q2: Verfahrensarten (Unterschiede, Unterlagen)            â”‚
â”‚  Q3: Â§ 58 LBO BW (Inhalt, Vergleich, Ã„nderungen)           â”‚
â”‚  Q4: Brandschutz (Klassen, Wege, Stoffe, Normen)           â”‚
â”‚  Q5: Verfahrensablauf (Schritte, BehÃ¶rden, Rechtsmittel)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DETAILLIERTE ANALYSE                                       â”‚
â”‚  âœ“ IEEE-Zitationen [1], [2], [3]                           â”‚
â”‚  âœ“ Direkte Zitate "..."                                    â”‚
â”‚  âœ“ Paragraphen-Referenzen (Â§ 58 LBO BW)                    â”‚
â”‚  âœ“ Follow-up-VorschlÃ¤ge                                    â”‚
â”‚  âœ“ Aspekt-Abdeckung                                        â”‚
â”‚  âœ“ Timing-Metriken (ms-Genauigkeit)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GOLDEN DATASET OUTPUT                                      â”‚
â”‚  golden_dataset_1760123456.json                             â”‚
â”‚  {                                                          â”‚
â”‚    "Q1": {                                                  â”‚
â”‚      "question": "...",                                     â”‚
â”‚      "expected_...",                                        â”‚
â”‚      "model_results": {                                     â”‚
â”‚        "llama3.1:latest": {...},                            â”‚
â”‚        "mistral:latest": {...}                              â”‚
â”‚      }                                                      â”‚
â”‚    }                                                        â”‚
â”‚  }                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FEEDBACK-SCHLEIFE                                          â”‚
â”‚  1. Analyse: Welche Modelle performen am besten?           â”‚
â”‚  2. Identifikation: Welche Prompt-Aspekte fehlen?          â”‚
â”‚  3. Optimierung: Prompt-Anpassungen implementieren          â”‚
â”‚  4. Re-Test: Verbesserung validieren                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Datenstrukturen

### GoldenDatasetEntry

```python
@dataclass
class GoldenDatasetEntry:
    """
    Ein Eintrag im Golden Dataset
    EnthÃ¤lt Frage + erwartete QualitÃ¤t + tatsÃ¤chliche Ergebnisse pro Modell
    """
    question_id: str                    # "Q1", "Q2", ...
    question: str                        # Die komplexe Frage
    expected_aspects: List[str]          # ["Fristen", "Kosten", ...]
    expected_citations_min: int          # Mindestanzahl IEEE-Zitationen
    expected_quotes_min: int             # Mindestanzahl direkte Zitate
    expected_legal_refs_min: int         # Mindestanzahl Paragraphen
    
    model_results: Dict[str, Any]        # Ergebnisse pro Modell
    # {
    #   "llama3.1:latest": {
    #     "metrics": {...},
    #     "timing": {...},
    #     "quotes": [...],
    #     "rating": "GOOD"
    #   },
    #   "mistral:latest": {...}
    # }
```

### TimingMetrics

```python
@dataclass
class TimingMetrics:
    """Detaillierte Performance-Messung"""
    total_time: float              # Gesamtzeit (Client-seitig)
    retrieval_time: float          # RAG-Retrieval (Backend)
    generation_time: float         # LLM-Generierung (Backend)
    post_processing_time: float    # Citation-Extraktion etc.
    network_latency: float         # HTTP Round-Trip
```

### QuoteMetrics

```python
@dataclass
class QuoteMetrics:
    """Zitat-QualitÃ¤t"""
    direct_quotes_count: int       # Anzahl "..." Zitate
    quote_length_avg: float        # Durchschnittliche LÃ¤nge
    quotes_with_source: int        # Zitate mit [1] Referenz
    quotes: List[str]              # Die tatsÃ¤chlichen Zitate
```

---

## ğŸ” Erweiterte Metriken

### Neue Metriken in v2.0

| Metrik | Beschreibung | Ziel | Aktuell (v1) |
|--------|--------------|------|--------------|
| **direct_quotes_count** | Anzahl direkter Zitate aus Quellen | â‰¥2 | 0 |
| **quotes_with_source** | Zitate mit [1] Referenz | 100% | 0% |
| **quote_length_avg** | Ã˜ LÃ¤nge der Zitate (Zeichen) | 50-150 | 0 |
| **legal_references** | Â§ 58 LBO BW, Art. 14 GG | â‰¥3 | ~5% |
| **retrieval_time** | RAG-Retrieval-Zeit (ms) | <500ms | - |
| **generation_time** | LLM-Generation-Zeit (ms) | <5000ms | - |
| **quote_source_ratio** | % Zitate mit Quelle | >80% | 0% |

### Timing-Analyse

**Zweck:** Identifiziere Performance-Bottlenecks

```
Total Time:     18.2s
  â”œâ”€ Network Latency:     0.3s  (2%)
  â”œâ”€ Retrieval Time:      1.2s  (7%)   â† ChromaDB + Neo4j
  â”œâ”€ Generation Time:    15.8s (87%)  â† LLM Inference
  â””â”€ Post-Processing:     0.9s  (4%)   â† Citation Extraction
```

**Optimierungspotential:**
- Generation Time: Model-Auswahl (kleineres Modell?)
- Retrieval Time: Index-Optimierung, Caching
- Post-Processing: Parallel-Processing

---

## ğŸ“ Direkte Zitate - Anforderungen

### Was sind "direkte Zitate"?

```python
# BEISPIEL 1: Gutes direktes Zitat
"Nach Â§ 58 Abs. 1 LBO BW ist die Baugenehmigung schriftlich zu beantragen [1]. 
Der Antrag muss enthalten: 'Angaben zur Person des Bauherrn, Beschreibung des 
Bauvorhabens und Lage des BaugrundstÃ¼cks' [1]."
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                            Direktes Zitat aus LBO BW mit Quelle [1]

# BEISPIEL 2: Schlechtes "Paraphrase"
"Der Bauantrag muss Informationen zum Bauherrn und GrundstÃ¼ck enthalten."
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 Keine AnfÃ¼hrungszeichen = Paraphrase (weniger belastbar)
```

### Erkennungskriterien

```python
def extract_direct_quotes(answer: str) -> List[str]:
    """
    Erkennt:
    - "Text in AnfÃ¼hrungszeichen"
    - â€Text in deutschen AnfÃ¼hrungszeichen"
    - MindestlÃ¤nge: 20 Zeichen (um Kurz-Zitate zu filtern)
    """
    patterns = [
        r'"([^"]{20,})"',        # "langes Zitat"
        r'â€([^"]{20,})"',        # â€deutsches Zitat"
    ]
    ...
```

### Zitat-QualitÃ¤t bewerten

```python
# EXZELLENT (Quote + Source + Legal Ref)
"GemÃ¤ÃŸ Â§ 59 Abs. 2 LBO BW gilt: 'Die BaugenehmigungsbehÃ¶rde hat Ã¼ber 
den Bauantrag innerhalb von drei Monaten zu entscheiden' [1]."
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
                                  Zitat mit [1] + Â§ 59 Abs. 2

# GUT (Quote + Source)
"Die Frist betrÃ¤gt 'drei Monate' [1]."

# VERBESSERUNGSWÃœRDIG (Quote ohne Source)
"Es gilt eine Frist von 'drei Monaten'."
                         ^^^^^^^^^^^^^
                         Zitat ohne [1] Referenz

# SCHLECHT (Keine Zitate)
"Die Bearbeitungsfrist betrÃ¤gt drei Monate."
```

---

## ğŸ”„ Feedback-Schleife - Workflow

### Iteration 1: Baseline

```bash
# Schritt 1: Baseline-Test durchfÃ¼hren
python tests/test_rag_quality_v3_19_0.py

# Output: golden_dataset_baseline.json
# Ergebnis:
#   - 0% Zitationen
#   - 0% Direkte Zitate
#   - 32% Aspekt-Abdeckung
```

### Iteration 2: Prompt-Optimierung

```bash
# Schritt 2: Prompt verbessern
# File: backend/agents/veritas_enhanced_prompts.py

# HINZUFÃœGEN:
# - "Verwende ZWINGEND direkte Zitate aus den Quellen"
# - "Setze Zitate in AnfÃ¼hrungszeichen: '...'"
# - "Nach jedem Zitat: Quellenangabe [1]"
```

### Iteration 3: Re-Evaluation

```bash
# Schritt 3: Erneut testen mit optimiertem Prompt
python tests/test_rag_quality_v3_19_0.py

# Output: golden_dataset_iteration2.json
# Vergleich mit Baseline:
#   - Zitationen: 0% â†’ 80% âœ…
#   - Direkte Zitate: 0% â†’ 65% âœ…
#   - Aspekt-Abdeckung: 32% â†’ 58% âœ…
```

### Iteration 4: Fine-Tuning

```bash
# Schritt 4: Modell-spezifische Optimierungen
# Erkenntnis aus Dataset:
#   - llama3.1:8b: Beste Zitation-Rate (90%)
#   - mistral:latest: Beste Aspekt-Abdeckung (72%)
#   - llama3.1:latest: Balance (80% / 65%)

# Entscheidung: llama3.1:latest als Standard-Modell
```

---

## ğŸ“ˆ Beispiel-Analyse

### Golden Dataset Entry - Q1 (Baugenehmigung)

```json
{
  "question_id": "Q1",
  "question": "Welche rechtlichen Voraussetzungen, Fristen und Kosten...",
  
  "expected_aspects": [
    "Rechtliche Voraussetzungen (Â§Â§ LBO BW)",
    "Fristen (Bearbeitungsdauer)",
    "Kosten (GebÃ¼hren)",
    "Ausnahmen (Genehmigungsfreistellung)",
    "Vereinfachtes Verfahren"
  ],
  
  "expected_citations_min": 3,
  "expected_quotes_min": 2,
  "expected_legal_refs_min": 3,
  
  "model_results": {
    
    "llama3.1:latest": {
      "metrics": {
        "answer_length": 770,
        "citation_count": 0,          // âŒ FEHLT
        "direct_quotes_count": 0,     // âŒ FEHLT
        "legal_references": [],       // âŒ FEHLT
        "aspect_coverage": 0.20       // âŒ NUR 20%
      },
      "timing": {
        "total_time": 18.2,
        "retrieval_time": 1.1,
        "generation_time": 16.3,
        "network_latency": 0.3
      },
      "rating": "GOOD",               // Trotzdem "GOOD" wg. LÃ¤nge
      "issues": [
        "âš ï¸ Zu wenig Zitationen: 0/3",
        "âš ï¸ Keine direkten Zitate: 0/2",
        "âš ï¸ Aspekt-Abdeckung zu niedrig: 1/5 (20%)"
      ]
    },
    
    "mistral:latest": {
      "metrics": {
        "answer_length": 821,
        "citation_count": 0,
        "direct_quotes_count": 0,
        "legal_references": [],
        "aspect_coverage": 0.00       // âŒ 0%!
      },
      "timing": {
        "total_time": 17.6,
        "retrieval_time": 1.0,
        "generation_time": 15.9,
        "network_latency": 0.2
      },
      "rating": "GOOD",
      "issues": [
        "âš ï¸ Zu wenig Zitationen: 0/3",
        "âš ï¸ Keine direkten Zitate: 0/2",
        "âš ï¸ Aspekt-Abdeckung: 0/5 (0%)"  // âŒ KRITISCH
      ]
    }
  }
}
```

### Erkenntnisse aus diesem Entry

1. **ALLE Modelle** haben 0 Zitationen â†’ **Prompt-Problem**, nicht modell-spezifisch
2. **Aspekt-Abdeckung variiert** (20% vs. 0%) â†’ Modell-Unterschiede
3. **Timing Ã¤hnlich** (~17-18s) â†’ LLM-Inferenz dominiert
4. **Rating "GOOD"** trotz fehlender Zitate â†’ **Metriken anpassen**

---

## ğŸ¯ Ziel-Metriken (Golden Standard)

### Nach Prompt-Optimierung

| Metrik | Baseline | Ziel | Status |
|--------|----------|------|--------|
| **IEEE-Zitationen** | 0.0 | â‰¥3.5 | â³ In Arbeit |
| **Direkte Zitate** | 0.0 | â‰¥2.0 | â³ In Arbeit |
| **Zitate mit Quelle** | 0% | â‰¥80% | â³ In Arbeit |
| **Paragraphen-Refs** | ~5% | â‰¥80% | â³ In Arbeit |
| **Aspekt-Abdeckung** | 32% | â‰¥65% | â³ In Arbeit |
| **Follow-up-VorschlÃ¤ge** | 0.0 | â‰¥4.0 | â³ In Arbeit |
| **Rating "EXCELLENT"** | 33% | â‰¥60% | â³ In Arbeit |

### Performance-Ziele

| Metrik | Aktuell | Ziel | Optimierung |
|--------|---------|------|-------------|
| **Total Time** | 18s | <10s | Kleineres Modell |
| **Retrieval Time** | 1.1s | <500ms | Index-Optimierung |
| **Generation Time** | 16s | <8s | llama3.1:8b statt :latest |

---

## ğŸš€ Nutzung

### Test durchfÃ¼hren

```bash
# ALLE Modelle testen (Golden Dataset)
python tests/test_rag_quality_v3_19_0.py

# Output:
# - Konsolen-Output mit Zwischenergebnissen
# - golden_dataset_1760123456.json (vollstÃ¤ndige Daten)
# - rag_test_results_1760123456.json (Flat-Format)
```

### Dataset analysieren

```python
import json

# Golden Dataset laden
with open('golden_dataset_1760123456.json') as f:
    dataset = json.load(f)

# Bestes Modell fÃ¼r Zitationen finden
for question_id, entry in dataset.items():
    print(f"\n{question_id}: {entry['question'][:50]}...")
    
    best_model = None
    best_citations = 0
    
    for model, results in entry['model_results'].items():
        citations = results['metrics']['citation_count']
        if citations > best_citations:
            best_citations = citations
            best_model = model
    
    print(f"  Bestes Modell: {best_model} ({best_citations} Zitationen)")
```

### Prompt-Verbesserung testen

```bash
# 1. Baseline
python tests/test_rag_quality_v3_19_0.py
mv golden_dataset_*.json golden_dataset_baseline.json

# 2. Prompt Ã¤ndern
# backend/agents/veritas_enhanced_prompts.py bearbeiten

# 3. Re-Test
python tests/test_rag_quality_v3_19_0.py
mv golden_dataset_*.json golden_dataset_iteration2.json

# 4. Vergleichen
python tests/compare_golden_datasets.py \
    golden_dataset_baseline.json \
    golden_dataset_iteration2.json
```

---

## ğŸ“š Verwandte Dokumentation

- `RAG_QUALITY_ANALYSIS_AND_PROMPT_IMPROVEMENTS.md` - Detaillierte Analyse v1.0
- `veritas_enhanced_prompts.py` - Verbesserte Prompt-Templates
- `test_rag_quality_v3_19_0.py` - Test-Script (mit Golden Dataset)

---

## ğŸ”® NÃ¤chste Schritte

### Kurzfristig (diese Woche)
1. âœ… Golden Dataset System dokumentieren
2. â³ Verwaltungsrecht-Prompts integrieren (aus `veritas_enhanced_prompts.py`)
3. â³ VollstÃ¤ndigen Test durchfÃ¼hren (ALLE Modelle)
4. â³ Baseline Golden Dataset erstellen

### Mittelfristig (nÃ¤chste Woche)
1. Prompt-Optimierungen implementieren (direkte Zitate erzwingen)
2. Re-Test nach Optimierung
3. Modell-Auswahl finalisieren (basierend auf Daten)
4. Performance-Optimierung (kleineres Modell?)

### Langfristig (nÃ¤chsten Monat)
1. Automatische Feedback-Schleife (CI/CD Integration)
2. Kontinuierliches Golden Dataset Update
3. A/B-Testing verschiedener Prompt-Varianten
4. Fine-Tuning eines eigenen Modells (optional)

---

**Erstellt von:** GitHub Copilot  
**Datum:** 10. Oktober 2025  
**Version:** 2.0
