# üé® VERITAS Structured LLM Response System - Konzeptplan

**Version:** v4.1.0 (Rich Media + Streaming + Dynamic Tokens + Templates)  
**Created:** 12. Oktober 2025, 18:00 Uhr  
**Updated:** 12. Oktober 2025, 18:15 Uhr  
**Status:** üìã KONZEPTPHASE - Ready for Implementation

---

## üìö Dokumentations-√úbersicht

**Dieses Dokument:** Vollst√§ndige technische Spezifikation (1,700+ Zeilen)

**Quick Access:**
- **Executive Summary:** `STRUCTURED_RESPONSE_EXECUTIVE_SUMMARY.md` (600 Zeilen, Business-Perspektive)
- **Quick Reference:** `STRUCTURED_RESPONSE_QUICKREF.md` (300 Zeilen, Cheat Sheet f√ºr Developer)

---

## üìë Inhaltsverzeichnis

1. **[Zielsetzung](#-zielsetzung)** - Die 7 Hauptanforderungen
2. **[Kritische Architektur-Anforderungen](#-kritische-architektur-anforderungen)**
   - 2.1 Streaming Client ‚ö° (BEREITS VORHANDEN)
   - 2.2 Dynamische Token-Size üìè (NEU - Adaptive Token Manager)
   - 2.3 Template-System üìã (NEU - 5 Verwaltungsrechtliche Templates)
3. **[Recherche: Bestehende VERITAS-Features](#-recherche-bestehende-veritas-features)**
   - Streaming-System (639 LOC) ‚úÖ
   - Markdown-Renderer (1,000 LOC) ‚úÖ
   - Chat-Formatter (2,100 LOC) ‚úÖ
   - Syntax-Highlighter (300 LOC) ‚úÖ
4. **[Vorgeschlagene Architektur](#-vorgeschlagene-architektur)**
   - Phase 1: Standardisiertes Response-Format (NDJSON)
   - Phase 2: Widget-Renderer-System
   - Phase 3: LLM-Prompt-Template
   - Phase 4: Integration in Chat-Formatter
5. **[Widget-Typen](#-widget-typen-vollst√§ndige-liste)** - Text, Media, Interactive, Visualisierungen, Custom
6. **[Implementierungs-Roadmap](#-implementierungs-roadmap-aktualisiert)** - 6 Phasen, 11-17 Tage
7. **[Beispiel-Use-Cases](#-beispiel-use-cases)** - Zust√§ndigkeit, Code-Beispiel, Visualisierung
8. **[Technische Details](#-technische-details)** - Dependencies, Memory-Management, Performance
9. **[Moderne Referenz-Systeme](#-moderne-referenz-systeme)** - Discord, Telegram, VS Code, Jupyter
10. **[Empfehlung & N√§chste Schritte](#-empfehlung--implementierungs-strategie)**

---

## üéØ Zielsetzung

**Problem:** LLM-Antworten sind aktuell reiner Text/Markdown. Wir brauchen:
1. **Strukturierte Metadaten** (Quellen, Confidence, Vorschl√§ge, etc.)
2. **Rich Media Support** (Buttons, Canvas, Images, Videos, Interactive Widgets)
3. **Standardisiertes Format** f√ºr LLM-Output (Markdown + JSON Mix)
4. **Tkinter-Native Rendering** aller Elemente
5. **Streaming Support** ‚ö° (Sequenzielles Fortschreiben w√§hrend LLM generiert) - **KRITISCH!**
6. **Dynamische Token-Size** üìè (Flexible Context-L√§nge basierend auf Komplexit√§t) - **KRITISCH!**
7. **Template-System** üìã (Verwaltungsrechtliche Spezial-Prompts serverseitig) - **KRITISCH!**

**Hinweis:** Anforderungen 5-7 wurden vom User explizit als kritisch f√ºr das Konzept spezifiziert.

---

## ‚ö° Kritische Architektur-Anforderungen

### 1. **Streaming Client** (BEREITS VORHANDEN ‚úÖ)

VERITAS hat bereits einen **voll funktionsf√§higen Streaming-Client!**

**Bestehende Architektur:**
```
veritas_streaming_service.py (Backend)
    ‚îú‚Üí VeritasStreamingService (WebSocket + HTTP Streaming)
    ‚îú‚Üí StreamingUIMixin (Frontend Integration)
    ‚îú‚Üí ProgressStage/ProgressType (Progress-Updates)
    ‚îî‚Üí ThreadManager Integration (Thread-Safe)

veritas_ollama_client.py (LLM Backend)
    ‚îî‚Üí generate_response(stream=True) ‚Üí AsyncGenerator
```

**Konsequenz f√ºr Structured Response:**
- ‚úÖ **Sequenzielles Fortschreiben** ist bereits implementiert
- ‚ö†Ô∏è **JSON + Markdown Mix** muss **streaming-f√§hig** sein
- ‚ö†Ô∏è **Widget-Rendering** muss **inkrementell** erfolgen

**Neue Anforderung:**
```python
# STREAMING STRUCTURED RESPONSE FORMAT
# Statt 1x komplettes JSON am Ende ‚Üí Mehrere JSON-Chunks w√§hrend Streaming

# Chunk 1 (Header, sofort nach Start)
{
  "type": "response_start",
  "metadata": {
    "confidence": null,  # Wird sp√§ter aktualisiert
    "template": "zustaendigkeit_behoerde",  # Welches Template wird genutzt
    "estimated_tokens": 2500  # Dynamisch gesch√§tzt
  }
}

# Chunk 2-N (Text-Chunks, w√§hrend LLM generiert)
{
  "type": "text_chunk",
  "content": "Gem√§√ü **BauGB ¬ß35** ist f√ºr Ihr Vorhaben...",
  "position": "main"  # main | metadata | source | suggestion
}

# Chunk N+1 (Widget erscheint, wenn im Text erw√§hnt)
{
  "type": "widget",
  "widget": {
    "type": "table",
    "headers": ["Beh√∂rde", "Zust√§ndigkeit"],
    "rows": [["Bauamt", "Baugenehmigung"], ...]
  }
}

# Letzter Chunk (Finalisierung)
{
  "type": "response_end",
  "metadata": {
    "confidence": 0.92,  # Finale Confidence
    "sources": [...],
    "suggestions": [...]
  }
}
```

**Implementation:**
```python
class StreamingStructuredResponseParser:
    """
    Parst Streaming-Response inkrementell
    Unterst√ºtzt Mix aus Text-Chunks und JSON-Widgets
    """
    
    def __init__(self, chat_formatter, widget_renderer):
        self.chat_formatter = chat_formatter
        self.widget_renderer = widget_renderer
        self.buffer = ""
        self.current_metadata = {}
        self.widgets_queue = []
    
    async def process_chunk(self, chunk: str) -> None:
        """
        Verarbeitet einzelnen Streaming-Chunk
        Kann Text ODER JSON sein
        """
        # Pr√ºfe ob JSON-Chunk (startet mit '{')
        if chunk.strip().startswith('{'):
            try:
                data = json.loads(chunk)
                await self._handle_json_chunk(data)
            except json.JSONDecodeError:
                # Falls JSON unvollst√§ndig ‚Üí in Buffer
                self.buffer += chunk
        else:
            # Text-Chunk ‚Üí sofort rendern
            await self._handle_text_chunk(chunk)
    
    async def _handle_json_chunk(self, data: dict):
        chunk_type = data.get('type')
        
        if chunk_type == 'response_start':
            # Initialisiere Response (Template, Metadata)
            self.current_metadata = data.get('metadata', {})
            # UI: Zeige Template-Badge
            
        elif chunk_type == 'text_chunk':
            # Text inkrementell rendern
            content = data.get('content', '')
            position = data.get('position', 'main')
            
            if position == 'main':
                # Markdown rendern (incrementell!)
                self.chat_formatter.markdown_renderer.render_markdown(
                    content, append=True  # WICHTIG: append statt replace
                )
            
        elif chunk_type == 'widget':
            # Widget sofort rendern
            widget_spec = data.get('widget', {})
            self.widget_renderer.render_widget(widget_spec)
            
        elif chunk_type == 'response_end':
            # Finalisiere (Quellen, Vorschl√§ge, Confidence)
            final_metadata = data.get('metadata', {})
            self.chat_formatter._insert_sources(final_metadata.get('sources', []))
            self.chat_formatter._insert_suggestions(final_metadata.get('suggestions', []))
            self.chat_formatter._insert_confidence_badge(final_metadata.get('confidence'))
    
    async def _handle_text_chunk(self, text: str):
        """Plain-Text-Chunk (kein JSON)"""
        # Sofort rendern (Streaming-Feeling!)
        self.chat_formatter.markdown_renderer.render_markdown(
            text, append=True
        )
```

---

### 2. **Dynamische Token-Size** üìè (NEU)

**Problem:** 
Feste Token-Limits (z.B. 4096) sind bei komplexen verwaltungsrechtlichen Themen kontraproduktiv:
- Einfache Frage ("Wer ist zust√§ndig?") ‚Üí 500 Tokens reichen
- Komplexe Frage ("Vollst√§ndige Pr√ºfung Bauantrag inkl. formell, rechtlich, sachlich") ‚Üí 8000+ Tokens n√∂tig

**L√∂sung: Adaptive Context Window**

```python
class AdaptiveTokenManager:
    """
    Dynamische Token-Size basierend auf:
    1. Frage-Komplexit√§t (NLP-Analyse)
    2. Template-Requirements (jedes Template hat Min/Max)
    3. RAG-Kontext-Gr√∂√üe (wie viele Quellen gefunden)
    4. User-Historie (Follow-up vs. Neue Frage)
    """
    
    TOKEN_LIMITS = {
        'min': 1024,      # Minimales Context-Window
        'default': 4096,  # Standard f√ºr einfache Fragen
        'extended': 8192, # Erweitert f√ºr komplexe Themen
        'max': 16384      # Maximum (nur f√ºr sehr komplexe Analysen)
    }
    
    TEMPLATE_TOKEN_REQUIREMENTS = {
        'zustaendigkeit_behoerde': 2048,  # Einfache Zust√§ndigkeits-Frage
        'formale_pruefung': 4096,         # Formale Pr√ºfung (Antragsbestandteile)
        'rechtliche_pruefung': 6144,      # Rechtliche Pr√ºfung (BauGB, BauNVO, LBO)
        'sachliche_pruefung': 8192,       # Sachliche Pr√ºfung (technische Details)
        'vollstaendige_pruefung': 16384,  # Alle 3 Pr√ºfungen kombiniert
    }
    
    def estimate_required_tokens(self, 
                                 user_query: str,
                                 template: str,
                                 rag_context_size: int,
                                 is_followup: bool = False) -> int:
        """
        Sch√§tzt ben√∂tigte Token-Anzahl dynamisch
        
        Args:
            user_query: User-Frage
            template: Gew√§hltes Prompt-Template
            rag_context_size: Gr√∂√üe des RAG-Kontexts (Anzahl Tokens)
            is_followup: Ist es eine Follow-up-Frage?
        
        Returns:
            int: Empfohlene Token-Anzahl
        """
        # 1. Template-Basis
        base_tokens = self.TEMPLATE_TOKEN_REQUIREMENTS.get(template, self.TOKEN_LIMITS['default'])
        
        # 2. Frage-Komplexit√§t analysieren
        query_complexity = self._analyze_query_complexity(user_query)
        complexity_factor = {
            'simple': 0.5,    # "Wer ist zust√§ndig?" ‚Üí 50% von base
            'medium': 1.0,    # Standard-Frage ‚Üí 100% von base
            'complex': 1.5,   # Multi-Teil-Frage ‚Üí 150% von base
            'very_complex': 2.0  # Vollst√§ndige Analyse ‚Üí 200% von base
        }.get(query_complexity, 1.0)
        
        # 3. RAG-Kontext einberechnen
        rag_overhead = min(rag_context_size, 2048)  # Max 2048 Tokens f√ºr RAG
        
        # 4. Follow-up Reduktion (Context bereits geladen)
        followup_reduction = 0.7 if is_followup else 1.0
        
        # Finale Berechnung
        estimated_tokens = int(
            (base_tokens * complexity_factor + rag_overhead) * followup_reduction
        )
        
        # Clamp auf min/max
        return max(
            self.TOKEN_LIMITS['min'],
            min(estimated_tokens, self.TOKEN_LIMITS['max'])
        )
    
    def _analyze_query_complexity(self, query: str) -> str:
        """
        Analysiert Frage-Komplexit√§t via NLP
        
        Indikatoren:
        - Wort-Anzahl
        - Anzahl Rechtsbegriffe
        - Anzahl Fragen (?, und, sowie)
        - Spezielle Keywords ("vollst√§ndig", "detailliert", "alle", "s√§mtliche")
        """
        words = query.split()
        word_count = len(words)
        
        # Rechtsbegriffe (vereinfacht)
        legal_terms = ['BauGB', 'BauNVO', 'LBO', 'Genehmigung', 'Zust√§ndigkeit', 
                       'Pr√ºfung', 'Verfahren', 'Antrag', 'Bebauungsplan']
        legal_term_count = sum(1 for term in legal_terms if term in query)
        
        # Komplexit√§ts-Keywords
        complex_keywords = ['vollst√§ndig', 'detailliert', 'alle', 's√§mtliche', 
                            'komplett', 'umfassend', 'inklusiv']
        has_complex_keywords = any(kw in query.lower() for kw in complex_keywords)
        
        # Multi-Teil-Fragen
        question_parts = query.count('?') + query.count(' und ') + query.count(' sowie ')
        
        # Klassifizierung
        if word_count < 10 and legal_term_count <= 1:
            return 'simple'
        elif word_count > 50 or legal_term_count > 5 or has_complex_keywords:
            return 'very_complex'
        elif question_parts > 2 or legal_term_count > 3:
            return 'complex'
        else:
            return 'medium'
```

**Backend-Integration:**
```python
# In Ollama-Client
async def generate_response(self, request: OllamaRequest, template: str = None):
    # Dynamische Token-Sch√§tzung
    token_manager = AdaptiveTokenManager()
    estimated_tokens = token_manager.estimate_required_tokens(
        user_query=request.prompt,
        template=template or 'default',
        rag_context_size=len(request.context or []),
        is_followup=request.context is not None
    )
    
    # Ollama Request mit dynamischer Token-Anzahl
    payload = {
        "model": request.model,
        "prompt": request.prompt,
        "stream": True,  # Streaming aktiviert
        "options": {
            "num_predict": estimated_tokens,  # Dynamisch!
            "temperature": request.temperature,
        }
    }
    
    logger.info(f"üéØ Estimated Tokens: {estimated_tokens} (Template: {template})")
```

---

### 3. **Template-System** üìã (NEU - SERVERSEITIG)

**Problem:**
Verwaltungsrechtliche Aspekte erfordern **spezialisierte Prompts**:
- **Zust√§ndigkeit der Beh√∂rde** ‚Üí Andere Prompt-Struktur als
- **Formale Pr√ºfung** ‚Üí Andere Struktur als
- **Rechtliche Pr√ºfung** ‚Üí Andere Struktur als
- **Sachliche Pr√ºfung**

Aktuell: 1 generischer System-Prompt f√ºr alles ‚Üí Suboptimal!

**L√∂sung: Serverseitiges Template-System**

```python
# backend/templates/prompt_templates.py

from typing import Dict, Any, List
from dataclasses import dataclass

@dataclass
class PromptTemplate:
    """Definition eines Prompt-Templates"""
    id: str
    name: str
    description: str
    system_prompt: str
    required_context: List[str]
    expected_tokens: int
    response_structure: Dict[str, Any]
    
class PromptTemplateLibrary:
    """
    Zentrale Template-Bibliothek f√ºr verwaltungsrechtliche Aspekte
    SERVERSEITIG - Templates werden NUR im Backend verwaltet
    """
    
    TEMPLATES = {
        'zustaendigkeit_behoerde': PromptTemplate(
            id='zustaendigkeit_behoerde',
            name='Zust√§ndigkeit der Beh√∂rde',
            description='Ermittelt zust√§ndige Beh√∂rde(n) f√ºr ein Vorhaben',
            system_prompt="""
Du bist VERITAS, ein Experte f√ºr deutsches Baurecht.

AUFGABE: Ermittle die zust√§ndige(n) Beh√∂rde(n) f√ºr das beschriebene Vorhaben.

VORGEHEN:
1. Identifiziere Vorhaben-Typ (Hochbau, Tiefbau, Au√üenbereich, etc.)
2. Pr√ºfe √∂rtliche Zust√§ndigkeit (Gemeinde, Landkreis, Bezirk)
3. Pr√ºfe sachliche Zust√§ndigkeit (Bauordnung, Naturschutz, Denkmalschutz, etc.)
4. Liste ALLE beteiligten Beh√∂rden auf

ANTWORT-FORMAT (Structured Response):
{
  "type": "text_chunk",
  "content": "F√ºr Ihr Vorhaben sind folgende Beh√∂rden zust√§ndig:\\n\\n**Hauptzust√§ndigkeit:**\\n- [Beh√∂rde] ([Rechtsgrundlage])\\n\\n**Beteiligte Beh√∂rden:**\\n- [Beh√∂rde 1] ([Grund])\\n- [Beh√∂rde 2] ([Grund])"
}

{
  "type": "widget",
  "widget": {
    "type": "table",
    "headers": ["Beh√∂rde", "Zust√§ndigkeit", "Rechtsgrundlage"],
    "rows": [
      ["Bauordnungsamt", "Baugenehmigung", "BauGB ¬ß29"],
      ["Untere Naturschutzbeh√∂rde", "Naturschutzpr√ºfung", "BNatSchG ¬ß17"]
    ]
  }
}

WICHTIG:
- Nenne konkrete Rechtsgrundlagen (BauGB ¬ßX, LBO ¬ßY)
- Unterscheide Hauptzust√§ndigkeit vs. beteiligte Beh√∂rden
- Erw√§hne Verfahrensart (vereinfacht, normal, erweitert)

QUELLEN: Verwende RAG-Kontext f√ºr lokale Besonderheiten.
""",
            required_context=['bauvorhaben_typ', 'standort', 'nutzung'],
            expected_tokens=2048,
            response_structure={
                'main_text': 'markdown',
                'widgets': ['table'],
                'metadata': ['sources', 'confidence']
            }
        ),
        
        'formale_pruefung': PromptTemplate(
            id='formale_pruefung',
            name='Formale Pr√ºfung Bauantrag',
            description='Pr√ºft Vollst√§ndigkeit und Form des Bauantrags',
            system_prompt="""
Du bist VERITAS, Experte f√ºr formale Anforderungen im Baugenehmigungsverfahren.

AUFGABE: Formale Pr√ºfung des Bauantrags (Vollst√§ndigkeit, Form, Fristen)

PR√úF-KATALOG:
1. **Antragsbestandteile** (BauPr√ºfVO):
   - Bauvorlagen vollst√§ndig? (Bauzeichnungen, Baubeschreibung, etc.)
   - Unterschriften vorhanden?
   - Bauvorlageberechtigung nachgewiesen?
   
2. **Formale Anforderungen**:
   - Richtiger Antragsweg?
   - Zust√§ndigkeit gegeben?
   - Fristen eingehalten?
   
3. **Geb√ºhren**:
   - Geb√ºhrentatbestand?
   - Betrag?

ANTWORT-FORMAT:
{
  "type": "text_chunk",
  "content": "## Formale Pr√ºfung\\n\\n### ‚úÖ Vollst√§ndig:\\n- [Punkt]\\n\\n### ‚ö†Ô∏è Fehlend/Unvollst√§ndig:\\n- [Punkt]"
}

{
  "type": "widget",
  "widget": {
    "type": "table",
    "headers": ["Bestandteil", "Status", "Rechtsgrundlage", "Handlungsbedarf"],
    "rows": [
      ["Bauzeichnungen", "‚úÖ Vollst√§ndig", "BauPr√ºfVO ¬ß3", "Keine"],
      ["Standsicherheitsnachweis", "‚ùå Fehlt", "BauPr√ºfVO ¬ß5", "Nachreichen binnen 4 Wochen"]
    ]
  }
}

WICHTIG:
- Klare Unterscheidung: Vollst√§ndig ‚úÖ vs. Fehlend ‚ùå
- Rechtsgrundlagen f√ºr JEDEN Punkt
- Konkrete Handlungsempfehlungen
- Fristen nennen
""",
            required_context=['bauantrag_dokumente', 'antragsteller', 'bauvorhaben'],
            expected_tokens=4096,
            response_structure={
                'main_text': 'markdown_checklist',
                'widgets': ['table', 'checklist'],
                'metadata': ['sources', 'suggestions']
            }
        ),
        
        'rechtliche_pruefung': PromptTemplate(
            id='rechtliche_pruefung',
            name='Rechtliche Pr√ºfung',
            description='Pr√ºft √úbereinstimmung mit √∂ffentlich-rechtlichen Vorschriften',
            system_prompt="""
Du bist VERITAS, Experte f√ºr materielles Baurecht.

AUFGABE: Rechtliche Pr√ºfung des Bauvorhabens (Vereinbarkeit mit √∂ffentlich-rechtlichen Vorschriften)

PR√úF-EBENEN:
1. **Bauplanungsrecht** (BauGB, BauNVO):
   - Zul√§ssigkeit im Plangebiet?
   - Art der baulichen Nutzung?
   - Ma√ü der baulichen Nutzung?
   - Bauweise, √ºberbaubare Grundst√ºcksfl√§che?
   
2. **Bauordnungsrecht** (LBO):
   - Abstandsfl√§chen?
   - Brandschutz?
   - Barrierefreiheit?
   
3. **Fachrecht**:
   - Naturschutz?
   - Denkmalschutz?
   - Immissionsschutz?

ANTWORT-FORMAT:
{
  "type": "text_chunk",
  "content": "## Rechtliche Pr√ºfung\\n\\n### Bauplanungsrecht\\n\\n#### Zul√§ssigkeit gem. BauGB ¬ß30\\n[Pr√ºfung]..."
}

{
  "type": "widget",
  "widget": {
    "type": "canvas",
    "draw_commands": [
      {"cmd": "rect", "x": 50, "y": 50, "width": 100, "height": 80, "fill": "lightgray", "outline": "black"},
      {"cmd": "text", "x": 100, "y": 90, "text": "Geb√§ude", "font": ["Arial", 10]},
      {"cmd": "line", "x1": 150, "y1": 90, "x2": 200, "y2": 90, "color": "red", "width": 2},
      {"cmd": "text", "x": 210, "y": 85, "text": "3m Abstand", "font": ["Arial", 9], "color": "red"}
    ],
    "width": 400,
    "height": 300,
    "caption": "Abstandsfl√§chen-Visualisierung"
  }
}

WICHTIG:
- Systematische Pr√ºfung (Bauplanungsrecht ‚Üí Bauordnungsrecht ‚Üí Fachrecht)
- Jede Norm mit Fundstelle (BauGB ¬ß35 Abs. 2)
- Bei Konflikten: Ausnahme/Befreiung m√∂glich?
- Visualisierungen f√ºr geometrische Anforderungen (Abstandsfl√§chen, etc.)
""",
            required_context=['bauvorhaben', 'grundstueck', 'bebauungsplan', 'nutzung'],
            expected_tokens=6144,
            response_structure={
                'main_text': 'markdown_hierarchical',
                'widgets': ['canvas', 'table'],
                'metadata': ['sources', 'confidence', 'suggestions']
            }
        ),
        
        'sachliche_pruefung': PromptTemplate(
            id='sachliche_pruefung',
            name='Sachliche/Technische Pr√ºfung',
            description='Pr√ºft technische und sachliche Anforderungen',
            system_prompt="""
Du bist VERITAS, Experte f√ºr technische Bauvorschriften.

AUFGABE: Sachliche/Technische Pr√ºfung des Bauvorhabens

PR√úF-ASPEKTE:
1. **Standsicherheit**:
   - Statik-Nachweis plausibel?
   - Gr√ºndung angemessen?
   
2. **Brandschutz**:
   - Geb√§udeklasse?
   - Rettungswege?
   - Feuerwiderstandsklassen?
   
3. **Schall-/W√§rmeschutz**:
   - EnEV/GEG Anforderungen?
   - Schallschutz nach DIN 4109?
   
4. **Erschlie√üung**:
   - Zufahrt vorhanden?
   - Ver-/Entsorgung gesichert?

ANTWORT-FORMAT:
{
  "type": "text_chunk",
  "content": "## Sachliche Pr√ºfung\\n\\n### Standsicherheit\\n[Pr√ºfung]..."
}

{
  "type": "widget",
  "widget": {
    "type": "chart",
    "chart_type": "bar",
    "title": "Energieeffizienz-Anforderungen",
    "data": {
      "labels": ["IST-Wert", "SOLL (GEG)", "Grenzwert"],
      "values": [45, 55, 70]
    },
    "unit": "kWh/m¬≤a"
  }
}

WICHTIG:
- Technische Normen referenzieren (DIN, VDE, EnEV/GEG)
- Bei Abweichungen: Kompensationsma√ünahmen?
- Visualisierung bei komplexen technischen Daten (Diagramme, Charts)
""",
            required_context=['bauvorhaben', 'technische_nachweise', 'gebaeudetyp'],
            expected_tokens=8192,
            response_structure={
                'main_text': 'markdown_technical',
                'widgets': ['chart', 'table', 'canvas'],
                'metadata': ['sources', 'confidence', 'suggestions']
            }
        ),
        
        'vollstaendige_pruefung': PromptTemplate(
            id='vollstaendige_pruefung',
            name='Vollst√§ndige Pr√ºfung (Formell + Rechtlich + Sachlich)',
            description='Kombiniert alle 3 Pr√ºfungsebenen',
            system_prompt="""
Du bist VERITAS, umfassender Experte f√ºr Baugenehmigungsverfahren.

AUFGABE: Vollst√§ndige Pr√ºfung des Bauantrags (Formell, Rechtlich, Sachlich)

STRUKTUR:
1. **Executive Summary** (Ampel: ‚úÖ Genehmigungsf√§hig / ‚ö†Ô∏è Mit Auflagen / ‚ùå Nicht genehmigungsf√§hig)
2. **Formale Pr√ºfung** (siehe Template 'formale_pruefung')
3. **Rechtliche Pr√ºfung** (siehe Template 'rechtliche_pruefung')
4. **Sachliche Pr√ºfung** (siehe Template 'sachliche_pruefung')
5. **Gesamtfazit + Handlungsempfehlungen**

ANTWORT-FORMAT:
{
  "type": "text_chunk",
  "content": "# Vollst√§ndige Bauantrags-Pr√ºfung\\n\\n## ‚úÖ Executive Summary\\n\\nDer Bauantrag ist **genehmigungsf√§hig mit Auflagen**..."
}

{
  "type": "widget",
  "widget": {
    "type": "table",
    "headers": ["Pr√ºfungsebene", "Status", "Kritische Punkte", "Handlungsbedarf"],
    "rows": [
      ["Formale Pr√ºfung", "‚ö†Ô∏è Mit M√§ngeln", "Standsicherheitsnachweis fehlt", "Nachreichen binnen 4 Wo"],
      ["Rechtliche Pr√ºfung", "‚úÖ OK", "-", "-"],
      ["Sachliche Pr√ºfung", "‚ö†Ô∏è Mit Auflagen", "Brandschutz Auflage", "Sprinkleranlage erforderlich"]
    ]
  }
}

WICHTIG:
- Executive Summary mit klarer Empfehlung (Ampel-System)
- Alle 3 Pr√ºfungsebenen systematisch durchgehen
- Priorisierung: Was MUSS sofort behoben werden?
- Konkrete Handlungsschritte mit Fristen
""",
            required_context=['bauantrag_komplett', 'grundstueck', 'bebauungsplan', 'technische_nachweise'],
            expected_tokens=16384,  # SEHR GROSS!
            response_structure={
                'main_text': 'markdown_comprehensive',
                'widgets': ['table', 'chart', 'canvas', 'checklist'],
                'metadata': ['sources', 'confidence', 'suggestions', 'next_steps']
            }
        ),
    }
    
    @classmethod
    def get_template(cls, template_id: str) -> PromptTemplate:
        """Holt Template nach ID"""
        return cls.TEMPLATES.get(template_id)
    
    @classmethod
    def select_template_auto(cls, user_query: str, rag_context: Dict) -> str:
        """
        W√§hlt automatisch passendes Template basierend auf User-Frage
        
        Keyword-Matching:
        - "zust√§ndig" ‚Üí zustaendigkeit_behoerde
        - "vollst√§ndig" oder "Unterlagen" ‚Üí formale_pruefung
        - "rechtlich" oder "zul√§ssig" ‚Üí rechtliche_pruefung
        - "technisch" oder "Statik" oder "Brandschutz" ‚Üí sachliche_pruefung
        - "komplett" oder "umfassend" ‚Üí vollstaendige_pruefung
        """
        query_lower = user_query.lower()
        
        # Keyword-Matching (vereinfacht, sp√§ter via NLP/Classifier)
        if 'zust√§ndig' in query_lower or 'beh√∂rde' in query_lower:
            return 'zustaendigkeit_behoerde'
        elif 'vollst√§ndig' in query_lower or 'unterlagen' in query_lower or 'formell' in query_lower:
            return 'formale_pruefung'
        elif 'rechtlich' in query_lower or 'zul√§ssig' in query_lower or 'baurecht' in query_lower:
            return 'rechtliche_pruefung'
        elif 'technisch' in query_lower or 'statik' in query_lower or 'brandschutz' in query_lower:
            return 'sachliche_pruefung'
        elif any(kw in query_lower for kw in ['komplett', 'umfassend', 'alle pr√ºfungen', 'vollst√§ndige pr√ºfung']):
            return 'vollstaendige_pruefung'
        else:
            # Default: Kein spezielles Template
            return 'default'
```

**Backend-Integration:**
```python
# In Backend API (z.B. FastAPI Endpoint)
from backend.templates.prompt_templates import PromptTemplateLibrary, AdaptiveTokenManager

@app.post("/api/v1/chat/structured")
async def chat_structured(
    user_query: str,
    template_id: Optional[str] = None,  # Optional: User kann Template explizit w√§hlen
    rag_context: Optional[Dict] = None
):
    """
    Structured Response Endpoint mit Template-System
    """
    # 1. Template ausw√§hlen (auto oder explizit)
    if template_id is None:
        template_id = PromptTemplateLibrary.select_template_auto(user_query, rag_context or {})
    
    template = PromptTemplateLibrary.get_template(template_id)
    
    if template is None:
        # Fallback auf Standard-Prompt
        system_prompt = "Du bist VERITAS, ein KI-Assistent f√ºr deutsches Baurecht."
        estimated_tokens = 4096
    else:
        system_prompt = template.system_prompt
        estimated_tokens = template.expected_tokens
    
    # 2. Dynamische Token-Anpassung
    token_manager = AdaptiveTokenManager()
    final_tokens = token_manager.estimate_required_tokens(
        user_query=user_query,
        template=template_id,
        rag_context_size=len(str(rag_context or {})),
        is_followup=False  # TODO: Session-History checken
    )
    
    logger.info(f"üìã Template: {template_id} | Tokens: {final_tokens}")
    
    # 3. Ollama Request mit Streaming
    ollama_request = OllamaRequest(
        model="llama3.2:latest",
        prompt=user_query,
        system=system_prompt,
        temperature=0.3,  # Niedrig f√ºr rechtliche Pr√§zision
        max_tokens=final_tokens  # DYNAMISCH!
    )
    
    # 4. Streaming Response
    async def stream_structured_response():
        # Header-Chunk (sofort)
        yield json.dumps({
            "type": "response_start",
            "metadata": {
                "template": template_id,
                "estimated_tokens": final_tokens,
                "confidence": None  # Wird sp√§ter aktualisiert
            }
        }) + "\n"
        
        # LLM Streaming
        async for chunk in ollama_client.generate_response(ollama_request, stream=True):
            # Text-Chunk
            yield json.dumps({
                "type": "text_chunk",
                "content": chunk.response,
                "position": "main"
            }) + "\n"
        
        # Finale Metadaten (am Ende)
        yield json.dumps({
            "type": "response_end",
            "metadata": {
                "confidence": 0.92,  # TODO: Aus Ollama Response
                "sources": [...],     # TODO: Aus RAG
                "suggestions": [...]  # TODO: Generieren
            }
        }) + "\n"
    
    return StreamingResponse(
        stream_structured_response(),
        media_type="application/x-ndjson"  # Newline-Delimited JSON
    )
```

---

## üîç Recherche: Bestehende VERITAS-Features

### ‚úÖ Bereits vorhanden (SEHR GUT!)

VERITAS hat **bereits** ein fortgeschrittenes System:

#### 0. **Streaming-System** (`veritas_streaming_service.py`) ‚ö° BEREITS VORHANDEN!
```python
class VeritasStreamingService:
    # Unterst√ºtzt:
    - WebSocket + HTTP Streaming
    - ProgressStage/ProgressType (RAG, Retrieval, Response, etc.)
    - Thread-Safe Integration (ThreadManager)
    - StreamingUIMixin (Frontend-Binding)
    - Cancel-Funktionalit√§t
    
class StreamingUIMixin:
    # Frontend-Integration:
    - init_streaming_ui() - UI Setup
    - setup_streaming_integration() - Backend-Binding
    - _handle_streaming_message() - Message Processing
    
# Ollama-Client Streaming
async def generate_response(stream=True) -> AsyncGenerator:
    # Liefert inkrementelle Chunks
    async for chunk in response.aiter_lines():
        yield OllamaResponse(...)
```

**‚ö†Ô∏è WICHTIG F√úR STRUCTURED RESPONSE:**
- Streaming ist BEREITS implementiert ‚úÖ
- Structured Response MUSS streaming-f√§hig sein
- JSON + Markdown Mix MUSS inkrementell renderbar sein
- Widgets M√úSSEN w√§hrend Streaming erscheinen k√∂nnen

---


#### 1. **Markdown-Renderer** (`veritas_ui_markdown.py`)
```python
class MarkdownRenderer:
    # Unterst√ºtzt:
    - Headings (#, ##, ###)
    - Bold (**text**), Italic (*text*)
    - Code-Bl√∂cke (```python ... ```)
    - Syntax-Highlighting (Pygments)
  - Links [Text](URL) <!-- TODO: replace 'URL' with actual target -->
    - Listen (-, *, 1.)
    - Tabellen (Markdown Tables)
    - Blockquotes (> Text)
    - Copy-Button f√ºr Code
```

#### 2. **Chat-Display-Formatter** (`veritas_ui_chat_formatter.py`)
```python
class ChatDisplayFormatter:
    # Unterst√ºtzt:
    - RAG-Metadaten (Confidence, Sources, Agents)
    - Collapsible Sections (Details ausklappbar)
    - Quellen-Liste mit Icons
    - Klickbare Vorschl√§ge
    - Feedback-Widgets (Thumbs Up/Down)
    - Relative Timestamps
    - Attachments-Liste
```

#### 3. **Syntax-Highlighter** (`veritas_ui_syntax.py`)
```python
class SyntaxHighlighter:
    # Unterst√ºtzt:
    - Pygments-Integration
    - 15+ Programmiersprachen
    - Custom Tag-Configuration
```

#### 4. **Icon-System** (`veritas_ui_icons.py`)
```python
class VeritasIcons:
    # Unterst√ºtzt:
    - Unicode Icons
    - Custom SVG Icons
    - Fallback-System
```

---

## üÜï Was fehlt noch?

### ‚ùå Noch NICHT vorhanden:

1. **Standardisiertes LLM-Response-Format** (JSON Schema)
2. **Canvas-Widgets** (f√ºr Diagramme, Zeichnungen)
3. **Image/Video-Embedding** (direkt im Chat)
4. **Interactive Buttons** (au√üer Feedback-Buttons)
5. **Custom Tkinter Widgets** (z.B. Slider, Dropdown, etc.)
6. **LLM-Prompt-Template** f√ºr strukturierte Ausgabe

---

## üìê Vorgeschlagene Architektur

### Phase 1: Standardisiertes Response-Format

**LLM Output Schema (JSON + Markdown Mix):**

```json
{
  "type": "structured_response",
  "version": "1.0",
  "content": {
    "text": "Dies ist die Hauptantwort in **Markdown**...",
    "metadata": {
      "confidence": 0.95,
      "sources": [
        {"url": "https://...", "title": "Quelle 1"},
        {"file": "doc.pdf", "page": 5}
      ],
      "timestamp": "2025-10-12T18:00:00Z",
      "agent": "VERITAS Legal RAG"
    },
    "suggestions": [
      "Weitere Frage 1?",
      "Weitere Frage 2?"
    ],
    "widgets": [
      {
        "type": "code_block",
        "language": "python",
        "code": "def hello():\n    print('Hello')",
        "caption": "Beispiel-Code"
      },
      {
        "type": "table",
        "headers": ["Spalte 1", "Spalte 2"],
        "rows": [["Wert 1", "Wert 2"]]
      },
      {
        "type": "image",
        "url": "https://example.com/image.png",
        "caption": "Beispiel-Bild",
        "width": 400
      },
      {
        "type": "button",
        "label": "Klick mich",
        "action": "show_details",
        "payload": {"detail_id": "123"}
      },
      {
        "type": "canvas",
        "draw_commands": [
          {"cmd": "line", "x1": 0, "y1": 0, "x2": 100, "y2": 100},
          {"cmd": "rect", "x": 50, "y": 50, "width": 100, "height": 50}
        ],
        "width": 400,
        "height": 300
      }
    ]
  }
}
```

---

### Phase 2: Widget-Renderer-System

**Neue Komponente:** `veritas_ui_widget_renderer.py`

```python
class WidgetRenderer:
    """
    Rendert strukturierte Widgets im Chat
    Erweitert MarkdownRenderer um Rich-Media-Support
    """
    
    def __init__(self, text_widget: tk.Text, parent_window: tk.Tk):
        self.text_widget = text_widget
        self.parent_window = parent_window
        self.embedded_widgets = []  # Referenzen f√ºr Cleanup
        
    def render_widget(self, widget_spec: dict) -> None:
        """Dispatcher f√ºr verschiedene Widget-Typen"""
        widget_type = widget_spec.get('type')
        
        if widget_type == 'code_block':
            self._render_code_block(widget_spec)
        elif widget_type == 'table':
            self._render_table(widget_spec)
        elif widget_type == 'image':
            self._render_image(widget_spec)
        elif widget_type == 'video':
            self._render_video(widget_spec)
        elif widget_type == 'button':
            self._render_button(widget_spec)
        elif widget_type == 'canvas':
            self._render_canvas(widget_spec)
        elif widget_type == 'chart':
            self._render_chart(widget_spec)
        # ... weitere Widget-Typen
    
    def _render_image(self, spec: dict) -> None:
        """Bindet Bild direkt im Chat ein"""
        from PIL import Image, ImageTk
        
        # Bild laden
        if 'url' in spec:
            # Von URL laden
            image = self._load_image_from_url(spec['url'])
        elif 'path' in spec:
            # Von lokalem Pfad laden
            image = Image.open(spec['path'])
        elif 'base64' in spec:
            # Von Base64 laden
            image = self._load_image_from_base64(spec['base64'])
        
        # Resize wenn n√∂tig
        if 'width' in spec:
            image = self._resize_image(image, spec['width'])
        
        # In Tkinter-Format konvertieren
        photo = ImageTk.PhotoImage(image)
        
        # Im Text-Widget einbetten
        self.text_widget.image_create(tk.END, image=photo)
        
        # Referenz speichern (wichtig!)
        self.embedded_widgets.append(photo)
        
        # Caption hinzuf√ºgen
        if 'caption' in spec:
            self.text_widget.insert(tk.END, f"\n{spec['caption']}\n", "image_caption")
    
    def _render_button(self, spec: dict) -> None:
        """Bindet klickbaren Button im Chat ein"""
        # Button-Frame erstellen
        button = tk.Button(
            self.text_widget,
            text=spec['label'],
            command=lambda: self._handle_button_click(spec),
            bg='#3498db',
            fg='white',
            relief=tk.FLAT,
            padx=10,
            pady=5
        )
        
        # Button im Text-Widget einbetten
        self.text_widget.window_create(tk.END, window=button)
        self.text_widget.insert(tk.END, "\n")
        
        # Referenz speichern
        self.embedded_widgets.append(button)
    
    def _render_canvas(self, spec: dict) -> None:
        """Rendert Canvas mit Zeichnungen/Diagrammen"""
        # Canvas erstellen
        canvas = tk.Canvas(
            self.text_widget,
            width=spec.get('width', 400),
            height=spec.get('height', 300),
            bg='white',
            relief=tk.SOLID,
            borderwidth=1
        )
        
        # Zeichnungen ausf√ºhren
        for cmd in spec.get('draw_commands', []):
            if cmd['cmd'] == 'line':
                canvas.create_line(
                    cmd['x1'], cmd['y1'], cmd['x2'], cmd['y2'],
                    fill=cmd.get('color', 'black'),
                    width=cmd.get('width', 2)
                )
            elif cmd['cmd'] == 'rect':
                canvas.create_rectangle(
                    cmd['x'], cmd['y'],
                    cmd['x'] + cmd['width'], cmd['y'] + cmd['height'],
                    fill=cmd.get('fill', ''),
                    outline=cmd.get('outline', 'black')
                )
            elif cmd['cmd'] == 'text':
                canvas.create_text(
                    cmd['x'], cmd['y'],
                    text=cmd['text'],
                    font=cmd.get('font', ('Arial', 12)),
                    fill=cmd.get('color', 'black')
                )
            # ... weitere Zeichenbefehle
        
        # Canvas einbetten
        self.text_widget.window_create(tk.END, window=canvas)
        self.text_widget.insert(tk.END, "\n")
        
        # Referenz speichern
        self.embedded_widgets.append(canvas)
    
    def _render_chart(self, spec: dict) -> None:
        """Rendert Diagramm (z.B. mit matplotlib)"""
        import matplotlib.pyplot as plt
        from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
        
        # Matplotlib-Figure erstellen
        fig, ax = plt.subplots(figsize=(6, 4))
        
        chart_type = spec.get('chart_type', 'bar')
        data = spec.get('data', {})
        
        if chart_type == 'bar':
            ax.bar(data['labels'], data['values'])
        elif chart_type == 'line':
            ax.plot(data['x'], data['y'])
        elif chart_type == 'pie':
            ax.pie(data['values'], labels=data['labels'])
        
        ax.set_title(spec.get('title', ''))
        
        # In Tkinter einbetten
        canvas = FigureCanvasTkAgg(fig, self.text_widget)
        canvas.draw()
        canvas_widget = canvas.get_tk_widget()
        
        self.text_widget.window_create(tk.END, window=canvas_widget)
        self.text_widget.insert(tk.END, "\n")
        
        # Referenz speichern
        self.embedded_widgets.append(canvas_widget)
        plt.close(fig)
    
    def cleanup(self) -> None:
        """Cleanup embedded widgets (wichtig f√ºr Memory-Management!)"""
        for widget in self.embedded_widgets:
            try:
                if isinstance(widget, tk.Widget):
                    widget.destroy()
            except:
                pass
        self.embedded_widgets.clear()
```

---

### Phase 3: LLM-Prompt-Template

**System-Prompt f√ºr strukturierte Ausgabe:**

```python
STRUCTURED_RESPONSE_PROMPT = """
Du bist VERITAS, ein KI-Assistent f√ºr deutsches Baurecht.

WICHTIG: Deine Antworten M√úSSEN folgendes JSON-Format haben:

{
  "type": "structured_response",
  "version": "1.0",
  "content": {
    "text": "Hauptantwort in **Markdown** mit allen Details...",
    "metadata": {
      "confidence": 0.95,
      "sources": [
        {"url": "https://...", "title": "BauGB ¬ß1"},
        {"file": "dokument.pdf", "page": 5}
      ],
      "agent": "VERITAS Legal RAG"
    },
    "suggestions": [
      "Welche Ausnahmen gibt es?",
      "Gilt das auch f√ºr Bestandsbauten?"
    ],
    "widgets": [
      // Optional: Code-Bl√∂cke
      {
        "type": "code_block",
        "language": "python",
        "code": "...",
        "caption": "Beispiel"
      },
      // Optional: Tabellen
      {
        "type": "table",
        "headers": ["Spalte 1", "Spalte 2"],
        "rows": [["Wert 1", "Wert 2"]]
      }
    ]
  }
}

REGELN:
1. "text" enth√§lt die HAUPTANTWORT in Markdown
2. "confidence" ist ein Wert zwischen 0.0 und 1.0
3. "sources" enth√§lt ALLE verwendeten Quellen
4. "suggestions" enth√§lt 2-3 Follow-up-Fragen
5. "widgets" ist OPTIONAL f√ºr Code, Tabellen, etc.

Beantworte folgende Frage:
"""
```

---

### Phase 4: Integration in Chat-Formatter

**Erweitere `ChatDisplayFormatter`:**

```python
class ChatDisplayFormatter:
    def __init__(self, ..., widget_renderer=None):
        # ... existing code ...
        self.widget_renderer = widget_renderer
    
    def _render_assistant_message(self, message: dict, ...):
        content = message.get('content', '')
        
        # Pr√ºfe ob strukturierte Response
        if self._is_structured_response(content):
            response_data = json.loads(content)
            self._render_structured_response(response_data)
        else:
            # Fallback: Alte Markdown-Rendering
            self.markdown_renderer.render_markdown(content)
    
    def _render_structured_response(self, data: dict):
        """Rendert strukturierte LLM-Response"""
        content = data.get('content', {})
        
        # 1. Haupttext rendern
        main_text = content.get('text', '')
        self.markdown_renderer.render_markdown(main_text)
        
        # 2. Widgets rendern
        for widget_spec in content.get('widgets', []):
            self.widget_renderer.render_widget(widget_spec)
        
        # 3. Metadaten rendern
        metadata = content.get('metadata', {})
        if metadata:
            self._insert_metadata_section(metadata)
        
        # 4. Quellen rendern
        sources = metadata.get('sources', [])
        if sources:
            self._insert_sources(sources)
        
        # 5. Vorschl√§ge rendern
        suggestions = content.get('suggestions', [])
        if suggestions:
            self._insert_suggestions(suggestions)
```

---

## üìä Widget-Typen (Vollst√§ndige Liste)

### 1. **Text-basiert** (bereits vorhanden ‚úÖ)
- Markdown
- Code-Bl√∂cke mit Syntax-Highlighting
- Tabellen

### 2. **Media** (neu ‚≠ê)
- **Images** (PNG, JPG, GIF)
  - Von URL, lokalem Pfad, oder Base64
  - Auto-Resize
  - Caption-Support
- **Videos** (MP4, WebM) ‚ö†Ô∏è Eingeschr√§nkt in Tkinter
  - Thumbnail + Externe Player-Link
  - Oder: Embedded mit `tkintervideo`

### 3. **Interactive Widgets** (neu ‚≠ê)
- **Buttons**
  - Custom Actions
  - Payload-Support
  - Styled (Farben, Icons)
- **Links** (bereits vorhanden ‚úÖ)
- **Checkboxes**
- **Radio Buttons**
- **Sliders**
- **Dropdown-Men√ºs**

### 4. **Visualisierungen** (neu ‚≠ê)
- **Canvas-Zeichnungen**
  - Linien, Rechtecke, Kreise, Text
  - Custom Draw-Commands
- **Charts/Diagramme** (matplotlib)
  - Bar Charts
  - Line Charts
  - Pie Charts
  - Scatter Plots
- **Mindmaps** (GraphViz oder Custom)
- **Timelines**

### 5. **Custom Widgets** (neu ‚≠ê)
- **Collapsible Sections** (bereits vorhanden ‚úÖ)
- **Tabs**
- **Accordions**
- **Progress Bars**
- **Rating Stars**

---

## üöÄ Implementierungs-Roadmap

### **Phase 1: Foundation** (1-2 Tage)
- [x] ‚úÖ Recherche bestehender VERITAS-Features (DONE)
- [ ] JSON Schema definieren
- [ ] `WidgetRenderer` Basisklasse erstellen
- [ ] Response-Parser implementieren

### **Phase 2: Basic Widgets** (2-3 Tage)
- [ ] Image-Rendering (PIL/Pillow)
- [ ] Button-Rendering
- [ ] Canvas-Rendering
- [ ] Chart-Rendering (matplotlib)

### **Phase 3: LLM Integration** (1-2 Tage)
- [ ] Structured Response Prompt
- [ ] Backend API-√Ñnderungen
- [ ] Frontend Parser-Integration
- [ ] Fallback f√ºr alte Responses

### **Phase 4: Advanced Features** (2-3 Tage)
- [ ] Video-Support (Thumbnails + Links)
- [ ] Interactive Widgets (Sliders, Dropdowns)
- [ ] Custom Widget Templates
- [ ] Widget-Galerie (Dokumentation)

### **Phase 5: Testing & Optimization** (1-2 Tage)
- [ ] Memory-Management (Cleanup)
- [ ] Performance-Tests
- [ ] Error-Handling
- [ ] Dokumentation

**Total Estimate:** 7-12 Tage

---

## üí° Beispiel-Use-Cases

### Use-Case 1: Rechtliche Frage mit Tabelle

**User:** "Welche Abst√§nde gelten f√ºr Windkraftanlagen?"

**LLM Response:**
```json
{
  "content": {
    "text": "Gem√§√ü **TA L√§rm** gelten folgende Mindestabst√§nde f√ºr Windkraftanlagen:",
    "widgets": [
      {
        "type": "table",
        "headers": ["Gebietstyp", "Mindestabstand", "Grenzwert Tag", "Grenzwert Nacht"],
        "rows": [
          ["Wohngebiet", "500m", "55 dB(A)", "40 dB(A)"],
          ["Mischgebiet", "300m", "60 dB(A)", "45 dB(A)"],
          ["Gewerbegebiet", "150m", "65 dB(A)", "50 dB(A)"]
        ]
      }
    ],
    "sources": [
      {"url": "https://...", "title": "TA L√§rm ¬ß6"}
    ],
    "suggestions": [
      "Gibt es Ausnahmen f√ºr Bestandsanlagen?",
      "Wie werden die Abst√§nde gemessen?"
    ]
  }
}
```

**Rendering:**
- Markdown-Text
- Sch√∂ne formatierte Tabelle (mit Rahmen)
- Klickbare Quellen
- Klickbare Vorschl√§ge

---

### Use-Case 2: Code-Beispiel mit Diagramm

**User:** "Zeige mir wie ich BImSchG-Daten analysiere"

**LLM Response:**
```json
{
  "content": {
    "text": "Hier ist ein Python-Beispiel f√ºr BImSchG-Datenanalyse:",
    "widgets": [
      {
        "type": "code_block",
        "language": "python",
        "code": "import pandas as pd\n\ndf = pd.read_csv('bimschg_data.csv')\nprint(df.head())",
        "caption": "Daten einlesen"
      },
      {
        "type": "chart",
        "chart_type": "bar",
        "title": "Genehmigungen pro Jahr",
        "data": {
          "labels": ["2020", "2021", "2022", "2023"],
          "values": [150, 180, 220, 195]
        }
      }
    ]
  }
}
```

**Rendering:**
- Code-Block mit Syntax-Highlighting + Copy-Button
- Bar-Chart (matplotlib embedded)

---

### Use-Case 3: Interaktive Visualisierung

**User:** "Zeige mir die L√§rmausbreitung"

**LLM Response:**
```json
{
  "content": {
    "text": "Hier ist eine schematische Darstellung der L√§rmausbreitung:",
    "widgets": [
      {
        "type": "canvas",
        "width": 500,
        "height": 300,
        "draw_commands": [
          {"cmd": "rect", "x": 50, "y": 100, "width": 30, "height": 80, "fill": "gray", "outline": "black"},
          {"cmd": "text", "x": 65, "y": 140, "text": "Anlage", "font": ["Arial", 10]},
          {"cmd": "line", "x1": 80, "y1": 140, "x2": 200, "y2": 140, "color": "red", "width": 2},
          {"cmd": "text", "x": 140, "y": 130, "text": "55 dB(A)", "font": ["Arial", 10], "color": "red"},
          {"cmd": "line", "x1": 80, "y1": 140, "x2": 350, "y2": 140, "color": "orange", "width": 2},
          {"cmd": "text", "x": 230, "y": 130, "text": "45 dB(A)", "font": ["Arial", 10], "color": "orange"}
        ]
      },
      {
        "type": "button",
        "label": "Detaillierte Berechnung anzeigen",
        "action": "show_calculation",
        "payload": {"calc_id": "noise_calc_123"}
      }
    ]
  }
}
```

**Rendering:**
- Canvas mit L√§rmausbreitungs-Diagramm
- Klickbarer Button f√ºr Details

---

## üîß Technische Details

### Dependencies

**Neue Dependencies:**
```python
# requirements.txt
Pillow>=10.0.0          # Image-Rendering
matplotlib>=3.7.0       # Charts/Diagramme
# Optional:
tkintervideo>=1.0.0    # Video-Support (experimental)
```

### Memory-Management

**Wichtig:** Embedded Widgets m√ºssen sauber aufger√§umt werden!

```python
def clear_chat_display(self):
    """Cleanup vor dem L√∂schen"""
    # 1. Widget-Renderer cleanup
    if hasattr(self, 'widget_renderer'):
        self.widget_renderer.cleanup()
    
    # 2. Text l√∂schen
    self.chat_text.delete('1.0', tk.END)
```

### Performance

**Optimierungen:**
- Images lazy-loaden (erst bei Sichtbarkeit)
- Charts cachen (nicht bei jedem Render neu zeichnen)
- Canvas-Objekte wiederverwenden
- Max Widget-Count pro Message (z.B. 10)

---

## üìö Moderne Referenz-Systeme

### 1. **Discord** (Electron + React)
- Rich Embeds (Images, Videos, Links)
- Code-Bl√∂cke mit Syntax-Highlighting
- Interactive Buttons
- **Limitation:** Web-basiert (nicht Tkinter)

### 2. **Telegram Desktop** (Qt/C++)
- Inline Bots mit Custom UIs
- Media-Rich Messages
- Buttons & Inline Keyboards
- **Lesson:** Qt hat besseres Widget-System als Tkinter

### 3. **VS Code Chat** (Electron)
- Code-Bl√∂cke mit Actions
- Collapsible Sections
- File References
- **Lesson:** Markdown + Custom Widgets Mix

### 4. **Jupyter Notebooks** (Web)
- Rich Output (HTML, Images, Charts, Interactive Widgets)
- Matplotlib-Integration
- **Lesson:** Best Practice f√ºr Chart-Embedding

### 5. **Python Tkinter Chat Examples** (GitHub)
**Gefundene Projekte:**
- `tkinter-chat` (basic text only)
- `tkinter-messaging-app` (basic UI)
- **Finding:** Keine mit Rich-Media-Support wie gew√ºnscht!

**Conclusion:** Wir entwickeln ein **eigenes System**, da es kein passendes Tkinter-Vorbild gibt!

---

## ‚úÖ Empfehlung & Implementierungs-Strategie

**Beste Strategie (AKTUALISIERT mit Streaming/Templates):**

1. **Nutze bestehende VERITAS-Features** ‚úÖ
   - MarkdownRenderer (1,000 LOC)
   - ChatDisplayFormatter (2,100 LOC)
   - **StreamingService** ‚ö° (639 LOC - BEREITS VORHANDEN!)
   
2. **Erweitere mit WidgetRenderer** (neue Komponente ~500 LOC)
   - Image/Video Support (PIL/Pillow)
   - Canvas-Widgets (tkinter.Canvas)
   - Chart-Widgets (matplotlib)
   - **Streaming-f√§hig** (inkrementelles Rendering)
   
3. **Implementiere Template-System** üìã (Backend, ~400 LOC)
   - `PromptTemplateLibrary` (5 Templates: Zust√§ndigkeit, Formell, Rechtlich, Sachlich, Vollst√§ndig)
   - Auto-Selection via Keyword-Matching
   - Template-spezifische System-Prompts
   
4. **Implementiere Adaptive Token-Manager** üìè (~200 LOC)
   - Dynamische Token-Size basierend auf:
     - Frage-Komplexit√§t (NLP-Analyse)
     - Template-Requirements
     - RAG-Kontext-Gr√∂√üe
     - Follow-up Detection
   
5. **Erweitere Streaming f√ºr Structured Response** ‚ö° (~300 LOC)
   - `StreamingStructuredResponseParser`
   - Newline-Delimited JSON (NDJSON)
   - Inkrementelles Widget-Rendering

**Warum kein externes System?**
- ‚úÖ VERITAS hat bereits 80% der Basis (inkl. Streaming!)
- ‚úÖ Tkinter-Native (keine Web-Tech n√∂tig)
- ‚úÖ Volle Kontrolle √ºber Rendering
- ‚úÖ Bessere Performance
- ‚úÖ Template-System erm√∂glicht verwaltungsrechtliche Spezialisierung
- ‚ùå Keine passenden Tkinter-Bibliotheken gefunden

---

## üöÄ Implementierungs-Roadmap (AKTUALISIERT)

### **Phase 1: Foundation + Streaming** (2-3 Tage)
- [x] ‚úÖ Recherche bestehender VERITAS-Features (DONE)
- [ ] JSON Schema definieren (Streaming-f√§hig, NDJSON)
- [ ] `StreamingStructuredResponseParser` erstellen
- [ ] `WidgetRenderer` Basisklasse (mit incremental rendering)
- [ ] Response-Parser f√ºr Streaming-Chunks

### **Phase 2: Backend Template-System** (2-3 Tage)
- [ ] `PromptTemplateLibrary` erstellen (5 Templates)
- [ ] `AdaptiveTokenManager` implementieren
- [ ] Backend API Endpoint `/api/v1/chat/structured`
- [ ] Template Auto-Selection (Keyword-Matching)
- [ ] Ollama-Integration (dynamische Tokens + Streaming)

### **Phase 3: Basic Widgets (Streaming-f√§hig)** (2-3 Tage)
- [ ] Image-Rendering (PIL/Pillow, incremental)
- [ ] Button-Rendering (incremental)
- [ ] Canvas-Rendering (incremental)
- [ ] Chart-Rendering (matplotlib, incremental)
- [ ] Table-Rendering (bereits vorhanden, aber streaming-update n√∂tig)

### **Phase 4: Frontend Integration** (1-2 Tage)
- [ ] Erweitere `ChatDisplayFormatter` f√ºr Streaming Structured Response
- [ ] Integration mit bestehendem `StreamingUIMixin`
- [ ] Progress-Updates w√§hrend Widget-Rendering
- [ ] Error-Handling (unvollst√§ndige JSON-Chunks)

### **Phase 5: Advanced Features** (2-3 Tage)
- [ ] Video-Support (Thumbnails + Links)
- [ ] Interactive Widgets (Sliders, Dropdowns)
- [ ] Custom Widget Templates
- [ ] Widget-Galerie (Dokumentation)
- [ ] Template-Auswahl im UI (User kann explizit Template w√§hlen)

### **Phase 6: Testing & Optimization** (2-3 Tage)
- [ ] Memory-Management (Cleanup, Streaming-Buffers)
- [ ] Performance-Tests (Streaming-Latency, Widget-Rendering)
- [ ] Template-Tests (alle 5 Templates mit Mock-Daten)
- [ ] Error-Handling (Streaming-Abbr√ºche, Netzwerkfehler)
- [ ] Dokumentation (Templates, Widget-Specs, Streaming-Format)

**Total Estimate:** 11-17 Tage (vorher 7-12 Tage, jetzt mit Streaming/Templates/Tokens)

---

## üéØ N√§chste Schritte (PRIORISIERT)

**Option 1: üèóÔ∏è Streaming-Prototyp (EMPFOHLEN)** ‚ö°
‚Üí Erstelle `StreamingStructuredResponseParser`  
‚Üí Test-Skript f√ºr Mock-Streaming-Responses (NDJSON)  
‚Üí Integration mit bestehendem `StreamingService`  
‚Üí **Zeit:** 60-90 Min  
‚Üí **Benefit:** Validiert Streaming-Architektur SOFORT

**Option 2: üìã Template-System Backend** 
‚Üí Erstelle `PromptTemplateLibrary` (5 Templates)  
‚Üí `AdaptiveTokenManager` implementieren  
‚Üí Backend API Endpoint  
‚Üí **Zeit:** 90-120 Min  
‚Üí **Benefit:** Validiert verwaltungsrechtliche Spezialisierung

**Option 3: üñºÔ∏è Widget-Renderer Prototyp**
‚Üí Erstelle `WidgetRenderer` mit Image + Button Support (streaming-f√§hig)  
‚Üí Test-Skript f√ºr Mock-Responses  
‚Üí **Zeit:** 60-90 Min  
‚Üí **Benefit:** Validiert UI-Rendering

**Option 4: üìê JSON-Schema finalisieren**
‚Üí Finalisiere NDJSON Streaming-Format  
‚Üí LLM-Prompt-Templates dokumentieren  
‚Üí **Zeit:** 30-45 Min  
‚Üí **Benefit:** Design-Dokumentation komplett

**Option 5: üìö Alle Specs finalisieren**
‚Üí Komplettes Design vor Implementation  
‚Üí Widget-Galerie dokumentieren  
‚Üí Template-Specs detaillieren  
‚Üí **Zeit:** 90-120 Min  
‚Üí **Benefit:** Vollst√§ndige Spezifikation

---

## üìä Zusammenfassung der Anforderungen

| Anforderung | Status | Implementierung | Aufwand |
|-------------|--------|-----------------|---------|
| **Strukturierte Metadaten** | ‚è≥ Design | JSON Schema + Parser | 1-2 Tage |
| **Rich Media (Images, Videos)** | ‚è≥ Design | WidgetRenderer + PIL/Pillow | 2-3 Tage |
| **Standardisiertes Format** | ‚è≥ Design | NDJSON Streaming Format | 1 Tag |
| **Tkinter-Rendering** | ‚úÖ Basis vorhanden | MarkdownRenderer erweitern | 1-2 Tage |
| **Streaming Support** ‚ö° | ‚úÖ VORHANDEN | StreamingStructuredResponseParser | 1-2 Tage |
| **Dynamische Token-Size** üìè | ‚è≥ Design | AdaptiveTokenManager | 1 Tag |
| **Template-System** üìã | ‚è≥ Design | PromptTemplateLibrary (5 Templates) | 2-3 Tage |

**Gesamt:** 11-17 Tage Development

---

## üí° Kritische Design-Entscheidungen

### 1. **Streaming-Format: NDJSON (Newline-Delimited JSON)**
**Warum?**
- ‚úÖ Jede Zeile = 1 vollst√§ndiges JSON-Objekt
- ‚úÖ Parsing m√∂glich ohne auf Ende zu warten
- ‚úÖ Standard-Format f√ºr Streaming-APIs
- ‚úÖ Kompatibel mit FastAPI `StreamingResponse`

**Beispiel:**
```ndjson
{"type":"response_start","metadata":{"template":"zustaendigkeit_behoerde"}}
{"type":"text_chunk","content":"Gem√§√ü **BauGB ¬ß35**..."}
{"type":"widget","widget":{"type":"table","headers":[...]}}
{"type":"response_end","metadata":{"confidence":0.92}}
```

---

### 2. **Template-System: Serverseitig (Backend)**
**Warum?**
- ‚úÖ Zentrale Verwaltung (keine Duplikation Frontend/Backend)
- ‚úÖ Einfachere Updates (nur Backend deployen)
- ‚úÖ Sicherheit (Prompts nicht im Frontend-Code)
- ‚úÖ Bessere Testbarkeit

**Frontend bekommt nur:**
- Template-ID im Response-Header
- Template-Name f√ºr UI-Badge ("Formale Pr√ºfung")

---

### 3. **Dynamische Tokens: Template-basiert + Complexity-Analysis**
**Warum?**
- ‚úÖ Template gibt Basis-Tokens vor (z.B. "Vollst√§ndige Pr√ºfung" = 16384)
- ‚úÖ Complexity-Analysis passt an (einfache Frage ‚Üí 50% Reduktion)
- ‚úÖ RAG-Kontext wird einberechnet
- ‚úÖ Follow-ups bekommen 30% Reduktion (Context-Reuse)

**Resultat:** Optimal zwischen Performance (kleine Tokens) und Qualit√§t (gro√üe Tokens f√ºr komplexe Themen)

---

### 4. **Widget-Rendering: Incremental w√§hrend Streaming**
**Warum?**
- ‚úÖ Besseres UX (User sieht sofort was passiert)
- ‚úÖ Keine "Freezing" w√§hrend LLM generiert
- ‚úÖ Widgets erscheinen "on-demand" wenn im Text erw√§hnt

**Herausforderung:**
- ‚ö†Ô∏è Widgets m√ºssen **vor** finaler Markdown-Verarbeitung bekannt sein
- ‚ö†Ô∏è LLM muss Widgets **explizit ank√ºndigen** via JSON-Chunk

**L√∂sung:**
```ndjson
{"type":"text_chunk","content":"Die Zust√§ndigkeiten sind in folgender Tabelle:"}
{"type":"widget","widget":{"type":"table",...}}  ‚Üê Widget-Chunk BEVOR Referenz
{"type":"text_chunk","content":"Wie Sie sehen k√∂nnen..."}
```

---

## üéâ Was macht dieses System BESONDERS?

1. **Streaming-Aware Structured Responses** ‚ö°
   - Kein Warten auf komplette Response
   - Widgets erscheinen w√§hrend LLM generiert
   - Progress-Updates f√ºr jeden Widget-Typ

2. **Verwaltungsrechtliche Spezialisierung** üìã
   - 5 spezialisierte Templates (Zust√§ndigkeit, Formell, Rechtlich, Sachlich, Vollst√§ndig)
   - Automatische Template-Selection via NLP
   - Template-spezifische Token-Limits

3. **Adaptive Context-Fenster** üìè
   - Keine festen 4096 Tokens mehr
   - Dynamisch 1024-16384 basierend auf Komplexit√§t
   - Bis zu 70% Token-Einsparung bei einfachen Fragen
   - Bis zu 300% mehr Tokens bei komplexen Analysen

4. **Tkinter-Native Rich Media** üé®
   - Images, Videos, Canvas, Charts DIREKT im Chat
   - Keine Browser-Dependency
   - Memory-effizientes Cleanup

5. **Production-Ready von Anfang an** ‚úÖ
   - Basiert auf bestehendem Streaming-System
   - Thread-Safe (ThreadManager-Integration)
   - Error-Handling f√ºr Streaming-Abbr√ºche
   - Graceful Degradation (Fallback auf Plain-Text)

---

**M√∂chtest du, dass ich mit der Implementation beginne?** üöÄ

**Meine Empfehlung:**  
**Option 1: Streaming-Prototyp** (60-90 Min)  
‚Üí Validiert Architektur SOFORT  
‚Üí Kann HEUTE getestet werden  
‚Üí Zeigt ob Streaming + Structured Response kompatibel sind

**Alternative:**  
**Option 2: Template-System** (90-120 Min)  
‚Üí Validiert verwaltungsrechtliche Spezialisierung  
‚Üí Kann sofort mit bestehendem Backend getestet werden

**Lass mich wissen, welche Option du bevorzugst!** üòä

---

**END OF KONZEPTPLAN (v4.1.0)**

