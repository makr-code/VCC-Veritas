# VERITAS Mock zu Production Migration Guide

## üö® Aktueller Status

### Backend Mockups (AKTIV):

1. **`/v2/query`** ‚Üê Frontend nutzt diesen
   - üìç Datei: `backend/api/veritas_api_backend.py` Zeile 772
   - ‚ùå Status: 100% Mock-Daten
   - üîß Simuliert: Agent-Ergebnisse, Antworten, Quellen
   - üìä Gibt zur√ºck: Hardcodierte Texte wie "Die Anfrage bezieht sich auf..."

2. **`/ask`**
   - üìç Datei: `backend/api/veritas_api_backend.py` Zeile 990
   - ‚ùå Status: 100% Mock (Test-Implementation)
   - üîß Gibt zur√ºck: "Test RAG-Antwort f√ºr: ..."

### Production-Ready Endpoints (UNGENUTZT):

3. **`/v2/intelligent/query`** ‚Üê ECHTE PIPELINE!
   - üìç Datei: `backend/api/veritas_api_backend.py` Zeile 333
   - ‚úÖ Status: Produktiv
   - üß† Features:
     - IntelligentMultiAgentPipeline
     - Ollama LLM Integration
     - RAG-basierte Agent-Selektion
     - Multi-Agent Orchestration
     - Real-time LLM Commentary
   - ‚ö†Ô∏è Problem: Frontend nutzt es NICHT

---

## üîß Migration Option 1: `/v2/query` auf echte Pipeline umstellen

### √Ñnderungen in `backend/api/veritas_api_backend.py`:

```python
@app.post("/v2/query")
async def veritas_chat_query(query_data: Dict[str, Any]):
    """
    MAIN ENDPOINT f√ºr Veritas Chat-App Integration
    Nutzt jetzt die echte Intelligent Pipeline!
    """
    start_time = time.time()
    
    try:
        query_text = query_data.get('query', '')
        if not query_text:
            raise HTTPException(status_code=400, detail="Query ist erforderlich")
        
        session_id = query_data.get('session_id', str(uuid.uuid4()))
        enable_streaming = query_data.get('enable_streaming', False)
        
        # PRODUKTIV: Nutze Intelligent Pipeline statt Mock
        if INTELLIGENT_PIPELINE_AVAILABLE and intelligent_pipeline:
            # Erstelle Pipeline Request
            pipeline_request = IntelligentPipelineRequest(
                query_id=f"query_{uuid.uuid4().hex[:8]}",
                query_text=query_text,
                user_context={"session_id": session_id, "mode": query_data.get('mode', 'veritas')},
                session_id=session_id,
                enable_llm_commentary=False,  # F√ºr schnellere Antworten
                enable_real_time_updates=enable_streaming,
                max_parallel_agents=5,
                timeout=60
            )
            
            # Pipeline ausf√ºhren
            pipeline_response = await intelligent_pipeline.process_intelligent_query(pipeline_request)
            processing_time = time.time() - start_time
            
            # Response im erwarteten Format
            return {
                'response_text': pipeline_response.response_text,
                'confidence_score': pipeline_response.confidence_score,
                'sources': pipeline_response.sources,
                'worker_results': pipeline_response.agent_results,
                'agent_results': pipeline_response.agent_results,
                'rag_context': pipeline_response.rag_context,
                'follow_up_suggestions': pipeline_response.follow_up_suggestions,
                'processing_metadata': {
                    'complexity': 'intelligent',
                    'processing_time': processing_time,
                    'agent_count': len(pipeline_response.agent_results),
                    'successful_agents': len(pipeline_response.agent_results),
                    'system_mode': 'intelligent_pipeline',
                    'streaming_available': STREAMING_AVAILABLE,
                    'timestamp': datetime.now().isoformat()
                }
            }
        else:
            # FALLBACK: Mock-Daten (wenn Pipeline nicht verf√ºgbar)
            logger.warning("‚ö†Ô∏è Intelligent Pipeline nicht verf√ºgbar - nutze Mock-Daten")
            return _generate_mock_response(query_text, session_id)
            
    except Exception as e:
        logger.error(f"‚ùå Fehler bei Query: {e}")
        return _generate_error_response(str(e))


def _generate_mock_response(query_text: str, session_id: str):
    """Fallback Mock-Daten"""
    # ... bestehender Mock-Code ...
```

### Vorteile:
- ‚úÖ Keine Frontend-√Ñnderung n√∂tig
- ‚úÖ Sofortige echte LLM-Integration
- ‚úÖ Echter RAG mit Dokumenten-Suche
- ‚úÖ Graceful Fallback bei Fehler

### Nachteile:
- ‚ö†Ô∏è Ben√∂tigt laufende Ollama-Installation
- ‚ö†Ô∏è Ben√∂tigt konfigurierte Dokumente/Datenbank

---

## üîß Migration Option 2: Frontend auf `/v2/intelligent/query` umstellen

### √Ñnderungen in `frontend/veritas_app.py`:

```python
def _send_to_backend(self, message: str):
    # ... bestehender Code ...
    
    # Erstelle Request f√ºr Intelligent Pipeline
    request_payload = {
        "query": message,
        "session_id": self.session_id,
        "enable_streaming": False,
        "enable_intermediate_results": False,
        "enable_llm_thinking": False  # oder True f√ºr LLM Commentary
    }
    
    # Sende an Intelligent Pipeline Endpoint
    logger.info(f"üß† Sende Query an /v2/intelligent/query")
    api_response = requests.post(
        f"{API_BASE_URL}/v2/intelligent/query",
        json=request_payload,
        timeout=60
    )
    
    # Response-Format ist kompatibel!
    # answer, confidence_score, sources, agent_results, etc.
```

### Vorteile:
- ‚úÖ Nutzt dedizierte Pipeline-Endpoint
- ‚úÖ Zugriff auf LLM Commentary (optional)
- ‚úÖ Bessere Trennung Mock/Prod

### Nachteile:
- ‚ö†Ô∏è Frontend-√Ñnderung n√∂tig
- ‚ö†Ô∏è Anderes Response-Format (aber √§hnlich)

---

## üìä Empfehlung: **Option 1**

√Ñndere `/v2/query` Backend-Endpoint, um echte Pipeline zu nutzen:

1. **Sofortige Produktivit√§t**: Frontend funktioniert ohne √Ñnderung
2. **Sauberer Fallback**: Mock-Daten bei Pipeline-Fehler
3. **Schrittweise Migration**: Kann sp√§ter optimiert werden

---

## ‚úÖ Checkliste vor Production:

### Backend:
- [ ] IntelligentMultiAgentPipeline initialisiert
- [ ] Ollama l√§uft und ist erreichbar
- [ ] LLM-Modelle heruntergeladen (llama3.1:latest etc.)
- [ ] RAG-Dokumente importiert
- [ ] UDS3 Datenbanken konfiguriert (optional)

### Frontend:
- [ ] API_BASE_URL korrekt (aktuell: http://127.0.0.1:5000)
- [ ] Error-Handling f√ºr Pipeline-Fehler
- [ ] Timeout erh√∂ht f√ºr echte LLM-Anfragen (60s+)

### Testing:
- [ ] Test-Query mit echter Pipeline: `curl POST /v2/intelligent/query`
- [ ] Frontend-Test mit echter Backend-Antwort
- [ ] Fehlerfall-Test (Pipeline nicht verf√ºgbar)

---

## üöÄ Quick-Start Migration:

```bash
# 1. Pr√ºfe ob Pipeline verf√ºgbar
curl http://localhost:5000/health

# Sollte zeigen:
# "intelligent_pipeline_available": true
# "ollama_available": true

# 2. Test echte Pipeline
curl -X POST http://localhost:5000/v2/intelligent/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Was ist VERITAS?", "session_id": "test", "enable_streaming": false}'

# 3. Wenn erfolgreich: Ersetze /v2/query Implementation im Backend
# 4. Starte Backend neu
# 5. Frontend testen - sollte echte Antworten bekommen
```

---

## üìù Aktueller Code-Status:

### Mock-Code in `/v2/query` (Zeile 772-898):
```python
# Simulierte Agent-Ergebnisse (wie im Test-Backend)
agent_results = {
    'geo_context': {
        'response_text': f'Geo-Kontext f√ºr Query: {query_text[:50]}...',
        # ... FAKE DATEN
    }
}

# Simuliere finale Antwort
main_response = f"""
**Antwort auf Ihre Frage**: {query_text}
**Geo-Kontext**: Die Anfrage bezieht sich auf den lokalen Verwaltungsbereich.
# ... HARDCODIERT
"""
```

### Produktiv-Code in `/v2/intelligent/query` (Zeile 377):
```python
# Pipeline ausf√ºhren - ECHT!
pipeline_response = await intelligent_pipeline.process_intelligent_query(pipeline_request)

# ECHTE Antworten vom LLM
# ECHTE RAG-Dokumente
# ECHTE Agent-Orchestration
```

---

## üéØ N√§chste Schritte:

1. **Entscheidung**: Option 1 oder Option 2?
2. **Backup**: Aktuellen `/v2/query` Code sichern
3. **Migration**: Code ersetzen
4. **Testen**: Mit echten Queries testen
5. **Monitoring**: Logs beobachten f√ºr Fehler

**Empfehlung: Option 1 - Backend `/v2/query` auf echte Pipeline umstellen**
