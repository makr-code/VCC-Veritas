Architektonisches Konzept für ein agentenbasiertes Deep-Research-System auf On-Premise-BasisI. Konzeptioneller Rahmen für ein On-Premise Agentic Deep Research System1.1. Einleitung: Die nächste Evolutionsstufe der InformationsgewinnungDie moderne Unternehmenslandschaft erfordert eine Informationsgewinnung, die weit über die Kapazitäten traditioneller Suchmaschinen und einfacher Frage-Antwort-Systeme hinausgeht. Der Übergang von reaktiven zu proaktiven, autonomen Forschungssystemen markiert einen Paradigmenwechsel. Nach dem Vorbild fortschrittlicher Funktionen wie Googles "Deep Research" wird hier ein System konzipiert, das nicht nur oberflächliche Antworten liefert, sondern komplexe Sachverhalte tiefgreifend analysiert.1 Es formuliert eigenständig Hypothesen, sammelt und bewertet Beweise aus heterogenen Datenquellen und synthetisiert die gewonnenen Erkenntnisse zu kohärenten, handlungsorientierten Narrativen.1Die Entscheidung für eine On-Premise-Implementierung dieser Architektur ist eine strategische Notwendigkeit, die sich aus mehreren kritischen Imperativen ergibt. In einer Zeit, in der Daten als das wertvollste Gut eines Unternehmens gelten, gewährleistet eine On-Premise-Lösung die vollständige Datenhoheit und -sicherheit.6 Sensible Unternehmensinformationen, Forschungsdaten und geistiges Eigentum verbleiben zu jeder Zeit innerhalb der kontrollierten Infrastruktur des Unternehmens. Dies ist insbesondere für regulierte Branchen wie das Finanzwesen, die Medizintechnik oder die Rechtsberatung von unabdingbarer Bedeutung.7 Darüber hinaus ermöglicht der direkte Zugriff auf die Infrastruktur eine tiefgreifende Anpassbarkeit, wie das Fine-Tuning von Sprachmodellen (LLMs) auf proprietären Datensätzen, um eine unübertroffene Domänenspezifität zu erreichen.9 Schließlich schützt dieser Ansatz vor den Risiken eines Vendor-Lock-ins und den oft unkalkulierbaren, eskalierenden Kosten, die mit der Nutzung kommerzeller Cloud-APIs verbunden sind.61.2. Kernprinzipien der ArchitekturDie Robustheit und Leistungsfähigkeit des vorgeschlagenen Systems basieren auf vier fundamentalen architektonischen Prinzipien, die zusammen ein agiles, intelligentes und nachvollziehbares Forschungsinstrument formen.Prinzip 1: Hierarchische OrchestrierungDas System implementiert eine zweistufige Steuerung, um sowohl Resilienz als auch dynamische Agilität zu gewährleisten. Auf der Macro-Ebene verwaltet ein robuster Workflow-Orchestrator den übergeordneten, langlebigen Forschungsprozess. Diese Ebene ist verantwortlich für das Scheduling, die Fehlerbehandlung (z.B. Wiederholungsversuche bei temporären Ausfällen) und die Integration menschlicher Freigabeschritte.10 Auf der Micro-Ebene koordiniert ein spezialisiertes Agenten-Framework die dynamische, kontextsensitive Zusammenarbeit der einzelnen KI-Agenten.11 Diese Trennung ermöglicht es, die Stabilität des Gesamtprozesses von der Flexibilität der einzelnen Forschungsschritte zu entkoppeln.Prinzip 2: Agenten als spezialisierte WerkzeugeAnstelle eines monolithischen, allwissenden KI-Modells wird das System als ein kollaboratives Team von hochspezialisierten Agenten konzipiert. Jeder Agent verfügt über eine klar definierte Fähigkeit und einen begrenzten Handlungsraum – sei es die Abfrage einer Graphendatenbank, die Durchführung einer semantischen Vektorsuche oder die Analyse von Webinhalten.12 Dieser Ansatz hat entscheidende Vorteile: Er erhöht die Zuverlässigkeit und Vorhersagbarkeit der einzelnen Operationen, vereinfacht die Wartung und das Debugging erheblich und ermöglicht die modulare Erweiterung des Systems durch das Hinzufügen neuer spezialisierter Agenten, ohne die Kernlogik zu beeinträchtigen.11Prinzip 3: Zustandsbehaftete Iteration (Stateful Iteration)Ein echter Forschungsprozess ist selten linear; er ist von Natur aus zyklisch und iterativ. Neue Erkenntnisse führen oft zur Neubewertung früherer Annahmen und zur Anpassung der Strategie. Das System muss diesen Prozess abbilden können. Eine explizite und persistente Zustandsverwaltung ist daher von zentraler Bedeutung. Das System muss in der Lage sein, den gesamten Kontext einer Recherche über mehrere Schritte und Schleifen hinweg aufrechtzuerhalten, zu früheren Zuständen zurückzukehren, um alternative Pfade zu erkunden, und seine Entscheidungen basierend auf dem kumulierten Wissen dynamisch anzupassen.Prinzip 4: Reflexion und SelbstkorrekturDie Qualitätssicherung ist kein nachgelagerter Schritt, sondern ein integraler Bestandteil jeder Iteration. Das System muss die Fähigkeit zur Selbstreflexion besitzen. Nach jedem Schritt der Informationssammlung bewertet ein dedizierter Agent die Qualität, Relevanz und Vertrauenswürdigkeit der gefundenen Ergebnisse.14 Diese kritische Selbsteinschätzung dient als Grundlage für die Planung des nächsten Schrittes. Führt ein Forschungspfad in eine Sackgasse oder liefert er Ergebnisse von geringer Qualität, kann das System dies erkennen und proaktiv den Kurs korrigieren, indem es beispielsweise Suchanfragen neu formuliert oder alternative Datenquellen erschließt.Der architektonische Wandel von einer einfachen, linearen Pipeline zu einem zyklischen, reflektierenden agentenbasierten System stellt eine fundamentale Entwicklung dar – weg von der reinen "Datenverarbeitung" hin zur "Emulation kognitiver Prozesse". Eine traditionelle RAG-Pipeline (Retrieval-Augmented Generation) ist ein zustandsloser, deterministischer Prozess: Anfrage, Abruf, Anreicherung, Generierung. Die hier geforderte Architektur hingegen ist inhärent zyklisch, da das LLM Ergebnisse bewertet und neue Anfragen formuliert. Frameworks wie LangGraph werden explizit gewählt, weil sie Zyklen und persistenten Zustand unterstützen, im Gegensatz zu rigiden Directed Acyclic Graphs (DAGs).15 Die Hinzunahme eines Selbstbewertungsschrittes, wie dem "LLM-as-a-Judge"-Muster 18, führt eine meta-kognitive Ebene ein. Das System führt nicht nur Aufgaben aus, sondern es denkt über seine eigene Leistung nach. Diese Architektur ist somit keine simple Datenpipeline mehr, sondern eine kognitive Architektur, die einen menschlichen Forschungsprozess nachbildet: Planung, Ausführung, Reflexion. Dies hat tiefgreifende Auswirkungen auf den Betrieb des Systems. Einfache Metriken wie Erfolg oder Misserfolg einer Aufgabe sind unzureichend. Es wird notwendig, den "kognitiven Zustand" des Systems zu überwachen: Warum wurde ein bestimmter Pfad gewählt? War die Selbsteinschätzung korrekt? Dies unterstreicht die Notwendigkeit von Observability-Tools wie LangSmith, die nicht nur die Ausführung, sondern den Argumentationspfad des Systems nachzeichnen können. Die operative Herausforderung verschiebt sich von der Überwachung der Infrastruktur zur Auditierung kognitiver Prozesse.II. Architektonische Blaupause: Eine Multi-Layer-OrchestrierungsstrategieDie Umsetzung der konzeptionellen Prinzipien erfordert eine durchdachte, mehrschichtige Architektur. Diese Blaupause definiert die technologischen Komponenten und ihre Interaktionen, um ein robustes und flexibles Deep-Research-System zu schaffen.2.1. Die Macro-Ebene: Resiliente Workflow-Orchestrierung mit PrefectLanglebige, komplexe KI-Workflows sind einer Vielzahl von potenziellen Störungen ausgesetzt. Dazu gehören temporäre Systemausfälle, Timeouts bei API-Aufrufen, eine unvorhersehbare, durch den Kontext bedingte Verzweigung der Logik und die Notwendigkeit, an kritischen Punkten menschliche Expertise einzuholen.20 Herkömmliche, auf starren DAGs basierende Orchestrierungs-Engines wie Apache Airflow sind für diese Art von dynamischen Prozessen oft unzureichend.Prefect positioniert sich hier als überlegene Lösung für die Macro-Orchestrierung. Im Gegensatz zu rigiden DAGs ermöglicht die dynamische Engine von Prefect die Modellierung von Workflows, die auf Daten und Ereignisse reagieren, anstatt einem vordefinierten Pfad zu folgen.17 Für das Deep-Research-System werden folgende Kernfunktionen von Prefect genutzt:Scheduling: Das planmäßige oder ereignisgesteuerte Starten von komplexen Forschungsaufträgen.Intelligent Retries: Die Konfiguration von anpassbaren Wiederholungsstrategien für fehlgeschlagene Agenten-Aufrufe. Schlägt beispielsweise eine Abfrage an eine externe Datenquelle aufgrund eines temporären Netzwerkproblems fehl, kann Prefect den Versuch automatisch nach einer definierten Wartezeit wiederholen, ohne den gesamten Prozess abzubrechen.Observability: Prefect bietet ein zentrales Dashboard zur Überwachung des End-to-End-Prozesses. Dies ermöglicht es den Betreibern, den Status jedes Forschungsauftrags in Echtzeit zu verfolgen, Engpässe zu identifizieren und Fehlerursachen schnell zu diagnostizieren.Human-in-the-Loop: Das System kann so konfiguriert werden, dass der Workflow an vordefinierten Punkten pausiert und auf eine menschliche Freigabe wartet. Beispielsweise könnte ein menschlicher Experte die vom System formulierten Hypothesen oder die finale Synthese validieren, bevor diese als Endergebnis freigegeben wird. Prefect verwaltet diesen Pausierungs- und Wiederaufnahmeprozess nahtlos.2.2. Die Micro-Ebene: Dynamische Agenten-Koordination mit LangGraphWährend Prefect den übergeordneten Rahmen vorgibt, ist für die feingranulare, iterative Koordination der Agenten ein spezialisiertes Framework erforderlich. Die Wahl des richtigen Micro-Orchestrators ist eine der kritischsten technologischen Entscheidungen in dieser Architektur. Die Analyse der verfügbaren Frameworks, insbesondere LangGraph und AutoGen, zeigt, dass LangGraph für die spezifischen Anforderungen des Deep-Research-Prozesses deutlich besser geeignet ist.LangGraph, entwickelt vom LangChain-Team, modelliert Agenten-Workflows als explizite, zustandsbehaftete Graphen. Dies ermöglicht eine präzise Kontrolle über den Ablauf, einschließlich der Implementierung von Schleifen und bedingten Verzweigungen – eine Grundvoraussetzung für den geforderten iterativen Forschungs- und Verfeinerungsprozess.21 AutoGen von Microsoft hingegen verfolgt einen konversationsbasierten Ansatz, bei dem Agenten durch den Austausch von Nachrichten interagieren. Dieser Ansatz eignet sich hervorragend für emergente, kollaborative Aufgaben wie Brainstorming, bietet aber weniger explizite Kontrolle und Nachvollziehbarkeit für einen strukturierten, mehrstufigen Prozess wie die Tiefenrecherche.12Die folgende Tabelle stellt die beiden Frameworks gegenüber und begründet die Entscheidung für LangGraph im Kontext des Deep-Research-Anwendungsfalls.Tabelle 2.1: Vergleich der Agenten-Orchestrierungs-FrameworksKriteriumLangGraphAutoGenImplikation für den Deep Research-ProzessKontrollflussExpliziter, gerichteter Graph (DAG) mit der Möglichkeit zur Definition von Zyklen.15 Der Ablauf ist deterministisch und nachvollziehbar.Konversationsbasiert, impliziter Fluss, der sich aus dem Dialog der Agenten ergibt.21 Der Ablauf ist emergent und weniger vorhersagbar.LangGraph ermöglicht die präzise Modellierung des geforderten iterativen Forschungs-, Evaluierungs- und Verfeinerungszyklus. Die Nachvollziehbarkeit ist für Audits entscheidend.ZustandsverwaltungFormalisiertes, persistentes Zustandsobjekt (StateGraph), das explizit zwischen den Knoten weitergegeben wird.23Der Zustand ist implizit im Konversationsverlauf enthalten und erfordert manuelle Verwaltung für langlebige Persistenz.12LangGraph bietet die notwendige Robustheit und Nachvollziehbarkeit für langlebige Recherchen, bei denen der Kontext über viele Schritte erhalten bleiben muss.Human-in-the-LoopErmöglicht explizite Breakpoints, an denen der Graph pausiert und eine Manipulation des Zustands durch einen Menschen erlaubt ist.Die Integration erfolgt über einen speziellen UserProxyAgent, der als Teilnehmer in der Konversation agiert.22LangGraph bietet feinere und direktere Kontrollmöglichkeiten für menschliche Eingriffe und Korrekturen im laufenden Prozess.Debugging & VisualisierungDer Graph kann visualisiert werden, was den Kontrollfluss und die Entscheidungslogik transparent macht.16Weniger ausgereifte visuelle Werkzeuge; das Debugging von komplexen, Multi-Agenten-Konversationen kann unübersichtlich sein.21Die Wartbarkeit und das Debugging des komplexen Forschungsprozesses sind mit LangGraph einfacher zu gewährleisten, was die Betriebskosten senkt.2.3. Schnittstellen: Containerisierte Services mit FastAPIUm Modularität, Skalierbarkeit und eine klare Trennung der Verantwortlichkeiten zu gewährleisten, wird das gesamte LangGraph-System (einschließlich aller Agenten) als eigenständiger Microservice implementiert. FastAPI wird als API-Framework für diesen Service gewählt.Dieses Architekturmuster bietet mehrere Vorteile:Entkopplung: Der Macro-Orchestrator (Prefect) kommuniziert mit dem Agenten-System über eine wohldefinierte REST-API, anstatt direkt von dessen interner Implementierung abhängig zu sein.Skalierbarkeit: Der FastAPI-Service kann unabhängig von anderen Systemkomponenten skaliert werden. Bei hoher Last können einfach weitere Instanzen des Agenten-Systems in einem Kubernetes-Cluster bereitgestellt werden.Asynchrone Verarbeitung: FastAPI basiert auf asyncio und ist für hoch-performante, asynchrone Operationen ausgelegt. Dies ist entscheidend, da viele Agenten-Aufgaben (z.B. LLM-API-Aufrufe, Datenbankabfragen) I/O-gebunden sind. Durch die asynchrone Verarbeitung kann der Service Tausende von Anfragen gleichzeitig bearbeiten, ohne blockiert zu werden, was die Gesamtdurchlaufzeit und Ressourceneffizienz des Systems erheblich verbessert.24Der FastAPI-Server wird zentrale Endpunkte bereitstellen, die von Prefect aufgerufen werden, wie z.B. /start-research, /get-status/{research_id} und /provide-feedback/{research_id}.III. Das Herzstück der Orchestrierung: Der LLM-Planer und die LangGraph-ZustandsmaschineDas intelligente Verhalten des Systems entsteht im Kern durch die Interaktion eines zentralen Planungs-LLMs mit der flexiblen Zustandsmaschine, die durch LangGraph bereitgestellt wird.3.1. Der Planungs-Agent (Planner): Vom Prompt zum GraphenDie zentrale Intelligenz des Systems wird von einem leistungsstarken, On-Premise gehosteten LLM (z.B. ein Mixtral- oder Llama-3-Modell) verkörpert, das als Planungs-Agent (Planner) agiert. Seine primäre Aufgabe ist es, die oft abstrakte und hoch-abstrakte Nutzeranfrage in einen konkreten, ausführbaren und mehrstufigen Forschungsplan zu übersetzen.Um einen robusten und nachvollziehbaren Plan zu erstellen, nutzt der Planner die Technik des Chain-of-Thought (CoT) Prompting. Anstatt direkt eine Antwort zu generieren, wird das LLM angewiesen, seinen "Gedankenprozess" Schritt für Schritt zu externalisieren. Es zerlegt die Hauptfrage in logische Teilfragen, identifiziert die für jede Teilfrage am besten geeigneten Agenten (z.B. "Für die Analyse von Unternehmensbeziehungen ist der Neo4j-Agent erforderlich"), legt eine initiale Reihenfolge der Schritte fest und definiert die Kriterien für den Erfolg jedes Schrittes. Dieser initial erstellte Plan bildet die Grundlage für die erste Iteration im LangGraph-Workflow.3.2. Implementierung des Workflows als StateGraphLangGraph modelliert den Workflow als einen StateGraph, eine spezialisierte Form eines gerichteten Graphen, der explizit einen Zustand über seine Knoten hinweg verwaltet.Definition des zentralen Zustandsobjekts (AgentState)Das Herzstück des StateGraph ist eine Python-Klasse, die den globalen Zustand der gesamten Recherche zu jedem Zeitpunkt repräsentiert. Diese Klasse wird typischerweise als TypedDict implementiert, um Typsicherheit zu gewährleisten.15 Sie enthält alle relevanten Informationen, wie die ursprüngliche Anfrage, eine Liste der bisher gesammelten Fakten, die formulierten Hypothesen, die Ergebnisse der letzten Agenten-Aufrufe und die Bewertung dieser Ergebnisse. Dieses In-Memory-Objekt ist die direkte Repräsentation des persistierten JSON-Frameworks, das in Abschnitt VI detailliert wird.Knoten (Nodes)Jeder Knoten im Graphen repräsentiert eine atomare Aktion oder einen Agentenaufruf. Dies sind typischerweise Python-Funktionen, die den aktuellen Zustand (AgentState) als Eingabe erhalten, eine spezifische Aufgabe ausführen (z.B. run_web_search(state), query_neo4j(state), evaluate_results(state)) und ein Update für den Zustand zurückgeben.Kanten (Edges)Die Kanten definieren den Kontrollfluss zwischen den Knoten. LangGraph unterscheidet hierbei zwei entscheidende Typen:Statische Kanten: Diese definieren einen festen, unveränderlichen Ablauf. Beispielsweise folgt auf den Knoten parse_initial_query immer der Knoten generate_initial_plan.16 Sie werden mit workflow.add_edge("start_node", "end_node") deklariert.Bedingte Kanten (Conditional Edges): Sie sind das mächtigste Werkzeug zur Implementierung der dynamischen und iterativen Logik des Systems. Eine bedingte Kante verbindet einen Knoten mit mehreren potenziellen Folgeknoten. Nach der Ausführung des Startknotens wird eine spezielle Router-Funktion aufgerufen. Diese Funktion analysiert den aktuellen AgentState und gibt den Namen des nächsten auszuführenden Knotens zurück.25 Dies ermöglicht die Implementierung komplexer Logik wie:Schleifen: Eine Router-Funktion nach dem evaluate_results-Knoten könnte prüfen, ob im Zustand das Flag refinement_needed auf True gesetzt ist. Wenn ja, leitet sie den Fluss zurück zum reformulate_query-Knoten. Wenn nein, geht es weiter zum synthesize_report-Knoten.Verzweigungen: Ein Router nach der initialen Planerstellung könnte basierend auf den identifizierten Entitäten entscheiden, ob der nächste Schritt eine Abfrage an die interne SQL-Datenbank oder eine externe Web-Recherche sein soll.Die Verwendung des StateGraph in LangGraph transformiert das Problem der Agenten-Orchestrierung fundamental. Es wandelt sich von einer Herausforderung der Kontrollfluss-Programmierung (komplexe, verschachtelte if/else-Strukturen) zu einer Herausforderung der Zustandsmanipulation. Die "Intelligenz" des Systems liegt nicht in einem starren, fest verdrahteten Code, sondern in der Fähigkeit der Agenten, ein gemeinsames, dynamisches Zustandsobjekt strategisch zu modifizieren.Der StateGraph fungiert dabei als eine Art "Blackboard" oder gemeinsamer Arbeitsbereich. Die Agenten kommunizieren nicht direkt miteinander, sondern sie lesen und schreiben auf diesen gemeinsamen Zustand. Der Planungs-Agent setzt beispielsweise den initialen Wert für state['next_action']. Der Evaluierungs-Agent aktualisiert diesen Wert basierend auf seiner Bewertung der Ergebnisse. Ein Router-Knoten liest lediglich state['next_action'], um den nächsten Schritt zu bestimmen.25 Diese Architektur entkoppelt die Agenten vollständig und macht das System hochgradig modular. Ein neuer Agent kann hinzugefügt werden, indem man dem Planungs- und Evaluierungs-Agenten beibringt, bei Bedarf einen neuen Wert in state['next_action'] zu schreiben.Diese Entkopplung hat weitreichende positive Konsequenzen für das Debugging und die Auditierbarkeit. Um zu verstehen, warum das System einen bestimmten Weg eingeschlagen hat, muss man keine komplexe Code-Ausführung nachverfolgen. Stattdessen genügt es, die Historie des Zustandsobjekts zu inspizieren. Die Abfolge der Zustandsänderungen wird zu einem vollständigen, für Menschen lesbaren Protokoll des "Denkprozesses" des Systems. Das in Abschnitt VI beschriebene progressive JSON-Objekt ist somit nicht nur ein Datencontainer, sondern das primäre Werkzeug für Fehleranalyse, Governance und die Nachvollziehbarkeit der KI-Entscheidungen.IV. Das Agenten-Ökosystem: Ein Team spezialisierter Task-ExecutorsDas Herzstück der ausführenden Ebene ist ein Ökosystem von spezialisierten Agenten, die als Werkzeuge vom Planungs-Agenten aufgerufen werden, um spezifische Aufgaben der Informationsbeschaffung und -verarbeitung durchzuführen.4.1. Datenbank-Agenten (RAG über heterogene Quellen)Die Fähigkeit, auf eine Vielzahl interner Datenquellen zuzugreifen, ist entscheidend für den Mehrwert des Systems. Anstatt eines einzigen RAG-Ansatzes wird eine Multi-Agenten-Strategie für verschiedene Datenbanktypen verfolgt.Graph-Datenbank-Agent (Neo4j): Dieser Agent ist spezialisiert auf die Analyse von komplexen Beziehungen und Netzwerken. Er nutzt die GraphCypherQAChain von LangChain, um Anfragen in natürlicher Sprache in optimierte Cypher-Abfragen für die Neo4j-Datenbank zu übersetzen.25 Er ist ideal für die Beantwortung von Fragen wie: "Welche Abteilungen arbeiten an Projekten, die mit Technologie X in Verbindung stehen, und welche Experten sind daran beteiligt?".25Vektor-Datenbank-Agent (ChromaDB): Dieser Agent ist für die semantische Ähnlichkeitssuche in großen Mengen unstrukturierter Textdokumente (z.B. Berichte, E-Mails, Artikel) zuständig. Ein On-Premise-Server von ChromaDB wird im Kubernetes-Cluster betrieben, um die Datenhoheit zu wahren.28 Er beantwortet Fragen wie: "Finde alle Dokumente, die ein ähnliches technisches Konzept wie dieser Patentantrag beschreiben."SQL-Datenbank-Agent (PostgreSQL): Für präzise Abfragen über hochstrukturierte, tabellarische Daten, wie sie in traditionellen Unternehmensdatenbanken (z.B. ERP-, CRM-Systeme) zu finden sind.Die wahre Stärke des Systems liegt in der Hybrid-RAG-Strategie. Ein dedizierter Router-Knoten im LangGraph-Workflow analysiert die eingehende Teilfrage und entscheidet, welcher Datenbank-Agent – oder welche Kombination von Agenten – am besten geeignet ist. Ein komplexer Request könnte beispielsweise zuerst den Vektor-Datenbank-Agenten nutzen, um eine Liste relevanter Projekte zu einem bestimmten Thema zu finden. Die IDs dieser Projekte werden dann an den Graph-Datenbank-Agenten übergeben, um eine gezielte Cypher-Abfrage durchzuführen, die die an diesen Projekten beteiligten Personen und deren Verbindungen aufzeigt.254.2. Web-Recherche-Agent (Sicher und On-Premise)Die Nutzung von öffentlichen Suchmaschinen-APIs (z.B. Google, Bing) in einem sicherheitskritischen On-Premise-System stellt ein erhebliches Risiko dar. Jede Suchanfrage würde sensible interne Forschungsinteressen an externe Dritte weitergeben.Die Lösung für dieses Problem ist der Einsatz von SearxNG, einer leistungsstarken, selbst-gehosteten Metasuchmaschine.29 SearxNG aggregiert Ergebnisse von Hunderten von Suchdiensten, ohne dabei Benutzerprofile zu erstellen oder Suchanfragen zu protokollieren.Die Implementierung erfolgt durch das Deployment eines offiziellen SearxNG-Docker-Containers innerhalb des Kubernetes-Clusters. Der Web-Recherche-Agent wird dann mit einem LangChain-Tool ausgestattet, das nicht eine öffentliche API, sondern die interne, sichere SearxNG-API-Schnittstelle aufruft.23 Dies stellt sicher, dass der gesamte Web-Rechercheprozess innerhalb des kontrollierten Unternehmensnetzwerks verbleibt und die Vertraulichkeit der Forschungsaktivitäten gewahrt bleibt.4.3. Analyse- und Extraktions-AgentViele wertvolle Informationen liegen in unstrukturierten Dokumentenformaten wie PDFs, Word-Dokumenten oder Präsentationen vor. Der Analyse- und Extraktions-Agent ist mit einer Reihe von Werkzeugen ausgestattet, um diese Daten zu erschließen. Zu seinen Fähigkeiten gehören:Optical Character Recognition (OCR): Um Text aus gescannten Dokumenten oder Bildern zu extrahieren.Named Entity Recognition (NER): Um Schlüsselentitäten wie Personen, Organisationen, Orte und Fachbegriffe in Texten zu identifizieren und zu klassifizieren.Zusammenfassung: Um lange Dokumente auf ihre Kernaussagen zu reduzieren.Dieser Agent fungiert oft als Vorverarbeitungsschritt. Er nimmt ein rohes Dokument entgegen, extrahiert die strukturierten Informationen und stellt diese dem AgentState zur Verfügung, damit andere Agenten (z.B. der Graph-Datenbank-Agent) diese angereicherten Daten für ihre weiteren Analysen nutzen können.V. Die iterative Verfeinerungsschleife: Reflexion und SelbstkorrekturDie Fähigkeit des Systems, nicht nur Informationen zu sammeln, sondern deren Qualität kritisch zu bewerten und den eigenen Kurs zu korrigieren, ist das, was es von einem einfachen Automatisierungswerkzeug zu einem intelligenten Forschungspartner macht. Dieser Prozess wird durch eine iterative Schleife aus Ausführung, Bewertung und Neuplanung realisiert.5.1. Der Evaluator-Agent: Implementierung eines "LLM-as-a-Judge"Nach jedem signifikanten Schritt der Informationsbeschaffung – sei es eine Datenbankabfrage oder eine Web-Recherche – wird das Ergebnis nicht blind akzeptiert. Stattdessen wird ein spezialisierter Evaluator-Agent aufgerufen. Bei diesem Agenten handelt es sich um dasselbe oder ein anderes LLM, das jedoch mit einem sehr spezifischen Prompt instruiert wird, um als unparteiischer "Richter" (Judge) über die Qualität der vorgelegten Ergebnisse zu urteilen.Das Prompt-Engineering für diesen Evaluator ist entscheidend für die Zuverlässigkeit des gesamten Systems. Der Prompt enthält:Klare Bewertungskriterien: Genaue Anweisungen, worauf bei der Bewertung zu achten ist (siehe RAG-Triade unten).Eine definierte Bewertungsskala: Zum Beispiel eine numerische Skala von 0-3 oder qualitative Labels wie "Hoch Relevant", "Teilweise Relevant", "Irrelevant".Beispiele (Few-Shot Prompting): Einige Beispiele für gute und schlechte Ergebnisse und deren korrekte Bewertung, um das LLM zu kalibrieren und konsistentere Urteile zu gewährleisten.5.2. Die RAG-Triade als Bewertungs-FrameworkUm die Bewertung zu strukturieren und objektivieren, wendet der Evaluator-Agent die Prinzipien der RAG-Triade an, ein etabliertes Framework zur Bewertung von RAG-Systemen.18 Jedes Ergebnis wird anhand von drei Kriterien bewertet:Context Relevance (Kontextrelevanz): Bewertet, ob die abgerufenen Informationen (z.B. Datenbankeinträge, Webseitenabschnitte) tatsächlich für die spezifische, aktuelle Teilfrage relevant sind. Eine Quelle, die zwar das richtige Thema behandelt, aber die Frage nicht beantwortet, würde hier schlecht bewertet.Groundedness / Faithfulness (Faktenbasiertheit): Überprüft, ob die vom ausführenden Agenten generierte Zusammenfassung oder Antwort ausschließlich auf den bereitgestellten Quellen basiert. Dies ist ein entscheidender Mechanismus zur Erkennung und Vermeidung von "Halluzinationen", bei denen das LLM Fakten erfindet.Answer Relevance (Antwortrelevanz): Misst, wie gut das Gesamtergebnis die ursprüngliche Teilfrage des Planers beantwortet. Es kann vorkommen, dass die Quellen relevant und die Zusammenfassung faktenbasiert sind, die finale Antwort aber dennoch die Kernfrage verfehlt.Die Ergebnisse dieser drei Bewertungen werden als strukturierte Daten (z.B. numerische Scores und eine textuelle Begründung) im AgentState gespeichert. Sie bilden die faktische Grundlage für die nächste Planungsentscheidung.5.3. Dynamische Neuformulierung und Plan-AnpassungDer Planungs-Agent erhält nun den aktualisierten Zustand, der die Ergebnisse des letzten Schritts und die kritische Bewertung des Evaluator-Agenten enthält. Basierend auf diesen Informationen entscheidet er über das weitere Vorgehen. Dieser reflektive Prozess ermöglicht eine dynamische Selbstkorrektur.14Mögliche Aktionen sind:Anfrage verfeinern: Wenn der Evaluator eine niedrige "Context Relevance" meldet, erkennt der Planer, dass die letzte Suchanfrage zu ungenau war. Er formuliert eine neue, präzisere Anfrage, die beispielsweise zusätzliche Schlüsselwörter oder Ausschlusskriterien enthält, die sich aus den bisherigen Erkenntnissen ergeben.Agent oder Werkzeug wechseln: Stellt der Evaluator fest, dass eine Web-Suche nur unstrukturierte Meinungsartikel liefert, obwohl Fakten benötigt werden, könnte der Planer entscheiden, stattdessen eine gezielte Abfrage an die interne Neo4j-Datenbank zu richten, um verifizierte interne Daten zu erhalten.Hypothese verwerfen: Wenn mehrere Iterationen mit unterschiedlichen Strategien keine stützenden Beweise für eine Hypothese liefern, kann der Planer diesen Forschungspfad als Sackgasse markieren und im AgentState vermerken, um redundante zukünftige Suchen zu vermeiden.Dieser iterative Zyklus aus Aktion, Reflexion und Anpassung ist der Kernmechanismus, der es dem System ermöglicht, komplexe Probleme zu lösen und eine Ergebnisqualität zu erreichen, die die eines einzelnen, nicht-reflektiven LLM-Aufrufs bei weitem übersteigt.VI. Das einheitliche Zustandsobjekt & Architektur für kryptographische IntegritätUm die Prinzipien der Zustandsbehaftung, Nachvollziehbarkeit und insbesondere der geforderten Rechtssicherheit technisch umzusetzen, wird ein zentrales, progressiv fortschreibendes JSON-Objekt als wiederverwendbarer Rahmen konzipiert. Dieses Objekt dient als "Single Source of Truth" für jeden Forschungsauftrag. Es ist die serialisierte, persistente Form des LangGraph AgentState und wird nach jedem abgeschlossenen Schritt in der Datenbank gespeichert.6.1. Design-Ziele und GrundstrukturDie Gestaltung des JSON-Objekts verfolgt vier Hauptziele:Vollständige Nachvollziehbarkeit: Jeder einzelne Schritt, jede Entscheidung und jede Bewertung muss lückenlos dokumentiert sein, um einen vollständigen Audit-Trail zu gewährleisten.Wiederaufsetzbarkeit: Im Falle eines Systemausfalls muss der Prozess genau an dem Punkt wieder aufgenommen werden können, an dem er unterbrochen wurde, indem der letzte gespeicherte Zustand geladen wird.Debugging: Entwickler und Betreiber müssen in der Lage sein, den "Gedankenprozess" des Systems durch die Analyse des Zustandsobjekts leicht nachzuvollziehen.Menschliche Intervention: Die Struktur muss es ermöglichen, dass ein menschlicher Experte den Zustand einsehen und potenziell modifizieren kann, um den Prozess zu steuern.Die folgende Struktur zeigt ein konzeptionelles Beispiel für das JSON-Schema:JSON{
  "research_id": "uuid-v4-a1b2c3d4-e5f6-7890-1234-567890abcdef",
  "initial_query": "Analyse der Auswirkungen von generativer KI auf die Lieferkettenlogistik in der europäischen Automobilindustrie.",
  "status": "IN_PROGRESS",
  "global_state": {
    "known_entities": ["Generative KI", "Lieferkettenlogistik", "Europa", "Automobilindustrie"],
    "hypotheses":,
    "rejected_paths": [
      "Analyse der Auswirkungen auf die Personalplanung wurde als 'out of scope' markiert."
    ]
  },
  "execution_trace": [
  ],
  "current_task": {
    "task_id": 3,
    "agent": "WebSearchAgent",
    "prompt": "Studien und Whitepaper zu 'generative AI in automotive supply chain forecasting Europe 2023-2024'",
    "status": "EXECUTING"
  },
  "results_summary": {
    "intermediate_findings": "Eine Studie von 'Logistik Heute' identifiziert potenzielle Effizienzsteigerungen von 10-15% in der Ersatzteillogistik.",
    "confidence_score": 0.85
  },
  "integrity": {
    "currentStateHash": "sha256-...",
    "previousStateHash": "sha256-...",
    "stateSignature": "...",
    "qualifiedTimestampToken": null
  },
  "timestamps": {
    "created_at": "2024-10-27T10:00:00Z",
    "updated_at": "2024-10-27T11:35:10Z"
  }
}
6.2. Die execution_trace-Struktur: Der unveränderliche Audit-LogDas Herzstück der Nachvollziehbarkeit ist das execution_trace-Array. Jedes Mal, wenn ein Knoten im LangGraph-Workflow erfolgreich abgeschlossen wird, wird ein neues, unveränderliches Objekt an dieses Array angehängt. Es protokolliert exakt, was passiert ist.Struktur eines Trace-Eintrags:JSON{
  "task_id": 2,
  "timestamp": "2024-10-27T11:35:09Z",
  "agent": "PlannerAgent",
  "action": "EVALUATE_AND_PLAN",
  "input": {
    "previous_results": "Die erste Web-Suche lieferte nur allgemeine Marketing-Artikel.",
    "evaluation_prompt": "Bewerte die Relevanz der vorherigen Ergebnisse und plane den nächsten Schritt."
  },
  "output": {
    "raw_llm_response": "Die vorherigen Ergebnisse sind zu allgemein und nicht spezifisch für die Automobilindustrie. Eine spezifischere Suche ist erforderlich. Ich werde die Suche mit den Keywords 'automotive' und 'forecasting' verfeinern.",
    "parsed_plan": {
      "next_agent": "WebSearchAgent",
      "new_prompt": "Studien und Whitepaper zu 'generative AI in automotive supply chain forecasting Europe 2023-2024'"
    }
  },
  "evaluation": {
    "evaluator": "EvaluatorAgent",
    "metrics": {
      "context_relevance": 0.4,
      "groundedness": 0.9,
      "answer_relevance": 0.5
    },
    "feedback": "Die Ergebnisse der Aufgabe 1 waren zu allgemein und nicht auf die angefragte Branche fokussiert."
  }
}
6.3. Architektur für kryptographische Integrität und RechtssicherheitUm die strengen Anforderungen an einen rechtssicheren Betrieb zu erfüllen, wird das JSON-Zustandsobjekt durch eine mehrschichtige kryptographische Absicherung erweitert. Dieser Prozess verwandelt das execution_trace von einem einfachen Protokoll in einen unveränderlichen, überprüfbaren und rechtlich belastbaren Beweis.32Fundament: Das Saga-Log als Beweismittel: Der gesamte Rechercheprozess wird als eine Saga-Transaktion behandelt.35 Das execution_trace-Array im JSON-Objekt fungiert als das zentrale "Saga Log", das jeden Schritt, jede Entscheidung und jedes Ergebnis lückenlos dokumentiert.35Schritt 1: Hash-Verkettung zur Sicherung der Integrität: Um nachträgliche Manipulationen am Protokoll nachweisbar zu machen, wird eine Hash-Kette implementiert.Prozess: Nach jedem abgeschlossenen Schritt im execution_trace berechnet der VCC-Orchestrator einen kryptographischen Hash (z. B. SHA-256) über den gesamten bisherigen Trace. Dieser neue Hash wird zusammen mit dem Hash des vorherigen Zustands im integrity-Block des JSON-Objekts gespeichert.Ergebnis: Es entsteht eine Kette, bei der jeder Zustand den vorherigen kryptographisch versiegelt. Eine Änderung an einem früheren Eintrag würde die gesamte Kette ungültig machen, was Manipulationen sofort aufdeckt. Dies ist das Kernprinzip einer Blockchain, hier angewendet auf das Saga-Log.33Schritt 2: Digitale Signatur zur Sicherung der Urheberschaft: Um die Authentizität und Nichtabstreitbarkeit des Protokolls zu gewährleisten, wird der finale Zustand digital signiert.Prozess: Nach Abschluss der gesamten Recherche (Status COMPLETED) wird der finale Hash-Wert des execution_trace vom VCC-Orchestrator-Dienst mit dessen privatem Schlüssel signiert. Dieser Dienst erhält dafür ein dediziertes "System"-Zertifikat von der souveränen, internen VCC Certificate Authority (CA).36Ergebnis: Die stateSignature beweist kryptographisch, dass das Protokoll vom autorisierten VCC-System erstellt und seit der Signatur nicht mehr verändert wurde.Schritt 3: Qualifizierter Zeitstempel für rechtliche Verbindlichkeit: Um einen unanfechtbaren, rechtlich gültigen Zeitnachweis zu erbringen, wird der finale Hash zusätzlich mit einem qualifizierten Zeitstempel versehen.Prozess: Der finale, signierte Hash wird an einen externen, nach der eIDAS-Verordnung zertifizierten qualifizierten Vertrauensdiensteanbieter (TSP) gesendet. Dieser Dienst gibt einen qualifizierten elektronischen Zeitstempel (QET) zurück.33Ergebnis: Der qualifiedTimestampToken im JSON-Objekt genießt EU-weit die rechtliche Vermutung der Richtigkeit des Datums und der Uhrzeit sowie der Integrität der Daten.33 Dies verleiht dem gesamten Audit-Trail eine juristische Beweiskraft, die weit über einen einfachen Server-Zeitstempel oder einen Blockchain-Zeitstempel hinausgeht.Dieser mehrschichtige Ansatz aus Saga-Protokollierung, Hash-Verkettung, interner CA-Signatur und externem qualifiziertem Zeitstempel transformiert den JSON-Rechercheplan von einem einfachen Zustandsobjekt in ein rechtssicheres, selbst-authentifizierendes digitales Dokument, das den höchsten Anforderungen an Transparenz, Nachvollziehbarkeit und juristischer Belastbarkeit genügt.VII. On-Premise-Deployment und OperationalisierungDie Bereitstellung eines solch komplexen, aus mehreren Komponenten bestehenden Systems in einer On-Premise-Umgebung erfordert eine moderne, auf Containern basierende Infrastruktur. Kubernetes (K8s) wird als Orchestrierungsplattform der Wahl für dieses Vorhaben festgelegt.7.1. Containerisierung mit DockerDer erste Schritt zur Bereitstellung ist die Containerisierung jeder einzelnen Komponente des Systems. Dies stellt sicher, dass die Anwendungen und ihre Abhängigkeiten gekapselt sind und konsistent über verschiedene Umgebungen hinweg (von der Entwicklung bis zur Produktion) laufen.Anwendungs-Image: Es wird ein Dockerfile für die FastAPI-Anwendung erstellt, die den LangGraph-Agenten-Workflow enthält. Dieses Image bündelt die Python-Laufzeitumgebung, alle erforderlichen Bibliotheken (wie langgraph, fastapi, psycopg2-binary) und den Anwendungscode selbst.24Infrastruktur-Images: Für die unterstützenden Dienste werden offizielle oder unternehmensintern geprüfte Docker-Images verwendet. Dazu gehören separate Images für PostgreSQL (für die Zustandspersistenz), SearxNG (für die Web-Recherche) und ChromaDB (für die Vektorsuche).7.2. Orchestrierung mit Kubernetes (K8s)Kubernetes bietet die notwendigen Werkzeuge, um diese containerisierten Anwendungen automatisiert bereitzustellen, zu skalieren und zu verwalten. Die Deployment-Architektur wird durch eine Reihe von K8s-Manifestdateien (YAML) definiert:Deployment: Dieses K8s-Objekt wird verwendet, um die stateless FastAPI-Server zu verwalten. Es stellt sicher, dass eine definierte Anzahl von Replikaten (Pods) des Agenten-Systems immer läuft. Bei Ausfall eines Pods startet Kubernetes automatisch einen neuen.StatefulSet & PersistentVolumeClaim: Für zustandsbehaftete Anwendungen wie die PostgreSQL-Datenbank ist ein Deployment ungeeignet. Stattdessen wird ein StatefulSet verwendet. Es garantiert eine stabile, eindeutige Netzwerkidentität für jeden Pod und sorgt in Verbindung mit PersistentVolumeClaims dafür, dass die Daten auf einem persistenten Speichermedium (z.B. einem Netzwerkspeicher) abgelegt werden und einen Pod-Neustart überleben.24Service: Um eine stabile Kommunikationsschnittstelle zwischen den Pods zu schaffen, werden K8s-Services definiert. Beispielsweise wird ein Service für PostgreSQL erstellt, der eine feste interne DNS-Adresse (z.B. postgres-svc.default.svc.cluster.local) bereitstellt, die die Agenten-Pods zur Verbindung mit der Datenbank nutzen können.Ingress: Um den FastAPI-Service sicher für Anfragen von außerhalb des K8s-Clusters (z.B. vom Macro-Orchestrator Prefect oder einer Benutzeroberfläche) zugänglich zu machen, wird ein Ingress-Controller (z.B. NGINX) konfiguriert. Dieser kümmert sich um das Routing des externen Traffics zum richtigen Service und kann auch für Aufgaben wie TLS-Terminierung (HTTPS) und Authentifizierung genutzt werden.24ConfigMap & Secret: Die Konfiguration der Anwendungen wird von der Implementierung getrennt. Nicht-sensible Konfigurationsparameter (z.B. die interne URL des SearxNG-Dienstes, Loglevel) werden in ConfigMaps gespeichert. Sensible Daten wie Datenbankpasswörter oder API-Schlüssel für das On-Premise-LLM werden in Secrets abgelegt. Diese werden dann zur Laufzeit sicher als Umgebungsvariablen in die entsprechenden Pods injiziert.24Die Anforderung einer On-Premise-Lösung führt zu einer fundamentalen Verschiebung der Systemabhängigkeiten: weg von externen, verwalteten APIs hin zu interner, selbstverwalteter Infrastruktur. Ein Cloud-nativer Ansatz würde auf Dienste wie die Google Search API, die OpenAI API und eine verwaltete Datenbank wie Amazon RDS zurückgreifen, was den operativen Aufwand minimiert. Die On-Premise-Beschränkung erzwingt den Ersatz jeder dieser Komponenten durch eine selbst gehostete Alternative: die Google Search API wird durch SearxNG ersetzt 30, die OpenAI API durch ein selbst gehostetes LLM (z.B. über vLLM) und die verwaltete Datenbank durch eine in Kubernetes bereitgestellte PostgreSQL-Instanz.24Die Bereitstellung und Wartung dieser komplexen, oft zustandsbehafteten Anwendungen auf Kubernetes ist eine anspruchsvolle Ingenieursaufgabe, die tiefgreifende Expertise in den Bereichen Container-Orchestrierung, Netzwerk und Speichermanagement erfordert. Folglich liegen die Gesamtbetriebskosten (TCO) dieses On-Premise-Systems nicht primär in den Softwarelizenzen – da die meisten empfohlenen Werkzeuge Open Source sind –, sondern in den hochqualifizierten Fachkräften (DevOps/MLOps), die für das Management der Infrastruktur benötigt werden.Diese Kontrolle eröffnet jedoch gleichzeitig neue, leistungsstarke Möglichkeiten. Das selbst gehostete LLM kann gezielt auf den internen, proprietären Daten des Unternehmens feingetunt werden, um eine unerreichte Domänenkompetenz zu entwickeln. Die selbst gehostete SearxNG-Instanz kann so konfiguriert werden, dass sie interne Wissensdatenbanken oder spezifische Fachjournale priorisiert. Der direkte Zugriff auf die PostgreSQL-Zustandsdatenbank ermöglicht tiefgreifende Meta-Analysen über den Forschungsprozess selbst. Die On-Premise-Anforderung ist also, obwohl operativ herausfordernd, ein strategischer Enabler. Sie ermöglicht die Schaffung eines hochgradig angepassten und proprietären Forschungsvermögens, anstatt nur ein Konsument öffentlicher APIs zu sein.VIII. Integrationspfad und Empfehlungen für das VCC-SystemDie erfolgreiche Implementierung des Deep-Research-Systems hängt von seiner nahtlosen Integration in die bestehende Anwendungslandschaft des VCC-Systems ab. Da die spezifischen Implementierungsdetails von Veritas, Covina und Clara nicht verfügbar sind, wird ein generischer, auf service-orientierter Architektur (SOA) basierender Integrationsansatz vorgeschlagen.8.1. Anbindung an die bestehende VCC-ArchitekturVeritas (Benutzeroberfläche): Veritas fungiert als primäres Frontend für den Benutzer. Es interagiert mit dem Deep-Research-System über die definierte API des FastAPI-Servers. Ein Benutzer würde seine Forschungsanfrage in Veritas eingeben, welches daraufhin einen Aufruf an den /start-research-Endpunkt sendet. Veritas kann den Fortschritt der Recherche periodisch über den /get-status/{research_id}-Endpunkt abfragen und die finalen Ergebnisse, die aus dem results_summary-Feld des JSON-Zustandsobjekts extrahiert werden, in einer benutzerfreundlichen Weise visualisieren.Covina (Vorverarbeitung): Covina kann als vorgeschalteter Datenverarbeitungsdienst agieren. Anstatt dass die rohe Benutzeranfrage direkt an das Deep-Research-System geht, könnte sie zuerst an Covina gesendet werden. Covina könnte Aufgaben wie die Bereinigung der Anfrage, die Extraktion von Schlüsselentitäten (NER) und die Anreicherung mit internem Kontext durchführen, bevor es die Anfrage in einem strukturierten Format an das Deep-Research-System übergibt. Dies würde die Effizienz und Genauigkeit des Planungs-Agenten erhöhen.Clara (Lernen & Feedback): Clara, als die lernende Komponente des VCC, ist der ideale Empfänger für die Audit-Daten, die vom Deep-Research-System generiert werden. Nach Abschluss jeder Recherche kann der vollständige execution_trace und die darin enthaltenen evaluation-Daten aus dem JSON-Objekt an Clara übermittelt werden. Clara kann diese Daten nutzen, um die Leistung des Systems über die Zeit zu analysieren, häufige Muster von ineffizienten Forschungspfaden zu identifizieren und Vorschläge zur automatischen Verbesserung der Prompts für den Planungs- und Evaluator-Agenten zu generieren. Dies schließt den Lernkreis und realisiert den selbstverbessernden Aspekt des Gesamtsystems.8.2. Zusammenfassung des empfohlenen TechnologiestacksMacro-Orchestrierung: PrefectMicro-Orchestrierung: LangGraphAPI-Layer: FastAPIContainer-Orchestrierung: Kubernetes, DockerZustandspersistenz: PostgreSQLDatenquellen-Tools: LangChain (insb. GraphCypherQAChain), ChromaDB (On-Premise), SearxNGEvaluierungs-Framework: LLM-as-a-Judge basierend auf RAG-Triade-Metriken8.3. Roadmap für die ImplementierungEine gestaffelte Vorgehensweise wird empfohlen, um Risiken zu minimieren und eine iterative Wertschöpfung zu ermöglichen.Phase 1: Proof-of-Concept (4-6 Wochen):Ziel: Validierung der Kernlogik.Umfang: Aufbau eines minimalen LangGraph-Workflows auf einem lokalen Entwicklerrechner. Implementierung von zwei Agenten (z.B. Web-Suche mit SearxNG und Zusammenfassung) und der iterativen Schleife mit einem einfachen Evaluator-Agenten.Ergebnis: Ein funktionaler Prototyp, der die Machbarkeit des Konzepts der selbstkorrigierenden Schleife beweist.Phase 2: Prototyp im Staging-Umfeld (8-12 Wochen):Ziel: Validierung der technischen Architektur und Persistenz.Umfang: Deployment des Systems auf einem Kubernetes-Testcluster. Integration von PostgreSQL als Checkpointer für die Zustandspersistenz. Anbindung von 2-3 realen, aber unkritischen Datenquellen (z.B. SearxNG und eine Test-SQL-Datenbank). Entwicklung der ersten Version des progressiven JSON-Frameworks.Ergebnis: Ein stabiler, containerisierter Prototyp, der langlebige, wiederaufsetzbare Recherchen durchführen kann.Phase 3: Minimum Viable Product (MVP) für Pilotanwender (3-4 Monate):Ziel: Erbringung eines ersten Geschäftsnutzens und Sammeln von Feedback.Umfang: Erweiterung des Agenten-Ökosystems um geschäftskritische Agenten (z.B. Neo4j-Agent). Härtung der Kubernetes-Deployments (Security-Richtlinien, Monitoring, Logging). Erste Integration mit der Veritas-Benutzeroberfläche, um ausgewählten Pilotanwendern den Zugriff zu ermöglichen.Ergebnis: Ein nutzbares System, das erste wertvolle Forschungsergebnisse liefert und dessen Leistung im realen Einsatz gemessen werden kann.Phase 4: Produktionsreife und Skalierung:Ziel: unternehmensweiter Rollout und kontinuierliche Verbesserung.Umfang: Optimierung der Performance und Skalierbarkeit des Systems. Fine-Tuning der LLMs (Planner und Evaluator) basierend auf den gesammelten Nutzungsdaten aus der Pilotphase. Vollständige und robuste Integration in die gesamte VCC-Systemlandschaft (Veritas, Covina, Clara).Ergebnis: Ein vollständig integriertes, skalierbares und lernendes Deep-Research-System, das einen signifikanten strategischen Vorteil für das Unternehmen darstellt.
