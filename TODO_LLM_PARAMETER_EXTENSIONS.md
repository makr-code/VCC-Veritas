# ğŸ“‹ TODO: LLM-Parameter UI Erweiterungen (Optional)

**Projekt:** VERITAS RAG System  
**Version:** v3.18.2 (geplant)  
**Erstellt:** 10.10.2025  
**PrioritÃ¤t:** NICE-TO-HAVE (alle Features bereits funktional)

---

## âœ… Abgeschlossen (v3.18.1)

- [x] Max Tokens Spinbox (100-2000)
- [x] Top-p Slider (0.0-1.0)
- [x] Erweiterte Tooltips mit Multi-line Support
- [x] Klickbare Hilfe-Links in Tooltips
- [x] `_open_help_docs()` Methode (plattformÃ¼bergreifend)
- [x] VollstÃ¤ndige Dokumentation (LLM_PARAMETERS.md)
- [x] Parameter-Flow Frontend â†’ Backend â†’ Ollama âœ…

---

## ğŸ¯ Optional Features (Priorisiert)

### ğŸ¥‡ HIGH PRIORITY

#### 1. Preset-Buttons fÃ¼r schnelle Konfiguration
**GeschÃ¤tzter Aufwand:** 30-45 Minuten  
**Nutzen:** â­â­â­â­â­ (Sehr hoch - User-Convenience)

**Beschreibung:**
4 Preset-Buttons zum schnellen Umschalten zwischen typischen Konfigurationen.

**UI-Mockup:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Presets: [âš–ï¸ PrÃ¤zise] [âœ… Standard] [ğŸ“– AusfÃ¼hrlich] [ğŸ¨ Kreativ] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Parameter-Sets:**

| Preset | Temp | Tokens | Top-p | Use Case |
|--------|------|--------|-------|----------|
| **âš–ï¸ PrÃ¤zise** | 0.3 | 300 | 0.7 | Gesetze, Fakten |
| **âœ… Standard** | 0.7 | 500 | 0.9 | Verwaltungsfragen |
| **ğŸ“– AusfÃ¼hrlich** | 0.6 | 1000 | 0.85 | Analysen |
| **ğŸ¨ Kreativ** | 0.9 | 600 | 0.95 | Umformulierungen |

**Implementation:**

```python
# In veritas_app.py, _create_settings_controls()

def _create_preset_buttons(self, parent):
    """Erstellt Preset-Buttons"""
    preset_frame = ttk.Frame(parent)
    preset_frame.pack(fill=tk.X, padx=5, pady=3)
    
    ttk.Label(preset_frame, text="Presets:", font=('Segoe UI', 8)).pack(side=tk.LEFT, padx=(0, 5))
    
    presets = [
        ("âš–ï¸ PrÃ¤zise", 0.3, 300, 0.7, "Fakten, Gesetze"),
        ("âœ… Standard", 0.7, 500, 0.9, "Verwaltungsfragen"),
        ("ğŸ“– AusfÃ¼hrlich", 0.6, 1000, 0.85, "Detaillierte Analysen"),
        ("ğŸ¨ Kreativ", 0.9, 600, 0.95, "Alternative Formulierungen")
    ]
    
    for label, temp, tokens, topp, tooltip_text in presets:
        btn = ttk.Button(
            preset_frame,
            text=label,
            command=lambda t=temp, tk=tokens, p=topp: self._apply_preset(t, tk, p),
            width=12
        )
        btn.pack(side=tk.LEFT, padx=2)
        
        if UI_COMPONENTS_AVAILABLE:
            Tooltip(btn, f"{label}\n\n{tooltip_text}\n\nTemp: {temp} | Tokens: {tokens} | Top-p: {p}")

def _apply_preset(self, temperature, max_tokens, top_p):
    """Wendet Preset an"""
    self.temperature_var.set(temperature)
    self.max_tokens_var.set(max_tokens)
    self.top_p_var.set(top_p)
    
    # Labels aktualisieren
    self._update_temperature_label(temperature)
    self._update_topp_label(top_p)
    
    # System-Message im Chat
    self.add_system_message(
        f"ğŸ›ï¸ Preset angewandt: Temp={temperature}, Tokens={max_tokens}, Top-p={top_p}"
    )
    
    logger.info(f"Preset angewandt: T={temperature}, Tokens={max_tokens}, p={top_p}")
```

**Datei:** `frontend/veritas_app.py`  
**Zeilen:** ~1310 (nach top_p Slider)

**Testing:**
- [ ] Buttons rendern korrekt
- [ ] Preset-Anwendung funktioniert
- [ ] Labels aktualisieren sich
- [ ] System-Message erscheint
- [ ] Tooltips zeigen Details

---

#### 2. Token-Counter & AntwortlÃ¤ngen-SchÃ¤tzung
**GeschÃ¤tzter Aufwand:** 45-60 Minuten  
**Nutzen:** â­â­â­â­ (Hoch - Transparenz)

**Beschreibung:**
Zeigt geschÃ¤tzte AntwortlÃ¤nge und Token-Verbrauch in Echtzeit.

**UI-Mockup:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“ Max Tokens: [500 â–²â–¼] tok                                â”‚
â”‚  ğŸ’¬ GeschÃ¤tzte AntwortlÃ¤nge: ~375 WÃ¶rter (~1500 Zeichen)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation:**

```python
# In veritas_app.py

def _create_token_counter_label(self, parent):
    """Erstellt Token-Counter Label"""
    self.token_info_label = ttk.Label(
        parent,
        text="ğŸ’¬ GeschÃ¤tzte AntwortlÃ¤nge: ~375 WÃ¶rter",
        font=('Segoe UI', 8),
        foreground='#666666'
    )
    self.token_info_label.pack(side=tk.LEFT, padx=(10, 0))

def _update_tokens_label(self, *args):
    """Aktualisiert Token-Counter bei Spinbox-Ã„nderung"""
    try:
        tokens = self.max_tokens_var.get()
        
        # Token â†’ WÃ¶rter (Deutsch: 1 Token â‰ˆ 0.75 WÃ¶rter)
        estimated_words = int(tokens * 0.75)
        
        # Token â†’ Zeichen (Deutsch: 1 Token â‰ˆ 4 Zeichen)
        estimated_chars = int(tokens * 4)
        
        # Label aktualisieren
        if hasattr(self, 'token_info_label'):
            self.token_info_label.config(
                text=f"ğŸ’¬ GeschÃ¤tzte AntwortlÃ¤nge: ~{estimated_words} WÃ¶rter (~{estimated_chars} Zeichen)"
            )
    except Exception as e:
        logger.error(f"Fehler beim Aktualisieren des Token-Counters: {e}")
```

**Datei:** `frontend/veritas_app.py`  
**Zeilen:** ~1245 (nach max_tokens_spinbox)

**Erweitert:**
- [ ] Token â†’ WÃ¶rter Konversion (0.75 Faktor)
- [ ] Token â†’ Zeichen Konversion (4 Faktor)
- [ ] Live-Update bei Spinbox-Ã„nderung
- [ ] Farbcodierung (GrÃ¼n <500, Orange 500-1000, Rot >1000)

---

#### 3. Antwortzeit-PrÃ¤diktion
**GeschÃ¤tzter Aufwand:** 30-45 Minuten  
**Nutzen:** â­â­â­ (Mittel - Erwartungsmanagement)

**Beschreibung:**
SchÃ¤tzt Antwortzeit basierend auf max_tokens und Modell.

**UI-Mockup:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â±ï¸ GeschÃ¤tzte Antwortzeit: ~3-5 Sekunden                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation:**

```python
# In veritas_app.py

# Benchmark-Daten (gemessen auf lokalem System)
MODEL_BENCHMARKS = {
    'llama3:latest': 150,      # Tokens/Sekunde
    'llama3.1:8b': 120,        # Tokens/Sekunde
    'phi3:latest': 200,        # Tokens/Sekunde (schneller, kleiner)
    'mixtral:latest': 80,      # Tokens/Sekunde (langsamer, grÃ¶ÃŸer)
    'codellama:latest': 130    # Tokens/Sekunde
}

def _estimate_response_time(self):
    """SchÃ¤tzt Antwortzeit"""
    try:
        max_tokens = self.max_tokens_var.get()
        model = self.llm_var.get()
        
        # Tokens/Sekunde fÃ¼r aktuelles Modell
        tokens_per_second = MODEL_BENCHMARKS.get(model, 120)  # Default: 120
        
        # Base-Zeit (Generation)
        generation_time = max_tokens / tokens_per_second
        
        # Overhead (RAG, Agents, etc.)
        overhead = 1.5  # Sekunden
        
        # Gesamt
        total_time = generation_time + overhead
        
        # Range (Â±20%)
        min_time = total_time * 0.8
        max_time = total_time * 1.2
        
        return min_time, max_time
    except Exception as e:
        logger.error(f"Fehler bei Antwortzeit-SchÃ¤tzung: {e}")
        return 2.0, 6.0  # Fallback

def _update_response_time_estimate(self):
    """Aktualisiert Antwortzeit-SchÃ¤tzung"""
    if hasattr(self, 'response_time_label'):
        min_time, max_time = self._estimate_response_time()
        self.response_time_label.config(
            text=f"â±ï¸ GeschÃ¤tzte Antwortzeit: ~{min_time:.0f}-{max_time:.0f} Sekunden"
        )
```

**Datei:** `frontend/veritas_app.py`  
**Zeilen:** ~1260 (nach Token-Counter)

**Features:**
- [ ] Modell-spezifische Benchmarks
- [ ] Token-basierte Berechnung
- [ ] Overhead fÃ¼r RAG/Agents
- [ ] Â±20% Range
- [ ] Live-Update bei Parameter-Ã„nderung

---

### ğŸ¥ˆ MEDIUM PRIORITY

#### 4. Parameter-History & Gespeicherte Presets
**GeschÃ¤tzter Aufwand:** 60-90 Minuten  
**Nutzen:** â­â­â­ (Mittel - Power-User-Feature)

**Beschreibung:**
Speichert die letzten 5 Parameter-Kombinationen + benutzerdefinierte Presets.

**UI-Mockup:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“œ History: [Letzte 5 â–¼]  ğŸ’¾ [Speichern...]  ğŸ—‘ï¸ [LÃ¶schen]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Features:**
- Dropdown mit letzten 5 Einstellungen
- "Speichern als Preset" Dialog (Name + Beschreibung)
- JSON-Persistierung in `config/user_presets.json`
- "LÃ¶schen" Button fÃ¼r gespeicherte Presets

**Datei:** `frontend/veritas_app.py` + `config/user_presets.json`

**Implementation:**
```python
def _save_parameter_history(self):
    """Speichert aktuelle Parameter in History"""
    current_params = {
        'temperature': self.temperature_var.get(),
        'max_tokens': self.max_tokens_var.get(),
        'top_p': self.top_p_var.get(),
        'timestamp': datetime.now().isoformat()
    }
    
    # Lade History
    history_file = 'config/parameter_history.json'
    history = self._load_json(history_file, default=[])
    
    # FÃ¼ge hinzu (max 5)
    history.insert(0, current_params)
    history = history[:5]
    
    # Speichere
    self._save_json(history_file, history)

def _load_parameter_preset(self, preset_name):
    """LÃ¤dt gespeichertes Preset"""
    presets_file = 'config/user_presets.json'
    presets = self._load_json(presets_file, default={})
    
    if preset_name in presets:
        preset = presets[preset_name]
        self.temperature_var.set(preset['temperature'])
        self.max_tokens_var.set(preset['max_tokens'])
        self.top_p_var.set(preset['top_p'])
```

---

#### 5. Visual Feedback & Farbcodierung
**GeschÃ¤tzter Aufwand:** 30-45 Minuten  
**Nutzen:** â­â­â­ (Mittel - UX-Verbesserung)

**Beschreibung:**
Farbliche Kennzeichnung der Parameter-Werte (GrÃ¼n=OK, Orange=Hoch, Rot=Sehr hoch).

**UI-Mockup:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸŒ¡ï¸ [â”â”â”â”â”â”â”] 0.7  â† GrÃ¼n (Standard)                       â”‚
â”‚  ğŸ“ [800 â–²â–¼] tok   â† Orange (Hoch)                          â”‚
â”‚  ğŸ² [â”â”â”â”â”â”â”] 0.95 â† Orange (Sehr hoch)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Color-Coding:**

| Parameter | GrÃ¼n | Orange | Rot |
|-----------|------|--------|-----|
| Temperature | 0.3-0.8 | 0.8-0.95 | >0.95 |
| Max Tokens | <600 | 600-1200 | >1200 |
| Top-p | 0.7-0.95 | 0.5-0.7, 0.95-1.0 | <0.5 |

**Implementation:**
```python
def _update_temperature_label(self, value):
    """Aktualisiert Temperatur-Label mit Farbcodierung"""
    temp_value = float(value)
    
    # Farbcodierung
    if temp_value <= 0.8:
        color = '#00AA00'  # GrÃ¼n
    elif temp_value <= 0.95:
        color = '#FF8800'  # Orange
    else:
        color = '#DD0000'  # Rot
    
    self.temp_label.config(text=f"{temp_value:.1f}", foreground=color)
```

---

### ğŸ¥‰ LOW PRIORITY (Nice-to-Have)

#### 6. A/B Testing Split-View
**GeschÃ¤tzter Aufwand:** 120-180 Minuten  
**Nutzen:** â­â­ (Niedrig - Expertenfeature)

**Beschreibung:**
Sendet gleiche Query mit 2 verschiedenen Parameter-Sets parallel und zeigt Ergebnisse nebeneinander.

**UI-Mockup:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query: "Was brauche ich fÃ¼r eine Baugenehmigung?"        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CONFIG A (PrÃ¤zise)      â”‚  CONFIG B (AusfÃ¼hrlich)         â”‚
â”‚  Temp: 0.3               â”‚  Temp: 0.6                      â”‚
â”‚  Tokens: 300             â”‚  Tokens: 1000                   â”‚
â”‚  Top-p: 0.7              â”‚  Top-p: 0.85                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Antwort A:              â”‚  Antwort B:                     â”‚
â”‚  [Kurze Antwort...]      â”‚  [AusfÃ¼hrliche Antwort...]      â”‚
â”‚                          â”‚                                 â”‚
â”‚  â±ï¸ 2.3s | ğŸ¯ 0.85       â”‚  â±ï¸ 5.1s | ğŸ¯ 0.92              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Features:**
- Parallel-Anfragen (asyncio)
- Side-by-Side Vergleich
- Metriken (Zeit, Confidence, LÃ¤nge)
- "Besser" Bewertung durch User

---

#### 7. Parameter-Analytics Dashboard
**GeschÃ¤tzter Aufwand:** 90-120 Minuten  
**Nutzen:** â­â­ (Niedrig - Statistik fÃ¼r Power-User)

**Beschreibung:**
Statistiken Ã¼ber verwendete Parameter-Kombinationen.

**Features:**
- HÃ¤ufigste Einstellungen
- Durchschnittliche Antwortzeiten pro Config
- Confidence-Scores pro Preset
- Charts (matplotlib/plotly)

**Beispiel:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“Š Parameter Analytics (30 Tage)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  HÃ¤ufigste Presets:                    â”‚
â”‚  âœ… Standard: 65%                      â”‚
â”‚  ğŸ“– AusfÃ¼hrlich: 20%                   â”‚
â”‚  âš–ï¸ PrÃ¤zise: 10%                       â”‚
â”‚  ğŸ¨ Kreativ: 5%                        â”‚
â”‚                                        â”‚
â”‚  Durchschnittliche Confidence:         â”‚
â”‚  âœ… Standard: 0.87                     â”‚
â”‚  ğŸ“– AusfÃ¼hrlich: 0.91                  â”‚
â”‚  âš–ï¸ PrÃ¤zise: 0.82                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“… Implementierungs-Roadmap

### Sprint 1 (2-3h)
- [x] **Woche 1:** Preset-Buttons (HIGH-1)
- [x] **Woche 1:** Token-Counter (HIGH-2)
- [x] **Woche 1:** Antwortzeit-PrÃ¤diktion (HIGH-3)

### Sprint 2 (3-4h)
- [ ] **Woche 2:** Parameter-History (MEDIUM-4)
- [ ] **Woche 2:** Visual Feedback (MEDIUM-5)

### Sprint 3 (Optional, 5-6h)
- [ ] **Woche 3+:** A/B Testing (LOW-6)
- [ ] **Woche 3+:** Analytics Dashboard (LOW-7)

---

## ğŸ¯ Akzeptanzkriterien

### Preset-Buttons:
- [ ] 4 Buttons rendern korrekt
- [ ] Klick wendet Parameter an
- [ ] System-Message erscheint
- [ ] Labels aktualisieren sich in Echtzeit
- [ ] Tooltips zeigen Details

### Token-Counter:
- [ ] Zeigt WÃ¶rter + Zeichen
- [ ] Aktualisiert bei Spinbox-Ã„nderung
- [ ] Farbcodierung (GrÃ¼n/Orange/Rot)
- [ ] Platzierung passt ins Layout

### Antwortzeit-PrÃ¤diktion:
- [ ] Modell-spezifische Benchmarks
- [ ] Range (Min-Max) angezeigt
- [ ] Aktualisiert bei Parameter-Ã„nderung
- [ ] Realistisch (Â±20% Abweichung OK)

---

## ğŸ“Š Erfolgsmetriken

**Nach Sprint 1:**
- âœ… User kÃ¶nnen mit 1 Klick zwischen Presets wechseln
- âœ… User sehen geschÃ¤tzte AntwortlÃ¤nge vor dem Senden
- âœ… User kennen erwartete Antwortzeit

**Nach Sprint 2:**
- âœ… User kÃ¶nnen eigene Presets speichern
- âœ… Farbcodierung hilft bei Parameter-Wahl

**Nach Sprint 3 (Optional):**
- âœ… Experten kÃ¶nnen A/B Tests durchfÃ¼hren
- âœ… Statistiken zeigen optimale Parameter-Kombinationen

---

## ğŸ”— Verwandte Dokumentation

- âœ… `docs/LLM_PARAMETERS.md` - VollstÃ¤ndige Parameter-Referenz
- âœ… `docs/LLM_PARAMETER_UI_CONTROLS.md` - Implementation v3.18.1
- âœ… `docs/LLM_PARAMETER_FLOW_VERIFICATION.md` - Flow-Verifizierung
- â³ `docs/PRESET_SYSTEM.md` - Preset-System Docs (zu erstellen)
- â³ `docs/AB_TESTING_GUIDE.md` - A/B Testing Guide (zu erstellen)

---

## ğŸš€ Quick Start (fÃ¼r Entwickler)

### Preset-Buttons implementieren:

1. **Datei Ã¶ffnen:** `frontend/veritas_app.py`
2. **Nach Zeile 1310 einfÃ¼gen:** `_create_preset_buttons()`
3. **Methoden hinzufÃ¼gen:** `_apply_preset()`
4. **Testen:** Hover + Klick

### Token-Counter implementieren:

1. **Datei Ã¶ffnen:** `frontend/veritas_app.py`
2. **Nach Zeile 1245 einfÃ¼gen:** `_create_token_counter_label()`
3. **Methode erweitern:** `_update_tokens_label()`
4. **Testen:** Spinbox Ã¤ndern â†’ Label updated

---

## âœ… Definition of Done

Eine Feature gilt als **DONE** wenn:

- [ ] Code implementiert & kommentiert
- [ ] UI rendert korrekt (Windows + Linux getestet)
- [ ] Tooltips vorhanden & informativ
- [ ] FunktionalitÃ¤t getestet (min. 3 Szenarien)
- [ ] Dokumentation aktualisiert
- [ ] Keine Errors im Log
- [ ] Performance OK (<100ms UI-Update)
- [ ] Code-Review durch 2. Entwickler

---

**Autor:** VERITAS System  
**Version:** 1.0  
**Erstellt:** 10.10.2025  
**Status:** OPEN (0/7 Features)  
**NÃ¤chste Review:** 17.10.2025
