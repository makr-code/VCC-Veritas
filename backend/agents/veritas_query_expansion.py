#!/usr/bin/env python3
"""
VERITAS QUERY EXPANSION SERVICE
================================

LLM-basierte Query-Expansion f√ºr verbesserte Retrieval-Recall.
Generiert semantische Varianten einer Suchanfrage via Ollama.

Query Expansion Strategien:
---------------------------
1. **Synonym Expansion**: "nachhaltig" ‚Üí "umweltfreundlich", "√∂kologisch", "gr√ºn"
2. **Context Expansion**: "Haus bauen" ‚Üí "Wie baue ich ein Haus nach DIN-Normen?"
3. **Multi-Perspective**: Rechtlich, Technisch, Prozessual

Vorteile:
---------
- H√∂here Recall-Rate (mehr relevante Dokumente gefunden)
- Robustheit gegen Query-Formulierung
- Bessere Abdeckung von Synonymen & Paraphrasen

Beispiel:
--------
Original: "Wie baue ich ein barrierefreies Haus?"

Expanded:
- "Welche DIN-Normen gelten f√ºr barrierefreies Bauen?"
- "Anforderungen an behindertengerechte Wohngeb√§ude"
- "Barrierefreiheit Neubau ¬ß 39 BauO NRW"

Author: VERITAS System
Date: 2025-10-06
Version: 1.0
"""

from __future__ import annotations

import asyncio
import logging
import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)

# Ollama Import
try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False
    logger.warning("‚ö†Ô∏è httpx nicht verf√ºgbar - Query Expansion arbeitet ohne Ollama")


class ExpansionStrategy(Enum):
    """Query Expansion Strategien."""
    
    SYNONYM = "synonym"  # Synonyme & √§hnliche Begriffe
    CONTEXT = "context"  # Kontextuelle Umformulierung
    MULTI_PERSPECTIVE = "multi_perspective"  # Verschiedene Perspektiven
    TECHNICAL = "technical"  # Technische/Fachliche Formulierung
    SIMPLE = "simple"  # Vereinfachte Formulierung


@dataclass
class QueryExpansionConfig:
    """Konfiguration f√ºr Query Expansion."""
    
    # LLM-Parameter
    model: str = "phi3:latest"  # Ollama Model (Best: phi3:latest 1654ms, llama3.2:latest 1185ms)
    temperature: float = 0.7  # Kreativit√§t (0.0-1.0)
    max_tokens: int = 256  # Max Tokens pro Expansion
    
    # Expansion-Parameter
    num_expansions: int = 2  # Anzahl generierte Varianten
    strategies: List[ExpansionStrategy] = field(
        default_factory=lambda: [
            ExpansionStrategy.SYNONYM,
            ExpansionStrategy.CONTEXT
        ]
    )
    
    # Ollama-Verbindung
    ollama_base_url: str = "http://localhost:11434"
    timeout: float = 10.0  # Timeout in Sekunden
    
    # Caching
    enable_cache: bool = True
    cache_ttl: int = 3600  # Cache TTL in Sekunden
    
    # Fallback
    fallback_to_original: bool = True  # Original-Query als Fallback


@dataclass
class ExpandedQuery:
    """Eine expandierte Query-Variante."""
    
    text: str
    strategy: ExpansionStrategy
    confidence: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)


class QueryExpander:
    """
    LLM-basierte Query-Expansion via Ollama.
    
    Funktionsweise:
    --------------
    1. Original-Query analysieren
    2. LLM-Prompt f√ºr Expansion-Strategie erstellen
    3. Ollama API aufrufen
    4. Varianten extrahieren & validieren
    5. Duplikate entfernen
    
    Performance:
    -----------
    - LLM-Latenz: ~500-2000ms (abh√§ngig von Model-Gr√∂√üe)
    - Cache-Hit: ~1ms
    - Fallback (ohne Ollama): ~0ms (original query)
    
    Beispiel:
    --------
    expander = QueryExpander()
    results = await expander.expand(
        "Wie baue ich ein barrierefreies Haus?",
        num_expansions=2
    )
    # results = [
    #   ExpandedQuery(text="Welche DIN-Normen gelten f√ºr barrierefreies Bauen?", ...),
    #   ExpandedQuery(text="Anforderungen behindertengerechtes Wohnen", ...)
    # ]
    """
    
    def __init__(self, config: Optional[QueryExpansionConfig] = None):
        """Initialisiert Query Expander.
        
        Args:
            config: Expansion-Konfiguration (optional)
        """
        self.config = config or QueryExpansionConfig()
        self._cache: Dict[str, List[ExpandedQuery]] = {}
        self._ollama_available = HTTPX_AVAILABLE
        
        if not self._ollama_available:
            logger.warning(
                "‚ö†Ô∏è Query Expansion l√§uft ohne Ollama (httpx fehlt) - "
                "Fallback auf Original-Query"
            )
    
    async def expand(
        self,
        query: str,
        num_expansions: Optional[int] = None,
        strategies: Optional[List[ExpansionStrategy]] = None
    ) -> List[ExpandedQuery]:
        """
        Expandiert Query in semantische Varianten.
        
        Args:
            query: Original-Suchanfrage
            num_expansions: Anzahl Varianten (default: config.num_expansions)
            strategies: Expansion-Strategien (default: config.strategies)
            
        Returns:
            Liste von ExpandedQuery-Objekten (inkl. Original)
        """
        num_expansions = num_expansions or self.config.num_expansions
        strategies = strategies or self.config.strategies
        
        # Cache-Check
        cache_key = self._get_cache_key(query, num_expansions, strategies)
        if self.config.enable_cache and cache_key in self._cache:
            logger.debug(f"üíæ Query Expansion Cache-Hit: '{query[:50]}...'")
            return self._cache[cache_key]
        
        start_time = time.time()
        
        # Original-Query immer inkludieren
        expanded_queries = [
            ExpandedQuery(
                text=query,
                strategy=ExpansionStrategy.SYNONYM,  # Placeholder
                confidence=1.0,
                metadata={"source": "original"}
            )
        ]
        
        # LLM-Expansion nur wenn Ollama verf√ºgbar
        if self._ollama_available:
            try:
                # F√ºr jede Strategie Varianten generieren
                for strategy in strategies[:num_expansions]:
                    variant = await self._expand_with_strategy(query, strategy)
                    if variant and variant.text.strip() and variant.text != query:
                        # Duplikat-Check
                        if not any(eq.text.lower() == variant.text.lower() for eq in expanded_queries):
                            expanded_queries.append(variant)
                    
                    # Limit erreicht?
                    if len(expanded_queries) >= num_expansions + 1:  # +1 f√ºr Original
                        break
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Query Expansion fehlgeschlagen: {e}")
                if not self.config.fallback_to_original:
                    raise
        
        # Cache speichern
        if self.config.enable_cache:
            self._cache[cache_key] = expanded_queries
        
        duration = (time.time() - start_time) * 1000
        logger.info(
            f"üîç Query Expansion: '{query[:50]}...' ‚Üí {len(expanded_queries)} Varianten ({duration:.0f}ms)"
        )
        
        return expanded_queries
    
    async def _expand_with_strategy(
        self,
        query: str,
        strategy: ExpansionStrategy
    ) -> Optional[ExpandedQuery]:
        """Generiert Query-Variante mit spezifischer Strategie."""
        
        # Prompt basierend auf Strategie
        prompt = self._build_prompt(query, strategy)
        
        # Ollama API aufrufen
        try:
            variant_text = await self._call_ollama(prompt)
            
            return ExpandedQuery(
                text=variant_text,
                strategy=strategy,
                confidence=0.85,  # LLM-generiert
                metadata={"source": "ollama", "model": self.config.model}
            )
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ollama-Call fehlgeschlagen f√ºr {strategy.value}: {e}")
            return None
    
    def _build_prompt(self, query: str, strategy: ExpansionStrategy) -> str:
        """Erstellt Prompt f√ºr Query Expansion."""
        
        base_instruction = (
            "Du bist ein Experte f√ºr Baurecht, Umweltrecht und technische Normen. "
            "Deine Aufgabe ist es, Suchanfragen umzuformulieren.\n\n"
        )
        
        if strategy == ExpansionStrategy.SYNONYM:
            instruction = (
                "Formuliere die folgende Suchanfrage mit SYNONYMEN und √§hnlichen Begriffen um. "
                "Behalte die gleiche Bedeutung bei, verwende aber andere W√∂rter.\n\n"
                f"Original: {query}\n"
                "Umformulierung mit Synonymen:"
            )
        
        elif strategy == ExpansionStrategy.CONTEXT:
            instruction = (
                "Formuliere die folgende Suchanfrage KONTEXTUELL um. "
                "F√ºge relevante rechtliche oder technische Details hinzu.\n\n"
                f"Original: {query}\n"
                "Kontextuelle Umformulierung:"
            )
        
        elif strategy == ExpansionStrategy.MULTI_PERSPECTIVE:
            instruction = (
                "Formuliere die folgende Suchanfrage aus einer ANDEREN PERSPEKTIVE um "
                "(z.B. rechtlich, technisch, oder prozessual).\n\n"
                f"Original: {query}\n"
                "Alternative Perspektive:"
            )
        
        elif strategy == ExpansionStrategy.TECHNICAL:
            instruction = (
                "Formuliere die folgende Suchanfrage TECHNISCH/FACHLICH um. "
                "Verwende Fachbegriffe, Normen, Paragraphen falls relevant.\n\n"
                f"Original: {query}\n"
                "Technische Formulierung:"
            )
        
        elif strategy == ExpansionStrategy.SIMPLE:
            instruction = (
                "Formuliere die folgende Suchanfrage EINFACHER und allgemeinverst√§ndlicher um.\n\n"
                f"Original: {query}\n"
                "Einfache Formulierung:"
            )
        
        else:
            instruction = f"Original: {query}\nUmformulierung:"
        
        return base_instruction + instruction
    
    async def _call_ollama(self, prompt: str) -> str:
        """Ruft Ollama API auf und extrahiert Response."""
        
        if not HTTPX_AVAILABLE:
            raise RuntimeError("httpx nicht verf√ºgbar - kann Ollama nicht aufrufen")
        
        url = f"{self.config.ollama_base_url}/api/generate"
        
        payload = {
            "model": self.config.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": self.config.temperature,
                "num_predict": self.config.max_tokens,
                "stop": ["\n\n", "Original:", "Umformulierung:"]
            }
        }
        
        async with httpx.AsyncClient(timeout=self.config.timeout) as client:
            response = await client.post(url, json=payload)
            response.raise_for_status()
            
            result = response.json()
            variant = result.get("response", "").strip()
            
            # Cleanup: Entferne Anf√ºhrungszeichen, Pr√§fixe, etc.
            variant = self._cleanup_variant(variant)
            
            return variant
    
    def _cleanup_variant(self, text: str) -> str:
        """Bereinigt LLM-Output."""
        
        # Entferne Anf√ºhrungszeichen
        text = text.strip('"\'')
        
        # Entferne h√§ufige Pr√§fixe
        prefixes = [
            "Umformulierung:",
            "Variante:",
            "Alternative:",
            "Synonym:",
            "Technisch:",
            "Einfach:",
        ]
        for prefix in prefixes:
            if text.startswith(prefix):
                text = text[len(prefix):].strip()
        
        # Entferne Zeilenumbr√ºche
        text = text.replace("\n", " ").strip()
        
        # Nur erste Zeile wenn mehrere
        if "." in text:
            sentences = text.split(".")
            # Nehme ersten vollst√§ndigen Satz
            text = sentences[0].strip() + "."
        
        return text
    
    def _get_cache_key(
        self,
        query: str,
        num_expansions: int,
        strategies: List[ExpansionStrategy]
    ) -> str:
        """Erstellt Cache-Key."""
        strategy_str = "_".join(s.value for s in strategies)
        return f"{query}_{num_expansions}_{strategy_str}"
    
    def get_stats(self) -> Dict[str, Any]:
        """Gibt Expansion-Statistiken zur√ºck."""
        return {
            "ollama_available": self._ollama_available,
            "cache_size": len(self._cache),
            "config": {
                "model": self.config.model,
                "num_expansions": self.config.num_expansions,
                "temperature": self.config.temperature,
                "strategies": [s.value for s in self.config.strategies]
            }
        }
    
    def clear_cache(self) -> None:
        """Leert Query-Cache."""
        self._cache.clear()
        logger.info("üóëÔ∏è Query Expansion Cache geleert")


# Singleton-Pattern
_query_expander_instance: Optional[QueryExpander] = None


def get_query_expander(config: Optional[QueryExpansionConfig] = None) -> QueryExpander:
    """Gibt Singleton Query Expander zur√ºck."""
    global _query_expander_instance
    
    if _query_expander_instance is None:
        _query_expander_instance = QueryExpander(config)
    
    return _query_expander_instance


# Multi-Query Generator f√ºr verschiedene Perspektiven
class MultiQueryGenerator:
    """
    Generiert Queries aus verschiedenen Perspektiven.
    
    Perspektiven:
    ------------
    - Rechtlich: Paragraphen, Gesetze, Vorschriften
    - Technisch: Normen, Standards, Spezifikationen
    - Prozessual: Abl√§ufe, Verfahren, Genehmigungen
    
    Beispiel:
    --------
    Original: "Wie baue ich ein Haus?"
    
    Rechtlich: "Welche baurechtlichen Vorschriften gelten f√ºr Wohngeb√§ude?"
    Technisch: "Welche DIN-Normen sind beim Hausbau zu beachten?"
    Prozessual: "Welche Genehmigungsverfahren durchlaufe ich beim Hausbau?"
    """
    
    def __init__(self, query_expander: Optional[QueryExpander] = None):
        """Initialisiert Multi-Query Generator.
        
        Args:
            query_expander: Query Expander Instanz (optional)
        """
        self.expander = query_expander or get_query_expander()
    
    async def generate_multi_perspective(
        self,
        query: str,
        perspectives: Optional[List[str]] = None
    ) -> Dict[str, str]:
        """
        Generiert Queries aus verschiedenen Perspektiven.
        
        Args:
            query: Original-Suchanfrage
            perspectives: Liste von Perspektiven (default: rechtlich, technisch, prozessual)
            
        Returns:
            Dict mit Perspektive ‚Üí Query Mapping
        """
        perspectives = perspectives or ["rechtlich", "technisch", "prozessual"]
        
        multi_queries = {}
        
        for perspective in perspectives:
            # Custom-Prompt f√ºr Perspektive
            prompt = (
                f"Du bist Experte f√ºr Baurecht und technische Normen. "
                f"Formuliere die folgende Suchanfrage aus {perspective.upper()}ER Sicht um:\n\n"
                f"Original: {query}\n"
                f"{perspective.capitalize()}e Formulierung:"
            )
            
            try:
                variant = await self.expander._call_ollama(prompt)
                multi_queries[perspective] = variant
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Multi-Query Generation f√ºr '{perspective}' fehlgeschlagen: {e}")
                multi_queries[perspective] = query  # Fallback
        
        return multi_queries
