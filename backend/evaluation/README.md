# VERITAS RAG Evaluation Framework

Automatisierte QualitÃ¤tsbewertung fÃ¼r RAG-Pipeline mit Golden Dataset.

---

## ğŸ“¦ Komponenten

| Datei | Zweck | Zeilen |
|-------|-------|--------|
| `veritas_rag_evaluator.py` | RAG Evaluator mit Metriken | 850 |
| `run_baseline_evaluation.py` | Baseline-Evaluation-Script | 194 |
| `golden_dataset_schema.json` | JSON-Schema fÃ¼r Test-Cases | 120 |
| `golden_dataset_examples.json` | 5 Beispiel-Test-Cases | 180 |

---

## ğŸš€ Quick Start

### Baseline-Evaluation durchfÃ¼hren

```powershell
cd c:\VCC\veritas
python backend/evaluation/run_baseline_evaluation.py --mode baseline
```

**Output:**
- Console-Summary mit Metriken
- JSON-Report: `baseline_evaluation_report.json`

---

## ğŸ“Š Metriken

### Retrieval
- **Precision@K:** Anteil relevanter Dokumente in Top-K
- **Recall@K:** Anteil gefundener relevanter Dokumente
- **MRR:** Mean Reciprocal Rank
- **NDCG:** Normalized Discounted Cumulative Gain

### Context
- **Relevance Score:** LLM-basierte Kontext-Bewertung
- **Graph Enrichment:** Anzahl gefundener Relations

### Answer
- **Faithfulness:** Treue zum Context
- **Hallucination Rate:** Anteil halluzinierter Fakten
- **Completeness:** VollstÃ¤ndigkeit der Antwort

---

## ğŸ“ Golden Dataset

### Schema

```json
{
  "id": "test_001",
  "category": "legal",
  "complexity": "simple",
  "question": "Was regelt Â§ 110 BGB?",
  "expected_retrieval": {
    "documents": ["doc_bgb_110"],
    "entities": ["BGB", "Â§ 110"],
    "min_relevance_score": 0.7
  },
  "expected_answer": {
    "must_contain": ["Taschengeld"],
    "must_not_contain": ["Geldtransport"]
  },
  "hallucination_triggers": ["Geldtransport"]
}
```

### Test-Cases (aktuell: 5)

1. **bgb_110_basic** - Taschengeldparagraph (legal/simple)
2. **bgb_110_practical** - Praktische Anwendung (legal/medium)
3. **baurecht_baugenehmigung** - Baugenehmigung (building/medium)
4. **umweltrecht_emissionsgrenzwerte** - Emissionsschutz (environmental/complex)
5. **sozialrecht_wohngeld** - Wohngeld (social/multi_hop)

**Erweitern auf 50+:** Siehe `TODO.md`

---

## ğŸ“ˆ Expected Results

### Mit Re-Ranking (Standard)
```
Precision@5: ~75%
Recall@20: ~70%
MRR: ~0.80
Hallucination Rate: ~5%
Pass Rate: ~75%
```

### Ohne Re-Ranking (Baseline)
```
Precision@5: ~60%
MRR: ~0.65
Pass Rate: ~60%
```

**Re-Ranking Impact:** +15% Precision@5

---

## ğŸ”§ Konfiguration

### RAG Evaluator

```python
from backend.evaluation.veritas_rag_evaluator import RAGEvaluator

evaluator = RAGEvaluator(
    pipeline=my_pipeline,         # IntelligentMultiAgentPipeline
    ollama_client=my_client,      # Ollama AsyncClient
    strict_mode=False             # True fÃ¼r strengere Bewertung
)
```

### Pipeline Integration

```python
# Automatische Initialisierung
from backend.agents.veritas_intelligent_pipeline import IntelligentMultiAgentPipeline

pipeline = IntelligentMultiAgentPipeline(max_workers=5)
await pipeline.initialize()  # LÃ¤dt UDS3, Ollama, Agents

# Evaluator nutzt Pipeline automatisch
evaluator = RAGEvaluator(pipeline=pipeline, ollama_client=pipeline.ollama_client)
```

---

## ğŸ§ª Testing

### Mock-Pipeline (ohne UDS3)

```python
evaluator = RAGEvaluator(pipeline=None)  # Nutzt Mock-Data
summary = await evaluator.run_evaluation()
```

### Filtered Evaluation

```python
# Nur legal Test-Cases
summary = await evaluator.run_evaluation(filter_category="legal")

# Nur simple Test-Cases
summary = await evaluator.run_evaluation(filter_complexity="simple")

# Spezifische IDs
summary = await evaluator.run_evaluation(filter_ids=["bgb_110_basic"])
```

---

## ğŸ“Š Report-Format

### Console-Output

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
VERITAS RAG EVALUATION SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š Gesamt-Statistik
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total Test Cases: 5
  Passed: 3 âœ…
  Failed: 2 âŒ
  Pass Rate: 60.0%

ğŸ“ˆ Performance nach Kategorie
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  legal: 80.0%
  building: 65.0%

ğŸ¯ Retrieval-Metriken
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Precision@5: 72.3%
  Recall@20: 68.5%
  MRR: 0.78
```

### JSON-Report

```json
{
  "summary": {...},
  "detailed_results": [
    {
      "test_id": "bgb_110_basic",
      "passed": true,
      "retrieval_score": 0.85,
      "context_score": 0.78,
      "answer_score": 0.82,
      "overall_score": 0.81,
      "duration_ms": 1250
    }
  ]
}
```

---

## ğŸ› Troubleshooting

### UDS3 nicht verfÃ¼gbar
```
RuntimeError: RAG Integration (UDS3) ist nicht verfÃ¼gbar!
```
**LÃ¶sung:** PostgreSQL, Neo4j, ChromaDB starten

### Ollama nicht erreichbar
```
RuntimeError: Ollama Client Initialisierung fehlgeschlagen
```
**LÃ¶sung:** `ollama serve` starten

### Keine Dokumente gefunden
```
Pass Rate: 0% - Alle retrieval_scores = 0
```
**LÃ¶sung:** UDS3-Datenbank befÃ¼llen oder Golden Dataset mit echten Doc-IDs aktualisieren

---

## ğŸ“š Dokumentation

- **Implementation Guide:** `docs/RERANKING_EVALUATION_IMPLEMENTATION.md` (600+ Zeilen)
- **Phase 2 Complete:** `docs/PHASE_2_EVALUATION_COMPLETE.md`
- **Integration Complete:** `docs/BASELINE_EVALUATION_INTEGRATION_COMPLETE.md`
- **Hyperscaler Comparison:** Siehe Implementation Guide

---

## ğŸ¯ Roadmap

### Phase 1: Re-Ranking âœ… DONE
- Cross-Encoder-Modell (ms-marco-MiniLM-L-6-v2)
- Zwei-Stufen-Retrieval (Recall â†’ Precision)

### Phase 2: Evaluation âœ… DONE
- Golden Dataset Framework
- RAG Evaluator mit 8 Metriken
- Baseline-Evaluation-Script

### Phase 3: Supervisor-Agent â³ NEXT
- Hierarchische Orchestrierung
- Query-Dekomposition
- Intelligente Agent-Selektion

### Phase 4: Agent Communication â³ PLANNED
- AgentMessage-Schema
- Event Bus
- Agent Registry

---

## âš™ï¸ Dependencies

```bash
# Core
pip install asyncio logging dataclasses

# Re-Ranking
pip install sentence-transformers

# Ollama
pip install ollama

# Optional: Jupyter (fÃ¼r interaktive Evaluation)
pip install jupyter notebook
```

---

## ğŸ“ License

Internal VERITAS Project - Proprietary

---

**Version:** 1.0  
**Last Updated:** 06.10.2025  
**Author:** VERITAS System
