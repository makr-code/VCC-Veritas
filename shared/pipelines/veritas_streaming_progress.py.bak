#!/usr/bin/env python3
"""
VERITAS STREAMING PROGRESS SYSTEM
=================================
Real-time Agent Progress Updates f√ºr Frontend Integration

Features:
- WebSocket-basierte Progress Updates
- Agent Deep-thinking Zwischenergebnisse
- LLM-aufbereitete Fortschritts-Nachrichten
- Frontend-Integration f√ºr veritas_app.py

Author: VERITAS System
Date: 2025-09-21
Version: 1.0.0
"""

import asyncio
import json
import logging
import threading
import time
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from queue import Queue
from typing import Any, AsyncGenerator, Callable, Dict, List, Optional, Set

logger = logging.getLogger(__name__)

# ===== PROGRESS TYPES =====


class ProgressStage(Enum):
    """Progress-Stadien f√ºr Agent-Pipeline"""

    INITIALIZING = "initializing"
    ANALYZING_QUERY = "analyzing_query"
    SELECTING_AGENTS = "selecting_agents"
    AGENT_PROCESSING = "agent_processing"
    GATHERING_CONTEXT = "gathering_context"
    LLM_REASONING = "llm_reasoning"
    SYNTHESIZING = "synthesizing"
    FINALIZING = "finalizing"
    COMPLETED = "completed"
    ERROR = "error"


class ProgressType(Enum):
    """Typ des Progress-Updates"""

    STAGE_START = "stage_start"
    STAGE_PROGRESS = "stage_progress"
    STAGE_COMPLETE = "stage_complete"
    AGENT_START = "agent_start"
    AGENT_PROGRESS = "agent_progress"
    AGENT_COMPLETE = "agent_complete"
    INTERMEDIATE_RESULT = "intermediate_result"
    LLM_THINKING = "llm_thinking"
    HYPOTHESIS = "hypothesis"  # NEU: Hypothesen-Generierung
    STAGE_REFLECTION = "stage_reflection"  # NEU: LLM Meta-Reflection zu Stage
    FULFILLMENT_ANALYSIS = "fulfillment_analysis"  # NEU: Erf√ºllungsgrad-Analyse
    GAP_IDENTIFICATION = "gap_identification"  # NEU: L√ºcken-Identifikation
    ERROR = "error"
    SYSTEM_MESSAGE = "system_message"


@dataclass
class ProgressUpdate:
    """Einzelnes Progress-Update"""

    session_id: str
    query_id: str
    update_type: ProgressType
    stage: ProgressStage
    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())

    # Content
    message: str = ""
    details: Dict[str, Any] = field(default_factory=dict)

    # Progress Metrics
    progress_percent: float = 0.0
    estimated_time_remaining: float = 0.0

    # Agent-specific
    agent_type: Optional[str] = None
    agent_result: Optional[Dict[str, Any]] = None

    # LLM-specific
    llm_thinking_step: Optional[str] = None
    intermediate_conclusion: Optional[str] = None

    # Hypothesis (NEU)
    hypothesis_data: Optional[Dict[str, Any]] = None
    confidence: Optional[str] = None
    information_gaps: Optional[List[Dict[str, Any]]] = None
    clarification_questions: Optional[List[str]] = None

    # Stage Reflection (NEU)
    reflection_data: Optional[Dict[str, Any]] = None
    completion_percent: Optional[float] = None
    identified_gaps: Optional[List[str]] = None
    gathered_info: Optional[List[str]] = None
    next_actions: Optional[List[str]] = None


# ===== PROGRESS MANAGER =====


class VeritasProgressManager:
    """
    Manager f√ºr Real-time Progress Updates

    FUNKTIONEN:
    - WebSocket-√§hnliche Progress-Streams
    - Agent-Pipeline Integration
    - LLM Deep-thinking Updates
    - Frontend-kompatible JSON-Updates
    """

    def __init__(self):
        """Initialisiert Progress Manager"""
        self.active_sessions: Dict[str, Dict[str, Any]] = {}
        self.progress_subscribers: Dict[str, List[Callable]] = {}
        self.progress_lock = threading.RLock()
        self.cancelled_sessions: Set[str] = set()  # Track cancelled sessions

        # LLM Integration f√ºr aufbereitete Messages
        self.llm_available = self._check_llm_availability()

        # Progress Templates
        self.stage_messages = {
            ProgressStage.INITIALIZING: "üöÄ Initialisiere Anfrage-Verarbeitung...",
            ProgressStage.ANALYZING_QUERY: "üîç Analysiere Ihre Anfrage...",
            ProgressStage.SELECTING_AGENTS: "üéØ W√§hle passende Agenten aus...",
            ProgressStage.AGENT_PROCESSING: "‚öôÔ∏è Agenten arbeiten an Ihrer Anfrage...",
            ProgressStage.GATHERING_CONTEXT: "üìö Sammle relevanten Kontext...",
            ProgressStage.LLM_REASONING: "üß† Verarbeite und analysiere Informationen...",
            ProgressStage.SYNTHESIZING: "üîó Erstelle umfassende Antwort...",
            ProgressStage.FINALIZING: "‚ú® Bereite finale Antwort vor...",
            ProgressStage.COMPLETED: "‚úÖ Verarbeitung abgeschlossen!",
            ProgressStage.ERROR: "‚ùå Fehler aufgetreten",
        }

        logger.info("üì° Progress Manager initialisiert")

    def _check_llm_availability(self) -> bool:
        """Pr√ºft ob LLM f√ºr Message-Aufbereitung verf√ºgbar ist"""
        try:
            # Hier w√ºrde native_ollama_integration importiert
            # from native_ollama_integration import OllamaClient
            return False  # Placeholder
        except ImportError:
            return False

    def start_session(self, session_id: str, query_id: str, query_text: str) -> None:
        """Startet neue Progress-Session"""
        with self.progress_lock:
            self.active_sessions[session_id] = {
                "query_id": query_id,
                "query_text": query_text,
                "start_time": time.time(),
                "current_stage": ProgressStage.INITIALIZING,
                "completed_stages": [],
                "active_agents": [],
                "progress_history": [],
                "estimated_total_time": self._estimate_total_time(query_text),
            }

            self.progress_subscribers[session_id] = []

        # Initial Progress Update
        self._emit_progress(
            session_id=session_id,
            query_id=query_id,
            update_type=ProgressType.STAGE_START,
            stage=ProgressStage.INITIALIZING,
            message=f"Starte Verarbeitung: '{query_text[:50]}...'",
            progress_percent=0.0,
        )

        logger.info(f"üì° Progress Session gestartet: {session_id}")

    def _estimate_total_time(self, query_text: str) -> float:
        """Sch√§tzt Gesamtverarbeitungszeit"""
        # Einfache Heuristik basierend auf Query-L√§nge und Komplexit√§t
        base_time = 5.0  # 5 Sekunden Basis

        complexity_indicators = ["analysiere", "vergleiche", "bewerte", "wahrscheinlichkeit"]
        complexity_boost = sum(1.5 for indicator in complexity_indicators if indicator in query_text.lower())

        length_factor = len(query_text.split()) * 0.1

        return base_time + complexity_boost + length_factor

    def subscribe_to_progress(self, session_id: str, callback: Callable[[ProgressUpdate], None]) -> None:
        """Registriert Callback f√ºr Progress Updates"""
        with self.progress_lock:
            if session_id not in self.progress_subscribers:
                self.progress_subscribers[session_id] = []
            self.progress_subscribers[session_id].append(callback)

    def unsubscribe_from_progress(self, session_id: str, callback: Callable) -> None:
        """Entfernt Callback"""
        with self.progress_lock:
            if session_id in self.progress_subscribers:
                try:
                    self.progress_subscribers[session_id].remove(callback)
                except ValueError:
                    pass

    def update_stage(self, session_id: str, stage: ProgressStage, details: Dict[str, Any] = None) -> None:
        """Updated aktuelles Processing-Stadium"""
        with self.progress_lock:
            if session_id not in self.active_sessions:
                return

            session = self.active_sessions[session_id]
            previous_stage = session["current_stage"]
            session["current_stage"] = stage

            if previous_stage != stage:
                session["completed_stages"].append(previous_stage)

        # Progress berechnen
        stage_progress = self._calculate_stage_progress(session_id, stage)

        # LLM-aufbereitete Message
        message = self._generate_stage_message(session_id, stage, details or {})

        self._emit_progress(
            session_id=session_id,
            query_id=session["query_id"],
            update_type=ProgressType.STAGE_START,
            stage=stage,
            message=message,
            details=details or {},
            progress_percent=stage_progress,
        )

    def _calculate_stage_progress(self, session_id: str, stage: ProgressStage) -> float:
        """Berechnet Progress-Prozent basierend auf Stadium"""
        stage_weights = {
            ProgressStage.INITIALIZING: 5.0,
            ProgressStage.ANALYZING_QUERY: 15.0,
            ProgressStage.SELECTING_AGENTS: 25.0,
            ProgressStage.AGENT_PROCESSING: 45.0,
            ProgressStage.GATHERING_CONTEXT: 60.0,
            ProgressStage.LLM_REASONING: 75.0,
            ProgressStage.SYNTHESIZING: 90.0,
            ProgressStage.FINALIZING: 95.0,
            ProgressStage.COMPLETED: 100.0,
        }

        return stage_weights.get(stage, 0.0)

    def _generate_stage_message(self, session_id: str, stage: ProgressStage, details: Dict[str, Any]) -> str:
        """Generiert kontextuelle Stage-Message"""
        base_message = self.stage_messages.get(stage, f"Verarbeitung: {stage.value}")

        # Kontext-spezifische Anreicherung
        if stage == ProgressStage.SELECTING_AGENTS and "selected_agents" in details:
            agents = details["selected_agents"]
            base_message += f" ({len(agents)} Agenten ausgew√§hlt)"

        elif stage == ProgressStage.AGENT_PROCESSING and "active_agents" in details:
            active_count = len(details["active_agents"])
            base_message += f" ({active_count} Agenten aktiv)"

        elif stage == ProgressStage.LLM_REASONING and "reasoning_step" in details:
            step = details["reasoning_step"]
            base_message += f" - {step}"

        return base_message

    def update_agent_progress(
        self, session_id: str, agent_type: str, progress_type: ProgressType, message: str = "", result: Dict[str, Any] = None
    ) -> None:
        """Updated Agent-spezifischen Progress"""

        with self.progress_lock:
            if session_id not in self.active_sessions:
                return

            session = self.active_sessions[session_id]

            if progress_type == ProgressType.AGENT_START:
                if agent_type not in session["active_agents"]:
                    session["active_agents"].append(agent_type)

            elif progress_type == ProgressType.AGENT_COMPLETE:
                if agent_type in session["active_agents"]:
                    session["active_agents"].remove(agent_type)

        # Agent-spezifische Message
        if not message:
            message = self._generate_agent_message(agent_type, progress_type, result)

        self._emit_progress(
            session_id=session_id,
            query_id=session["query_id"],
            update_type=progress_type,
            stage=session["current_stage"],
            message=message,
            agent_type=agent_type,
            agent_result=result,
        )

    def _generate_agent_message(self, agent_type: str, progress_type: ProgressType, result: Dict[str, Any] = None) -> str:
        """Generiert Agent-spezifische Messages"""

        agent_names = {
            "geo_context": "Geo-Kontext Agent",
            "legal_framework": "Rechts-Framework Agent",
            "document_retrieval": "Dokument-Retrieval Agent",
            "environmental": "Umwelt-Agent",
            "construction": "Bau-Agent",
            "traffic": "Verkehrs-Agent",
            "financial": "Finanz-Agent",
            "social": "Sozial-Agent",
        }

        agent_name = agent_names.get(agent_type, f"{agent_type.title()} Agent")

        if progress_type == ProgressType.AGENT_START:
            return f"üîÑ {agent_name} startet Analyse..."

        elif progress_type == ProgressType.AGENT_PROGRESS:
            return f"‚öôÔ∏è {agent_name} verarbeitet..."

        elif progress_type == ProgressType.AGENT_COMPLETE:
            if result and "confidence_score" in result:
                confidence = result["confidence_score"]
                return f"‚úÖ {agent_name} abgeschlossen (Confidence: {confidence:.0%})"
            else:
                return f"‚úÖ {agent_name} abgeschlossen"

        return f"{agent_name}: {progress_type.value}"

    def add_intermediate_result(
        self, session_id: str, result_type: str, content: str, confidence: float = 0.0, sources: List[str] = None
    ) -> None:
        """F√ºgt Zwischenergebnis hinzu"""

        # LLM-aufbereitetes Zwischenergebnis
        formatted_content = self._format_intermediate_result(result_type, content, confidence)

        self._emit_progress(
            session_id=session_id,
            query_id=self.active_sessions[session_id]["query_id"],
            update_type=ProgressType.INTERMEDIATE_RESULT,
            stage=self.active_sessions[session_id]["current_stage"],
            message=f"üìÑ Zwischenergebnis: {result_type}",
            details={
                "result_type": result_type,
                "content": formatted_content,
                "confidence": confidence,
                "sources": sources or [],
            },
            intermediate_conclusion=formatted_content,
        )

    def _format_intermediate_result(self, result_type: str, content: str, confidence: float) -> str:
        """Formatiert Zwischenergebnis f√ºr Frontend"""

        confidence_emoji = "üü¢" if confidence > 0.8 else "üü°" if confidence > 0.6 else "üî¥"

        formatted = f"{confidence_emoji} **{result_type}** (Confidence: {confidence:.0%})\n\n{content}"

        if len(formatted) > 300:
            formatted = formatted[:297] + "..."

        return formatted

    def add_llm_thinking_step(self, session_id: str, thinking_step: str, reasoning: str = "") -> None:
        """F√ºgt LLM Deep-thinking Step hinzu"""

        self._emit_progress(
            session_id=session_id,
            query_id=self.active_sessions[session_id]["query_id"],
            update_type=ProgressType.LLM_THINKING,
            stage=ProgressStage.LLM_REASONING,
            message=f"üß† {thinking_step}",
            details={"reasoning": reasoning},
            llm_thinking_step=thinking_step,
        )

    def add_hypothesis(self, session_id: str, hypothesis: Any, confidence: str = "medium") -> None:
        """
        F√ºgt Hypothesen-Generierung als Progress-Event hinzu

        Args:
            session_id: Session-ID
            hypothesis: Hypothesis object from HypothesisService
            confidence: Confidence level (high/medium/low)
        """

        confidence_emoji = {"high": "üü¢", "medium": "üü°", "low": "üî¥", "unknown": "‚ö™"}

        emoji = confidence_emoji.get(confidence, "‚ö™")

        # Extrahiere Hypothesis-Daten (nutze Methoden statt Attribute wo n√∂tig)
        hypothesis_data = {
            "question_type": hypothesis.question_type.value
            if hasattr(hypothesis.question_type, "value")
            else str(hypothesis.question_type),
            "confidence": hypothesis.confidence.value
            if hasattr(hypothesis.confidence, "value")
            else str(hypothesis.confidence),
            "primary_intent": hypothesis.primary_intent,
            "information_gaps": [
                {
                    "gap_type": gap.gap_type,
                    "severity": gap.severity.value if hasattr(gap.severity, "value") else str(gap.severity),
                    "suggested_query": gap.suggested_query,
                    "examples": gap.examples,
                }
                for gap in hypothesis.information_gaps
            ],
            "assumptions": hypothesis.assumptions,
            "clarification_questions": hypothesis.get_clarification_questions(),  # ‚Üê Methode nutzen!
            "required_information": hypothesis.required_information,  # ‚Üê Korrektes Attribut
            "suggested_steps": hypothesis.suggested_steps,
        }

        # Formatierte Message
        gap_count = len(hypothesis.information_gaps)
        critical_gaps = sum(
            1 for gap in hypothesis.information_gaps if hasattr(gap.severity, "value") and gap.severity.value == "critical"
        )

        message = f"{emoji} Hypothese: {hypothesis.question_type.value} | Konfidenz: {hypothesis.confidence.value}"
        if gap_count > 0:
            message += f" | L√ºcken: {gap_count} ({critical_gaps} kritisch)"

        self._emit_progress(
            session_id=session_id,
            query_id=self.active_sessions[session_id]["query_id"],
            update_type=ProgressType.HYPOTHESIS,
            stage=ProgressStage.ANALYZING_QUERY,
            message=message,
            details=hypothesis_data,
            hypothesis_data=hypothesis_data,
            confidence=hypothesis.confidence.value if hasattr(hypothesis.confidence, "value") else str(hypothesis.confidence),
            information_gaps=hypothesis_data["information_gaps"],
            clarification_questions=hypothesis.get_clarification_questions(),  # ‚Üê Methode nutzen!
        )

    def add_stage_reflection(
        self,
        session_id: str,
        reflection_stage: str,
        completion_percent: float,
        fulfillment_status: str,
        identified_gaps: List[str],
        gathered_info: List[str],
        next_actions: List[str],
        confidence: float,
        llm_reasoning: str,
    ) -> None:
        """
        F√ºgt LLM-gest√ºtzte Stage Reflection hinzu

        Args:
            session_id: Session-ID
            reflection_stage: Stage die reflektiert wird (z.B. "hypothesis", "retrieval")
            completion_percent: Erf√ºllungsgrad 0-100
            fulfillment_status: "incomplete", "partial", "complete"
            identified_gaps: Liste von identifizierten L√ºcken
            gathered_info: Liste von gesammelten Informationen
            next_actions: Empfohlene n√§chste Schritte
            confidence: LLM Konfidenz 0-1
            llm_reasoning: LLM Begr√ºndung
        """

        status_emoji = {"incomplete": "üî¥", "partial": "üü°", "complete": "üü¢"}

        emoji = status_emoji.get(fulfillment_status, "‚ö™")

        # Formatierte Message
        message = f"{emoji} Stage Reflection: {reflection_stage} | Erf√ºllung: {completion_percent:.0f}%"

        # Strukturierte Reflection-Daten
        reflection_data = {
            "stage": reflection_stage,
            "completion_percent": completion_percent,
            "fulfillment_status": fulfillment_status,
            "identified_gaps": identified_gaps,
            "gathered_info": gathered_info,
            "next_actions": next_actions,
            "confidence": confidence,
            "llm_reasoning": llm_reasoning,
        }

        self._emit_progress(
            session_id=session_id,
            query_id=self.active_sessions[session_id]["query_id"],
            update_type=ProgressType.STAGE_REFLECTION,
            stage=self.active_sessions[session_id]["current_stage"],
            message=message,
            details=reflection_data,
            reflection_data=reflection_data,
            completion_percent=completion_percent,
            identified_gaps=identified_gaps,
            gathered_info=gathered_info,
            next_actions=next_actions,
        )

    def complete_session(self, session_id: str, final_result: Dict[str, Any]) -> None:
        """Beendet Progress-Session"""

        self._emit_progress(
            session_id=session_id,
            query_id=self.active_sessions[session_id]["query_id"],
            update_type=ProgressType.STAGE_COMPLETE,
            stage=ProgressStage.COMPLETED,
            message="‚úÖ Verarbeitung erfolgreich abgeschlossen!",
            details=final_result,
            progress_percent=100.0,
        )

        # Session cleanup
        with self.progress_lock:
            if session_id in self.active_sessions:
                del self.active_sessions[session_id]
            if session_id in self.progress_subscribers:
                del self.progress_subscribers[session_id]

        logger.info(f"üì° Progress Session beendet: {session_id}")

    def cancel_session(self, session_id: str, reason: str = "user_cancelled") -> None:
        """
        Bricht Progress-Session ab

        Args:
            session_id: Session-ID zum Abbrechen
            reason: Grund f√ºr Abbruch
        """
        with self.progress_lock:
            # Markiere Session als abgebrochen
            self.cancelled_sessions.add(session_id)

            # Pr√ºfe ob Session existiert
            if session_id not in self.active_sessions:
                logger.warning(f"‚ö†Ô∏è Session {session_id} nicht gefunden f√ºr Abbruch")
                return

            # Update Session Status
            self.active_sessions[session_id]["current_stage"] = ProgressStage.ERROR
            self.active_sessions[session_id]["cancelled"] = True
            self.active_sessions[session_id]["cancel_reason"] = reason

        # Sende Cancel-Update
        self._emit_progress(
            session_id=session_id,
            query_id=self.active_sessions[session_id]["query_id"],
            update_type=ProgressType.ERROR,
            stage=ProgressStage.ERROR,
            message=f"üõë Verarbeitung abgebrochen: {reason}",
            details={"cancelled": True, "reason": reason, "cancel_time": datetime.now(timezone.utc).isoformat()},
            progress_percent=0.0,
        )

        # Session cleanup nach kurzer Verz√∂gerung
        def delayed_cleanup():
            time.sleep(1.0)  # Kurz warten damit Frontend Cancel-Message erh√§lt
            with self.progress_lock:
                if session_id in self.active_sessions:
                    del self.active_sessions[session_id]
                if session_id in self.progress_subscribers:
                    del self.progress_subscribers[session_id]

        # Cleanup in separatem Thread
        cleanup_thread = threading.Thread(target=delayed_cleanup, daemon=True)
        cleanup_thread.start()

        logger.info(f"üõë Progress Session abgebrochen: {session_id} (Grund: {reason})")

    def is_session_cancelled(self, session_id: str) -> bool:
        """Pr√ºft ob Session abgebrochen wurde"""
        return session_id in self.cancelled_sessions

    def _emit_progress(self, **kwargs) -> None:
        """Emittiert Progress Update an alle Subscriber"""

        progress_update = ProgressUpdate(**kwargs)
        session_id = kwargs["session_id"]

        # Update zu History hinzuf√ºgen
        with self.progress_lock:
            if session_id in self.active_sessions:
                self.active_sessions[session_id]["progress_history"].append(progress_update)

        # An alle Subscriber senden
        if session_id in self.progress_subscribers:
            for callback in self.progress_subscribers[session_id]:
                try:
                    callback(progress_update)
                except Exception as e:
                    logger.error(f"Progress Callback Fehler: {e}")

        logger.debug(f"üì° Progress emitted: {progress_update.update_type.value} - {progress_update.message}")

    def get_session_progress(self, session_id: str) -> Dict[str, Any]:
        """Holt aktuellen Session-Progress"""

        with self.progress_lock:
            if session_id not in self.active_sessions:
                return {}

            session = self.active_sessions[session_id]
            elapsed_time = time.time() - session["start_time"]
            estimated_remaining = max(0, session["estimated_total_time"] - elapsed_time)

            return {
                "session_id": session_id,
                "query_id": session["query_id"],
                "current_stage": session["current_stage"].value,
                "completed_stages": [stage.value for stage in session["completed_stages"]],
                "active_agents": session["active_agents"],
                "progress_percent": self._calculate_stage_progress(session_id, session["current_stage"]),
                "elapsed_time": elapsed_time,
                "estimated_remaining": estimated_remaining,
                "total_updates": len(session["progress_history"]),
            }


# ===== STREAMING ENDPOINTS =====


class VeritasProgressStreamer:
    """
    Streaming Interface f√ºr Frontend-Integration

    INTEGRATION mit veritas_app.py:
    - Server-Sent Events (SSE) f√ºr Real-time Updates
    - JSON Progress-Format
    - WebSocket-√§hnliche Funktionalit√§t √ºber HTTP
    """

    def __init__(self, progress_manager: VeritasProgressManager):
        self.progress_manager = progress_manager
        self.active_streams: Dict[str, asyncio.Queue] = {}

    async def create_progress_stream(self, session_id: str) -> AsyncGenerator[str, None]:
        """Erstellt Progress-Stream f√ºr Session"""

        # Queue f√ºr diese Session
        progress_queue = asyncio.Queue()
        self.active_streams[session_id] = progress_queue

        # Progress Callback registrieren
        def progress_callback(update: ProgressUpdate):
            try:
                # Async queue von sync callback - verwende thread-safe put
                asyncio.run_coroutine_threadsafe(progress_queue.put(update), asyncio.get_event_loop())
            except Exception as e:
                logger.error(f"Stream Callback Fehler: {e}")

        self.progress_manager.subscribe_to_progress(session_id, progress_callback)

        # üî• WICHTIG: Sende bereits vorhandene Events aus progress_history!
        # Wenn Client nach Hypothesis-Generation connected, w√ºrde er sonst das Event verpassen!
        try:
            with self.progress_manager.progress_lock:
                if session_id in self.progress_manager.active_sessions:
                    history = self.progress_manager.active_sessions[session_id]["progress_history"]
                    for past_update in history:
                        await progress_queue.put(past_update)
                        logger.debug(f"üìú Sending historical event: {past_update.update_type.value}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not send historical events: {e}")

        try:
            while True:
                # Warte auf n√§chstes Update
                try:
                    update = await asyncio.wait_for(progress_queue.get(), timeout=30.0)

                    # SSE-Format f√ºr Frontend
                    sse_data = {
                        "type": update.update_type.value,
                        "stage": update.stage.value,
                        "message": update.message,
                        "progress": update.progress_percent,
                        "timestamp": update.timestamp,
                        "details": update.details,
                    }

                    # Optional: Agent/LLM spezifische Daten
                    if update.agent_type:
                        sse_data["agent_type"] = update.agent_type
                    if update.intermediate_conclusion:
                        sse_data["intermediate_result"] = update.intermediate_conclusion
                    if update.llm_thinking_step:
                        sse_data["llm_thinking"] = update.llm_thinking_step

                    # ‚ú® NEU: Hypothesis-Daten
                    if update.hypothesis_data:
                        sse_data["hypothesis_data"] = update.hypothesis_data
                    if update.confidence:
                        sse_data["confidence"] = update.confidence
                    if update.information_gaps:
                        sse_data["information_gaps"] = update.information_gaps
                    if update.clarification_questions:
                        sse_data["clarification_questions"] = update.clarification_questions

                    # ‚ú® NEU: Stage Reflection Daten
                    if update.reflection_data:
                        sse_data["reflection_data"] = update.reflection_data
                    if update.completion_percent is not None:
                        sse_data["completion_percent"] = update.completion_percent
                    if update.identified_gaps:
                        sse_data["identified_gaps"] = update.identified_gaps
                    if update.gathered_info:
                        sse_data["gathered_info"] = update.gathered_info
                    if update.next_actions:
                        sse_data["next_actions"] = update.next_actions

                    # Server-Sent Event Format
                    yield f"data: {json.dumps(sse_data)}\n\n"

                    # Beende Stream bei Completion
                    if update.stage == ProgressStage.COMPLETED or update.stage == ProgressStage.ERROR:
                        break

                except asyncio.TimeoutError:
                    # Heartbeat
                    yield f"data: {json.dumps({'type': 'heartbeat', 'timestamp': datetime.now(timezone.utc).isoformat()})}\n\n"

        finally:
            # Cleanup
            self.progress_manager.unsubscribe_from_progress(session_id, progress_callback)
            if session_id in self.active_streams:
                del self.active_streams[session_id]


# ===== FACTORY FUNCTIONS =====


def create_progress_manager() -> VeritasProgressManager:
    """Factory f√ºr Progress Manager"""
    return VeritasProgressManager()


def create_progress_streamer(progress_manager: VeritasProgressManager) -> VeritasProgressStreamer:
    """Factory f√ºr Progress Streamer"""
    return VeritasProgressStreamer(progress_manager)


# ===== EXAMPLE USAGE =====


async def example_agent_processing_with_progress():
    """Beispiel f√ºr Agent-Processing mit Progress Updates"""

    # Setup
    progress_manager = create_progress_manager()
    session_id = "example_session"
    query_id = "example_query"

    # Session starten
    progress_manager.start_session(
        session_id=session_id,
        query_id=query_id,
        query_text="Wie kann ich eine Baugenehmigung beantragen und welche Kosten entstehen?",
    )

    # Simulated Agent Processing
    stages = [
        (ProgressStage.ANALYZING_QUERY, 1.0),
        (ProgressStage.SELECTING_AGENTS, 0.5),
        (ProgressStage.AGENT_PROCESSING, 3.0),
        (ProgressStage.GATHERING_CONTEXT, 1.5),
        (ProgressStage.LLM_REASONING, 2.0),
        (ProgressStage.SYNTHESIZING, 1.0),
        (ProgressStage.FINALIZING, 0.5),
    ]

    for stage, duration in stages:
        progress_manager.update_stage(session_id, stage)

        if stage == ProgressStage.AGENT_PROCESSING:
            # Simuliere Agent-Arbeit
            agents = ["legal_framework", "geo_context", "financial"]
            for agent in agents:
                progress_manager.update_agent_progress(session_id, agent, ProgressType.AGENT_START)
                await asyncio.sleep(duration / len(agents))

                # Zwischenergebnis
                progress_manager.add_intermediate_result(
                    session_id=session_id,
                    result_type=f"{agent}_result",
                    content=f"Ergebnis von {agent} Agent",
                    confidence=0.85,
                )

                progress_manager.update_agent_progress(session_id, agent, ProgressType.AGENT_COMPLETE)

        elif stage == ProgressStage.LLM_REASONING:
            # Simuliere LLM Deep-thinking
            thinking_steps = [
                "Analysiere rechtliche Rahmenbedingungen",
                "Bewerte lokale Bestimmungen",
                "Berechne Kostenstruktuur",
                "Erstelle Handlungsempfehlungen",
            ]

            for step in thinking_steps:
                progress_manager.add_llm_thinking_step(session_id, step)
                await asyncio.sleep(duration / len(thinking_steps))

        else:
            await asyncio.sleep(duration)

    # Session beenden
    progress_manager.complete_session(
        session_id, {"final_answer": "Umfassende Antwort zur Baugenehmigung...", "confidence": 0.92}
    )


if __name__ == "__main__":
    # Test des Progress Systems
    asyncio.run(example_agent_processing_with_progress())
