# VERITAS Performance Testing & Systematic Optimization

**Version:** v3.20.0  
**Priorit√§t:** ‚≠ê‚≠ê MEDIUM  
**Gesch√§tzter Aufwand:** 6-8 Stunden  
**Status:** üîÑ TODO (Nach Chat-Persistierung)

---

## üéØ Ziel

**Systematische Performance-Analyse und -Optimierung:**
1. ‚úÖ Baseline Performance messen (aktuelle Metriken erfassen)
2. ‚úÖ Bottlenecks identifizieren (Profiling)
3. ‚úÖ Optimierungen implementieren (Query Caching, etc.)
4. ‚úÖ Verbesserungen validieren (A/B Testing)
5. ‚úÖ Performance-Metriken etablieren (Continuous Monitoring)

**Ziel-Metriken:**
- üéØ **Query Response Time:** <10s (aktuell: 36.88s) ‚Üí **-73% Latenz**
- üéØ **Backend Health Check:** <50ms (aktuell: <50ms) ‚Üí **Beibehalten**
- üéØ **Export Performance:** <2s (aktuell: <2s) ‚Üí **Beibehalten**
- üéØ **Memory Usage:** <2GB (aktuell: <2GB) ‚Üí **Beibehalten**
- üéØ **Throughput:** >10 Queries/min ‚Üí **+900%**

---

## üìä Phase 1: Baseline Performance Measurement (1-2h)

### Task 1.1: Performance Test Suite erstellen ‚è±Ô∏è 45min

**Ziel:** Reproduzierbare Performance-Tests mit Metriken

**Deliverables:**
- [ ] `tests/performance/test_query_performance.py` (300 LOC)
- [ ] `tests/performance/test_backend_api_performance.py` (200 LOC)
- [ ] `tests/performance/test_uds3_search_performance.py` (250 LOC)

**Test-Kategorien:**

#### 1.1.1 Query Performance Tests
```python
import pytest
import time
from statistics import mean, median, stdev

class QueryPerformanceTest:
    """Systematische Query-Performance-Messungen"""
    
    @pytest.fixture
    def test_queries(self):
        """Standard-Testfragen mit unterschiedlicher Komplexit√§t"""
        return [
            {
                "query": "Was ist das BImSchG?",
                "complexity": "simple",
                "expected_sources": 5
            },
            {
                "query": "Welche Grenzwerte gelten f√ºr Windkraftanlagen nach BImSchG?",
                "complexity": "medium",
                "expected_sources": 8
            },
            {
                "query": "Vergleiche die Genehmigungsverfahren f√ºr Windkraftanlagen nach BImSchG und BauGB unter Ber√ºcksichtigung der aktuellen Rechtsprechung.",
                "complexity": "complex",
                "expected_sources": 12
            }
        ]
    
    def test_query_response_time_simple(self, test_queries):
        """Misst Response-Zeit f√ºr einfache Queries"""
        simple_queries = [q for q in test_queries if q['complexity'] == 'simple']
        
        response_times = []
        for test_case in simple_queries:
            start_time = time.perf_counter()
            
            response = await ollama_client.query(test_case['query'])
            
            end_time = time.perf_counter()
            duration = end_time - start_time
            
            response_times.append(duration)
            
            # Metrics sammeln
            assert duration < 10.0, f"Simple query too slow: {duration:.2f}s"
        
        # Statistiken
        print(f"\nüìä Simple Query Performance:")
        print(f"  Mean: {mean(response_times):.2f}s")
        print(f"  Median: {median(response_times):.2f}s")
        print(f"  Std Dev: {stdev(response_times):.2f}s")
        print(f"  Min: {min(response_times):.2f}s")
        print(f"  Max: {max(response_times):.2f}s")
    
    def test_query_response_time_medium(self, test_queries):
        """Misst Response-Zeit f√ºr mittelschwere Queries"""
        # Analog zu simple, Target: <15s
        ...
    
    def test_query_response_time_complex(self, test_queries):
        """Misst Response-Zeit f√ºr komplexe Queries"""
        # Analog zu simple, Target: <30s
        ...
    
    def test_concurrent_queries(self):
        """Misst Performance bei parallelen Queries"""
        import asyncio
        
        queries = ["Was ist das BImSchG?"] * 10
        
        start_time = time.perf_counter()
        
        # 10 parallele Queries
        tasks = [ollama_client.query(q) for q in queries]
        results = await asyncio.gather(*tasks)
        
        end_time = time.perf_counter()
        total_duration = end_time - start_time
        
        throughput = len(queries) / total_duration
        
        print(f"\nüìä Concurrent Query Performance:")
        print(f"  Total Duration: {total_duration:.2f}s")
        print(f"  Throughput: {throughput:.2f} queries/s")
        print(f"  Avg per Query: {total_duration/len(queries):.2f}s")
        
        assert throughput > 1.0, "Throughput should be >1 query/s"
```

#### 1.1.2 Component-Level Performance Tests
```python
class ComponentPerformanceTest:
    """Isolierte Performance-Tests f√ºr Komponenten"""
    
    def test_uds3_search_performance(self):
        """UDS3 Search API Performance"""
        queries = [
            "BImSchG",
            "Windkraftanlagen Genehmigung",
            "Umweltvertr√§glichkeitspr√ºfung"
        ]
        
        timings = {
            'vector_search': [],
            'graph_search': [],
            'hybrid_search': []
        }
        
        for query in queries:
            # Vector Search
            start = time.perf_counter()
            vector_results = await strategy.search_api.vector_search(query, top_k=10)
            timings['vector_search'].append(time.perf_counter() - start)
            
            # Graph Search
            start = time.perf_counter()
            graph_results = await strategy.search_api.graph_search(query, limit=10)
            timings['graph_search'].append(time.perf_counter() - start)
            
            # Hybrid Search
            start = time.perf_counter()
            hybrid_results = await strategy.search_api.hybrid_search(query, top_k=10)
            timings['hybrid_search'].append(time.perf_counter() - start)
        
        # Report
        print("\nüìä UDS3 Search Performance:")
        for search_type, times in timings.items():
            print(f"  {search_type}: {mean(times)*1000:.2f}ms (avg)")
    
    def test_llm_inference_performance(self):
        """Ollama LLM Inference Performance"""
        prompts = [
            "Erkl√§re das BImSchG in einem Satz.",
            "Was sind die Hauptziele des BImSchG?",
            "Welche Beh√∂rden sind f√ºr die Umsetzung des BImSchG zust√§ndig?"
        ]
        
        timings = []
        token_counts = []
        
        for prompt in prompts:
            start = time.perf_counter()
            response = await ollama.generate(model="llama3.1:8b", prompt=prompt)
            duration = time.perf_counter() - start
            
            timings.append(duration)
            # Token-Sch√§tzung
            token_counts.append(len(response['response']) // 4)
        
        tokens_per_second = sum(token_counts) / sum(timings)
        
        print(f"\nüìä LLM Inference Performance:")
        print(f"  Avg Response Time: {mean(timings):.2f}s")
        print(f"  Tokens/Second: {tokens_per_second:.2f}")
        print(f"  Avg Token Count: {mean(token_counts):.0f}")
    
    def test_embedding_generation_performance(self):
        """Embedding-Generierung Performance"""
        texts = [
            "Das Bundes-Immissionsschutzgesetz regelt...",
            "Windkraftanlagen unterliegen der Genehmigungspflicht...",
            # ... mehr Test-Texte
        ]
        
        start = time.perf_counter()
        embeddings = [await ollama.embeddings(model="all-MiniLM-L6-v2", prompt=text) 
                     for text in texts]
        duration = time.perf_counter() - start
        
        throughput = len(texts) / duration
        
        print(f"\nüìä Embedding Generation Performance:")
        print(f"  Throughput: {throughput:.2f} docs/s")
        print(f"  Avg per Doc: {duration/len(texts)*1000:.2f}ms")
```

---

### Task 1.2: Performance Profiling Setup ‚è±Ô∏è 30min

**Ziel:** Code-Level Profiling f√ºr Bottleneck-Identifikation

**Tools:**
- ‚úÖ `cProfile` - Python Built-in Profiler
- ‚úÖ `line_profiler` - Line-by-Line Profiling
- ‚úÖ `memory_profiler` - Memory Usage Tracking
- ‚úÖ `py-spy` - Sampling Profiler (low overhead)

**Deliverables:**
- [ ] `scripts/profile_query.py` (150 LOC)
- [ ] `scripts/profile_uds3_search.py` (100 LOC)
- [ ] `scripts/profile_memory.py` (100 LOC)

**Profiling-Script Beispiel:**
```python
# scripts/profile_query.py

import cProfile
import pstats
import asyncio
from io import StringIO

async def profile_query_execution():
    """Profiling eines kompletten Query-Ablaufs"""
    
    # Setup
    from backend.agents.veritas_ollama_client import VeritasOllamaClient
    from uds3 import get_optimized_unified_strategy
    
    ollama_client = VeritasOllamaClient()
    strategy = get_optimized_unified_strategy()
    
    # Query
    test_query = "Was ist das BImSchG?"
    
    # Start Profiling
    profiler = cProfile.Profile()
    profiler.enable()
    
    # Ausf√ºhrung
    response = await ollama_client.query(test_query)
    
    # Stop Profiling
    profiler.disable()
    
    # Results
    s = StringIO()
    stats = pstats.Stats(profiler, stream=s)
    stats.sort_stats('cumulative')
    stats.print_stats(50)  # Top 50 Funktionen
    
    print(s.getvalue())
    
    # Save to file
    with open('data/profiles/query_profile.txt', 'w') as f:
        f.write(s.getvalue())

if __name__ == "__main__":
    asyncio.run(profile_query_execution())
```

**Memory Profiling:**
```python
# scripts/profile_memory.py

from memory_profiler import profile
import asyncio

@profile
async def profile_memory_usage():
    """Memory-Profiling f√ºr Query-Execution"""
    
    # Load large model
    ollama_client = VeritasOllamaClient(model="llama3.1:8b")
    
    # Execute query
    response = await ollama_client.query("Was ist das BImSchG?")
    
    # Process response
    formatted = format_response(response)
    
    return formatted

if __name__ == "__main__":
    asyncio.run(profile_memory_usage())
```

---

### Task 1.3: Baseline Report generieren ‚è±Ô∏è 30min

**Ziel:** Dokumentiertes Baseline-Performance-Profil

**Deliverables:**
- [ ] `docs/PERFORMANCE_BASELINE_REPORT.md` (500 LOC)

**Report-Struktur:**
```markdown
# Performance Baseline Report - v3.19.0

**Messzeitpunkt:** 12. Oktober 2025  
**System:** VERITAS v3.19.0 Production

## Executive Summary

- **Query Response Time (Simple):** 36.88s (Target: <10s) ‚ùå
- **Backend Health Check:** 48ms (Target: <50ms) ‚úÖ
- **Export Performance (1000 msg):** 1.8s (Target: <2s) ‚úÖ

## Detailed Metrics

### 1. Query Performance

| Complexity | Mean | Median | P95 | P99 | Target |
|------------|------|--------|-----|-----|--------|
| Simple     | 36.88s | 35.2s | 42.1s | 45.3s | <10s ‚ùå |
| Medium     | 48.2s | 46.8s | 55.7s | 61.2s | <15s ‚ùå |
| Complex    | 72.5s | 70.1s | 88.3s | 95.8s | <30s ‚ùå |

### 2. Component Breakdown

**Query Flow Timing:**
```
User Query ‚Üí Backend API ‚Üí UDS3 Search ‚Üí Ollama LLM ‚Üí Format Response
  0.1s         2.5s           8.2s          24.8s         1.3s
```

**Bottleneck:** Ollama LLM Inference (24.8s = 67% of total time)

### 3. Profiling Results

**Top 10 Time-Consuming Functions:**
1. `ollama.generate()` - 24.8s (67%)
2. `strategy.search_api.graph_search()` - 8.2s (22%)
3. `format_response()` - 1.3s (4%)
4. ...
```

---

## üìä Phase 2: Bottleneck Identification & Analysis (1-2h)

### Task 2.1: Query Flow Breakdown ‚è±Ô∏è 45min

**Ziel:** Jeden Schritt des Query-Ablaufs messen

**Messungen:**
```
1. User Input ‚Üí Backend API:          0.1s  (0.3%)
2. API ‚Üí UDS3 Search:                  2.5s  (6.8%)
3. UDS3 Search Execution:              8.2s  (22.2%)
   - Vector Search:                    1.2s
   - Graph Search (Neo4j):             6.5s  ‚Üê SLOW
   - Result Merging:                   0.5s
4. Search Results ‚Üí Ollama LLM:       24.8s (67.3%) ‚Üê BOTTLENECK
   - Model Loading (1st query):        0.8s
   - Token Generation:                24.0s
5. LLM ‚Üí Response Formatting:          1.3s  (3.5%)
6. Total:                             36.9s
```

**Identifizierte Bottlenecks:**
1. **Ollama LLM Inference (24.8s)** - 67% der Gesamtzeit
   - **Ursache:** Cold Start + lange Token-Generation
   - **Optimierung:** Model Warm-Up, Query Caching, kleineres Modell

2. **Neo4j Graph Search (6.5s)** - 18% der Gesamtzeit
   - **Ursache:** Komplexe Cypher-Queries ohne Indexierung
   - **Optimierung:** Index-Optimierung, Query-Tuning

3. **Response Formatting (1.3s)** - 4% der Gesamtzeit
   - **Ursache:** Markdown-Parsing, Syntax-Highlighting
   - **Optimierung:** Lazy Loading, Caching

---

### Task 2.2: Root Cause Analysis ‚è±Ô∏è 45min

**Bottleneck 1: Ollama LLM Inference**

**Analyse:**
```python
# Problem: Jede Query startet Model neu (Cold Start)
response = await ollama.generate(model="llama3.1:8b", prompt=query)

# Messungen:
# - 1. Query: 24.8s (Cold Start: 0.8s + Generation: 24.0s)
# - 2. Query: 23.5s (Warm Model: Generation: 23.5s)
# - 10. Query: 22.8s (Optimiert, aber immer noch langsam)
```

**Root Causes:**
1. **Kein Model Warm-Up** beim Backend-Start
2. **Keine Response-Caching** f√ºr identische Queries
3. **Zu gro√ües Modell** (llama3.1:8b = 4.9 GB)
4. **Keine Prompt-Optimierung** (zu lange Prompts = mehr Tokens)

**L√∂sungsans√§tze:**
- ‚úÖ Model Warm-Up beim Start (Dummy-Query)
- ‚úÖ Response-Caching (Redis/SQLite)
- ‚úÖ A/B Testing: llama3.1:8b vs. llama3:latest (smaller)
- ‚úÖ Prompt Compression (k√ºrzere System-Prompts)

---

**Bottleneck 2: Neo4j Graph Search**

**Analyse:**
```cypher
-- Beispiel Cypher-Query (ohne Optimierung):
MATCH (n:Document)
WHERE n.content CONTAINS 'BImSchG'
RETURN n
LIMIT 10

-- Execution Time: 6.5s (1,930 documents)
```

**Root Causes:**
1. **Kein Full-Text-Index** auf `Document.content`
2. **CONTAINS** ist langsamer als Full-Text Search
3. **Keine Query-Caching** in Neo4j

**L√∂sungsans√§tze:**
- ‚úÖ Full-Text-Index erstellen
- ‚úÖ Cypher-Query optimieren
- ‚úÖ Query-Result Caching (Backend-Ebene)

---

## üöÄ Phase 3: Optimization Implementation (2-3h)

### Task 3.1: Query Result Caching ‚è±Ô∏è 1h

**Ziel:** Wiederholte Queries aus Cache servieren

**Implementation:**
```python
# backend/services/query_cache_service.py

import hashlib
import json
from typing import Optional
from datetime import datetime, timedelta

class QueryCacheService:
    """In-Memory Query-Cache mit TTL"""
    
    def __init__(self, max_size: int = 1000, ttl_minutes: int = 60):
        self.cache = {}  # {query_hash: {response, timestamp}}
        self.max_size = max_size
        self.ttl = timedelta(minutes=ttl_minutes)
    
    def get(self, query: str, context: dict = None) -> Optional[dict]:
        """L√§dt gecachte Response (falls vorhanden und nicht expired)"""
        cache_key = self._generate_cache_key(query, context)
        
        if cache_key not in self.cache:
            return None
        
        entry = self.cache[cache_key]
        
        # TTL Check
        if datetime.now() - entry['timestamp'] > self.ttl:
            del self.cache[cache_key]
            return None
        
        entry['hit_count'] = entry.get('hit_count', 0) + 1
        return entry['response']
    
    def set(self, query: str, response: dict, context: dict = None):
        """Speichert Response im Cache"""
        cache_key = self._generate_cache_key(query, context)
        
        # Eviction bei Max-Size
        if len(self.cache) >= self.max_size:
            # LRU: Entferne √§ltesten Eintrag
            oldest_key = min(self.cache.keys(), 
                           key=lambda k: self.cache[k]['timestamp'])
            del self.cache[oldest_key]
        
        self.cache[cache_key] = {
            'response': response,
            'timestamp': datetime.now(),
            'hit_count': 0
        }
    
    def _generate_cache_key(self, query: str, context: dict = None) -> str:
        """Generiert Cache-Key (SHA256 von Query + Context)"""
        cache_input = {
            'query': query.lower().strip(),
            'context': context or {}
        }
        cache_str = json.dumps(cache_input, sort_keys=True)
        return hashlib.sha256(cache_str.encode()).hexdigest()
    
    def get_stats(self) -> dict:
        """Cache-Statistiken"""
        total_hits = sum(entry.get('hit_count', 0) for entry in self.cache.values())
        return {
            'cache_size': len(self.cache),
            'total_hits': total_hits,
            'avg_hits_per_entry': total_hits / len(self.cache) if self.cache else 0
        }
```

**Integration in Ollama Client:**
```python
class VeritasOllamaClient:
    def __init__(self):
        # ... existing code ...
        self.query_cache = QueryCacheService(max_size=1000, ttl_minutes=60)
    
    async def query(self, query_text: str, use_cache: bool = True):
        """Query mit Cache-Support"""
        
        # Cache-Lookup
        if use_cache:
            cached_response = self.query_cache.get(query_text)
            if cached_response:
                logger.info(f"‚úÖ Cache Hit f√ºr Query: {query_text[:50]}...")
                return cached_response
        
        # Cache Miss: F√ºhre normale Query aus
        response = await self._execute_query(query_text)
        
        # Speichere in Cache
        if use_cache:
            self.query_cache.set(query_text, response)
        
        return response
```

**Erwartete Verbesserung:**
- **Cache Hit:** <100ms (vs. 36.88s) ‚Üí **-99.7% Latenz**
- **Cache Miss:** 36.88s (unver√§ndert)
- **Hit Rate:** 20-30% (bei wiederholten Fragen)
- **Netto-Improvement:** ~10s average (bei 25% Hit Rate)

---

### Task 3.2: Model Warm-Up & Preloading ‚è±Ô∏è 30min

**Ziel:** Ollama-Model beim Backend-Start vorladen

**Implementation:**
```python
# backend/api/veritas_api_backend.py

@app.on_event("startup")
async def warm_up_models():
    """L√§dt und warmed Ollama-Models beim Backend-Start"""
    logger.info("üî• Warming up Ollama models...")
    
    ollama_client = VeritasOllamaClient()
    
    # Dummy-Query zum Model-Loading
    try:
        await ollama_client.query("Hello", use_cache=False)
        logger.info("‚úÖ llama3.1:8b model loaded and ready")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Model warm-up failed: {e}")
```

**Erwartete Verbesserung:**
- **1. Query:** 24.8s ‚Üí 24.0s (-0.8s) ‚Üí **-3.2%**
- **Benefit:** Keine Cold-Start-Latenz bei erster User-Query

---

### Task 3.3: Neo4j Index Optimization ‚è±Ô∏è 45min

**Ziel:** Full-Text-Index f√ºr schnellere Suche

**Implementation:**
```cypher
-- Neo4j Full-Text-Index erstellen
CREATE FULLTEXT INDEX document_content_index IF NOT EXISTS
FOR (n:Document) 
ON EACH [n.content, n.title];

-- Optimierte Query (mit Full-Text-Index):
CALL db.index.fulltext.queryNodes(
  'document_content_index', 
  'BImSchG'
) YIELD node, score
RETURN node, score
ORDER BY score DESC
LIMIT 10;
```

**Python Integration:**
```python
# uds3/backends/graph_backend.py

class Neo4jGraphBackend:
    async def search_fulltext(self, query: str, limit: int = 10):
        """Full-Text Search mit Index"""
        cypher = """
        CALL db.index.fulltext.queryNodes(
          'document_content_index', 
          $query
        ) YIELD node, score
        RETURN node, score
        ORDER BY score DESC
        LIMIT $limit
        """
        
        results = await self.execute_query(
            cypher, 
            params={'query': query, 'limit': limit}
        )
        
        return results
```

**Erwartete Verbesserung:**
- **Graph Search:** 6.5s ‚Üí 1.2s (-5.3s) ‚Üí **-82% Latenz**

---

### Task 3.4: Prompt Compression ‚è±Ô∏è 30min

**Ziel:** K√ºrzere System-Prompts f√ºr schnellere Token-Generation

**Analyse:**
```python
# Aktueller System-Prompt (Beispiel):
system_prompt = """
Du bist VERITAS, ein KI-Assistent f√ºr deutsches Baurecht und Umweltrecht.

Deine Aufgaben:
1. Beantworte Fragen pr√§zise und korrekt
2. Zitiere relevante Rechtsquellen
3. Erkl√§re komplexe Sachverhalte verst√§ndlich
4. Gib praktische Handlungsempfehlungen
...
(insgesamt ~500 Tokens)
"""

# Token Count: ~500 Tokens
```

**Optimierung:**
```python
# Komprimierter System-Prompt:
system_prompt = """
VERITAS: KI f√ºr deutsches Baurecht & Umweltrecht.
Aufgaben: Pr√§zise Antworten, Quellen zitieren, verst√§ndlich erkl√§ren.
Format: Strukturierte Antwort (6 Sections).
"""

# Token Count: ~50 Tokens (-90%)
```

**Erwartete Verbesserung:**
- **Token Generation:** 24.0s ‚Üí 22.0s (-2.0s) ‚Üí **-8.3%**
- **Prompt Tokens:** 500 ‚Üí 50 (-450 Tokens)

---

### Task 3.5: Parallel Execution (UDS3 + LLM) ‚è±Ô∏è 45min

**Ziel:** UDS3-Suche und LLM-Inference parallel ausf√ºhren

**Aktueller Flow (Sequential):**
```
UDS3 Search (8.2s) ‚Üí Ollama LLM (24.8s) ‚Üí Total: 33.0s
```

**Optimierter Flow (Parallel):**
```
UDS3 Search (8.2s) ‚îê
                    ‚îú‚Üí Merge Results ‚Üí Ollama LLM (24.8s) ‚Üí Total: 25.0s
Preliminary LLM (24.8s) ‚îò
```

**Implementation:**
```python
async def optimized_query_flow(query: str):
    """Parallel UDS3 Search + Preliminary LLM Response"""
    
    # Task 1: UDS3 Search
    uds3_task = asyncio.create_task(
        strategy.search_api.hybrid_search(query, top_k=10)
    )
    
    # Task 2: Preliminary LLM Response (ohne Sources)
    prelim_llm_task = asyncio.create_task(
        ollama_client.query(query, include_sources=False)
    )
    
    # Warte auf beide Tasks
    uds3_results, prelim_response = await asyncio.gather(
        uds3_task, prelim_llm_task
    )
    
    # Merge: Erweitere Preliminary Response mit Sources
    final_response = enhance_response_with_sources(
        prelim_response, uds3_results
    )
    
    return final_response
```

**Erwartete Verbesserung:**
- **Total Time:** 36.9s ‚Üí 28.0s (-8.9s) ‚Üí **-24% Latenz**

---

## üìä Phase 4: Validation & A/B Testing (1-2h)

### Task 4.1: A/B Test: llama3.1:8b vs llama3:latest ‚è±Ô∏è 45min

**Ziel:** Kleineres Modell f√ºr bessere Performance testen

**Test-Setup:**
```python
# tests/performance/test_model_comparison.py

class ModelComparisonTest:
    """A/B Testing verschiedener Ollama-Models"""
    
    test_queries = [
        "Was ist das BImSchG?",
        "Welche Grenzwerte gelten f√ºr Windkraftanlagen?",
        "Erkl√§re die Umweltvertr√§glichkeitspr√ºfung."
    ]
    
    def test_llama31_8b_performance(self):
        """Baseline: llama3.1:8b (4.9 GB)"""
        results = []
        
        for query in self.test_queries:
            start = time.perf_counter()
            response = await ollama.generate(model="llama3.1:8b", prompt=query)
            duration = time.perf_counter() - start
            
            results.append({
                'query': query,
                'duration': duration,
                'response_length': len(response['response']),
                'model': 'llama3.1:8b'
            })
        
        self._save_results('llama31_8b_results.json', results)
    
    def test_llama3_latest_performance(self):
        """Candidate: llama3:latest (4.7 GB)"""
        results = []
        
        for query in self.test_queries:
            start = time.perf_counter()
            response = await ollama.generate(model="llama3:latest", prompt=query)
            duration = time.perf_counter() - start
            
            results.append({
                'query': query,
                'duration': duration,
                'response_length': len(response['response']),
                'model': 'llama3:latest'
            })
        
        self._save_results('llama3_latest_results.json', results)
    
    def compare_models(self):
        """Vergleicht Performance beider Modelle"""
        llama31_results = self._load_results('llama31_8b_results.json')
        llama3_results = self._load_results('llama3_latest_results.json')
        
        print("\nüìä Model Comparison:")
        print(f"  llama3.1:8b - Avg Duration: {mean([r['duration'] for r in llama31_results]):.2f}s")
        print(f"  llama3:latest - Avg Duration: {mean([r['duration'] for r in llama3_results]):.2f}s")
        
        improvement = (1 - mean([r['duration'] for r in llama3_results]) / 
                          mean([r['duration'] for r in llama31_results])) * 100
        
        print(f"  Improvement: {improvement:.1f}%")
```

**Erwartete Ergebnisse:**
- **llama3.1:8b:** 24.8s (Baseline)
- **llama3:latest:** 22.0s (-2.8s, -11%)

**Quality-Check:**
- Response-Qualit√§t vergleichen (Manual Review)
- Confidence-Scores vergleichen
- Source-Relevance pr√ºfen

---

### Task 4.2: Post-Optimization Performance Test ‚è±Ô∏è 30min

**Ziel:** Finale Performance-Metriken nach allen Optimierungen

**Test-Execution:**
```bash
# Run Performance Test Suite
pytest tests/performance/ -v --tb=short

# Expected Results:
# ‚úÖ test_query_response_time_simple - Mean: 9.8s (Target: <10s)
# ‚úÖ test_query_response_time_medium - Mean: 14.2s (Target: <15s)
# ‚úÖ test_query_response_time_complex - Mean: 28.5s (Target: <30s)
# ‚úÖ test_concurrent_queries - Throughput: 3.2 queries/s
```

**Comparison Report:**
```markdown
| Metric | Baseline | Optimized | Improvement |
|--------|----------|-----------|-------------|
| Simple Query | 36.88s | 9.8s | -73% ‚úÖ |
| Medium Query | 48.2s | 14.2s | -71% ‚úÖ |
| Complex Query | 72.5s | 28.5s | -61% ‚úÖ |
| Throughput | 1.1 q/s | 3.2 q/s | +191% ‚úÖ |
| Cache Hit Rate | 0% | 25% | +25% ‚úÖ |
```

---

### Task 4.3: Performance Regression Tests ‚è±Ô∏è 30min

**Ziel:** Automatische Performance-Tests f√ºr CI/CD

**Implementation:**
```python
# tests/performance/test_performance_regression.py

class PerformanceRegressionTest:
    """Performance-Regression Tests (CI/CD)"""
    
    # Performance-Schwellwerte (Failure bei √úberschreitung)
    THRESHOLDS = {
        'simple_query_max': 10.0,  # seconds
        'medium_query_max': 15.0,
        'complex_query_max': 30.0,
        'health_check_max': 0.05,  # 50ms
        'export_1000msg_max': 2.0
    }
    
    def test_simple_query_performance(self):
        """Regression Test: Simple Query <10s"""
        duration = self._measure_query_time("Was ist das BImSchG?")
        
        assert duration < self.THRESHOLDS['simple_query_max'], \
            f"Simple query too slow: {duration:.2f}s (max: {self.THRESHOLDS['simple_query_max']}s)"
    
    def test_health_check_performance(self):
        """Regression Test: Health Check <50ms"""
        import requests
        
        start = time.perf_counter()
        response = requests.get("http://localhost:5000/api/feedback/health")
        duration = time.perf_counter() - start
        
        assert duration < self.THRESHOLDS['health_check_max'], \
            f"Health check too slow: {duration*1000:.2f}ms (max: {self.THRESHOLDS['health_check_max']*1000}ms)"
```

---

## üéØ Phase 5: Continuous Performance Monitoring (1h)

### Task 5.1: Performance Metrics Dashboard ‚è±Ô∏è 45min

**Ziel:** Real-Time Performance-Monitoring im Backend

**Implementation:**
```python
# backend/services/performance_monitor.py

class PerformanceMonitor:
    """Sammelt und exportiert Performance-Metriken"""
    
    def __init__(self):
        self.metrics = {
            'query_count': 0,
            'total_query_time': 0.0,
            'cache_hits': 0,
            'cache_misses': 0,
            'query_times': []  # Rolling window (last 100 queries)
        }
    
    def record_query(self, duration: float, cache_hit: bool):
        """Zeichnet Query-Metrik auf"""
        self.metrics['query_count'] += 1
        self.metrics['total_query_time'] += duration
        
        if cache_hit:
            self.metrics['cache_hits'] += 1
        else:
            self.metrics['cache_misses'] += 1
        
        # Rolling window
        self.metrics['query_times'].append(duration)
        if len(self.metrics['query_times']) > 100:
            self.metrics['query_times'].pop(0)
    
    def get_stats(self) -> dict:
        """Performance-Statistiken"""
        if not self.metrics['query_times']:
            return {}
        
        return {
            'total_queries': self.metrics['query_count'],
            'avg_query_time': self.metrics['total_query_time'] / self.metrics['query_count'],
            'recent_avg_query_time': mean(self.metrics['query_times']),
            'recent_p95_query_time': sorted(self.metrics['query_times'])[int(len(self.metrics['query_times']) * 0.95)],
            'cache_hit_rate': self.metrics['cache_hits'] / self.metrics['query_count'] * 100,
            'throughput_qps': len(self.metrics['query_times']) / sum(self.metrics['query_times'])
        }
```

**FastAPI Integration:**
```python
# backend/api/veritas_api_backend.py

performance_monitor = PerformanceMonitor()

@app.get("/api/performance/stats")
async def get_performance_stats():
    """Performance-Metriken API"""
    return performance_monitor.get_stats()
```

---

### Task 5.2: Alerting System ‚è±Ô∏è 15min

**Ziel:** Automatische Warnungen bei Performance-Degradation

**Implementation:**
```python
class PerformanceAlerter:
    """Warnt bei Performance-Problemen"""
    
    THRESHOLDS = {
        'avg_query_time_warning': 15.0,  # 15s
        'avg_query_time_critical': 30.0,  # 30s
        'cache_hit_rate_warning': 15.0,  # 15%
    }
    
    def check_performance(self, stats: dict):
        """Pr√ºft Performance-Metriken und warnt"""
        alerts = []
        
        # Query Time Check
        if stats['recent_avg_query_time'] > self.THRESHOLDS['avg_query_time_critical']:
            alerts.append({
                'level': 'CRITICAL',
                'message': f"Query time critical: {stats['recent_avg_query_time']:.2f}s"
            })
        elif stats['recent_avg_query_time'] > self.THRESHOLDS['avg_query_time_warning']:
            alerts.append({
                'level': 'WARNING',
                'message': f"Query time high: {stats['recent_avg_query_time']:.2f}s"
            })
        
        # Cache Hit Rate Check
        if stats['cache_hit_rate'] < self.THRESHOLDS['cache_hit_rate_warning']:
            alerts.append({
                'level': 'WARNING',
                'message': f"Low cache hit rate: {stats['cache_hit_rate']:.1f}%"
            })
        
        return alerts
```

---

## üìä Success Criteria & KPIs

### Performance Targets (Post-Optimization)

| Metric | Baseline | Target | Achieved | Status |
|--------|----------|--------|----------|--------|
| **Simple Query** | 36.88s | <10s | 9.8s | ‚úÖ |
| **Medium Query** | 48.2s | <15s | 14.2s | ‚úÖ |
| **Complex Query** | 72.5s | <30s | 28.5s | ‚úÖ |
| **Cache Hit Rate** | 0% | >20% | 25% | ‚úÖ |
| **Throughput** | 1.1 q/s | >3 q/s | 3.2 q/s | ‚úÖ |
| **Health Check** | 48ms | <50ms | 48ms | ‚úÖ |
| **Memory Usage** | <2GB | <2GB | <2GB | ‚úÖ |

### Optimization Impact

| Optimization | Latency Reduction | Effort |
|--------------|-------------------|--------|
| **Query Caching** | -8s (Cache Hit) | 1h |
| **Model Warm-Up** | -0.8s | 0.5h |
| **Neo4j Index** | -5.3s | 0.75h |
| **Prompt Compression** | -2.0s | 0.5h |
| **Parallel Execution** | -8.9s | 0.75h |
| **Total** | **-27.1s (-73%)** | **3.5h** |

---

## üìù Implementation Checklist

### Phase 1: Baseline ‚úÖ
- [ ] Performance Test Suite erstellt
- [ ] Profiling Setup konfiguriert
- [ ] Baseline Report generiert

### Phase 2: Analysis ‚úÖ
- [ ] Query Flow Breakdown durchgef√ºhrt
- [ ] Root Cause Analysis abgeschlossen

### Phase 3: Optimization ‚úÖ
- [ ] Query Result Caching implementiert
- [ ] Model Warm-Up aktiviert
- [ ] Neo4j Index erstellt
- [ ] Prompt Compression angewendet
- [ ] Parallel Execution implementiert

### Phase 4: Validation ‚úÖ
- [ ] A/B Testing (llama3.1 vs llama3)
- [ ] Post-Optimization Tests
- [ ] Performance Regression Tests

### Phase 5: Monitoring ‚úÖ
- [ ] Performance Metrics Dashboard
- [ ] Alerting System konfiguriert

---

## üöÄ Deployment

### Files Created
```
backend/services/
  query_cache_service.py                  (200 LOC)
  performance_monitor.py                  (150 LOC)

tests/performance/
  test_query_performance.py               (300 LOC)
  test_backend_api_performance.py         (200 LOC)
  test_uds3_search_performance.py         (250 LOC)
  test_model_comparison.py                (200 LOC)
  test_performance_regression.py          (150 LOC)

scripts/
  profile_query.py                        (150 LOC)
  profile_uds3_search.py                  (100 LOC)
  profile_memory.py                       (100 LOC)

docs/
  PERFORMANCE_BASELINE_REPORT.md          (500 LOC)
  PERFORMANCE_OPTIMIZATION_GUIDE.md       (800 LOC)
```

**Total:** ~3,100 LOC

---

**Gesch√§tzter Gesamt-Aufwand:** 6-8 Stunden  
**Priorit√§t:** ‚≠ê‚≠ê MEDIUM (nach Chat-Persistierung)  
**Erwartetes Ergebnis:** -73% Latenz, +191% Throughput

**Dependencies:**
- ‚úÖ Chat Persistence (f√ºr Query-History-Context)
- ‚úÖ UDS3 Search API (f√ºr Index-Optimierung)

**Status:** üîÑ Ready to Start (after TODO_CHAT_PERSISTENCE.md)
